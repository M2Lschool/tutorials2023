{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QuolPkoI7i9X"
   },
   "source": [
    "# Practical 2 - Graph Attention Networks\n",
    "---\n",
    "\n",
    "**Tutorial overview:** In this tutorial you will implement Graph attention (GAT). Consequently, you will run the full training loop of GCN and GAT on the real-world citation graph OGBN-Arxiv for node classification and analyze the results.\n",
    "\n",
    "**Tutorial outline:**\n",
    "- Graph Attention Networks (GAT)\n",
    "- GAT on Karate Dataset\n",
    "- GAT and GCN in large scale OGBN-Arxiv dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5EriCRwB4FBy"
   },
   "source": [
    "## Theory Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YUwAKF8Y4Iko"
   },
   "source": [
    "### Graph Attention (GAT) Layer\n",
    "\n",
    "While the GCN we covered in the previous section can learn meaningful representations, it also has some shortcomings. Can you think of any?\n",
    "\n",
    "In the GCN layer, the messages from all its neighbors and the node itself are equally weighted -- well, this is not exactly true because of the symmetric normalization. However, the main limitation is that the aggregation weights are **hard-crafted** (division by the in- and out-degree). This may lead to loss of node-specific information. E.g., consider the case when a set of nodes shares the same set of neighbors, and start out with different node features. Then because of averaging, their resulting output features would be the same. Adding self-edges mitigates this issue by a small amount, but this problem is magnified with increasing number of GCN layers and number of edges connecting to a node.\n",
    "\n",
    "In more formal words, the implemented GCN uses *isotropic* learnable filters, while we want\n",
    "*anisotropic* filters as they can catch more complex pattern -- as it happens for traditional convolutional filters on images.\n",
    "\n",
    "The graph attention (GAT) mechanism, as proposed by [Velickovic et al. ( 2017)](https://arxiv.org/abs/1710.10903), allows the network to learn how to weight / assign importance to the node features from the neighborhood when computing the new node features. This is very similar to the idea of using attention in Transformers, which were introduced in [Vaswani et al. (2017)](https://arxiv.org/abs/1706.03762). Indeed,\n",
    "transformers has been shown to be a special case of graph attention networks, where a fully-connected graph structure is assumed (see articles from [Joshi (2020)](https://graphdeeplearning.github.io/files/transformers-are-gnns-slides.pdf) and [Dwivedi et al. (2020)](https://arxiv.org/abs/2012.09699)).\n",
    "\n",
    "In the figure below, $\\vec{h}$ are the node features and $\\vec{\\alpha}$ are the learned attention weights.\n",
    "\n",
    "\n",
    "<center><image src=\"https://storage.googleapis.com/dm-educational/assets/graph-nets/gat1.png\" width=\"400px\"></center>\n",
    "\n",
    "Figure Credit: [Velickovic et al. ( 2017)](https://arxiv.org/abs/1710.10903).\n",
    "(Detail: This image is showing multi-headed attention with 3 heads, each color corresponding to a different head. At the end, an aggregation function is applied over all the heads.)\n",
    "\n",
    "To obtain the output node features of a single head GAT layer, we compute:\n",
    "\n",
    "$$ \\vec{h}'_i = \\sum _{j \\in \\mathcal{N}(i)}\\alpha_{ij} \\mathbf{W} \\vec{h}_j$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "puYBnCdfXrIk"
   },
   "source": [
    "## Implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cdptlndy2eqn"
   },
   "source": [
    "### Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oxZ9FwjUZouW"
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/deepmind/jraph.git\n",
    "!pip install flax\n",
    "!pip install dm-haiku\n",
    "!pip install networkx\n",
    "!pip install ogb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-lrshhVob7z6"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "%matplotlib inline\n",
    "import functools\n",
    "import matplotlib.pyplot as plt\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.tree_util as tree\n",
    "import jraph\n",
    "import flax\n",
    "import haiku as hk\n",
    "import optax\n",
    "import pickle\n",
    "import numpy as onp\n",
    "import networkx as nx\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xNWZQ7yPcL7s"
   },
   "outputs": [],
   "source": [
    "# Helpers for visualization\n",
    "def convert_jraph_to_networkx_graph(jraph_graph: jraph.GraphsTuple) -> nx.Graph:\n",
    "  nodes, edges, receivers, senders, _, _, _ = jraph_graph\n",
    "  nx_graph = nx.DiGraph()\n",
    "  if nodes is None:\n",
    "    for n in range(jraph_graph.n_node[0]):\n",
    "      nx_graph.add_node(n)\n",
    "  else:\n",
    "    for n in range(jraph_graph.n_node[0]):\n",
    "      nx_graph.add_node(n, node_feature=nodes[n])\n",
    "  if edges is None:\n",
    "    for e in range(jraph_graph.n_edge[0]):\n",
    "      nx_graph.add_edge(int(senders[e]), int(receivers[e]))\n",
    "  else:\n",
    "    for e in range(jraph_graph.n_edge[0]):\n",
    "      nx_graph.add_edge(\n",
    "          int(senders[e]), int(receivers[e]), edge_feature=edges[e])\n",
    "  return nx_graph\n",
    "\n",
    "def draw_jraph_graph_structure(jraph_graph: jraph.GraphsTuple) -> None:\n",
    "  nx_graph = convert_jraph_to_networkx_graph(jraph_graph)\n",
    "  pos = nx.spring_layout(nx_graph)\n",
    "  nx.draw(\n",
    "      nx_graph, pos=pos, with_labels=True, node_size=500, font_color='yellow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Pp5sdGN-set"
   },
   "source": [
    "#### Recap: Toy Graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vgLmXbW8-w2I"
   },
   "outputs": [],
   "source": [
    "def build_toy_graph() -> jraph.GraphsTuple:\n",
    "  \"\"\"Define a four node graph, each node has a scalar as its feature.\"\"\"\n",
    "\n",
    "  # Nodes are defined implicitly by their features.\n",
    "  # We will add four nodes, each with a feature, e.g.\n",
    "  # node 0 has feature [0.],\n",
    "  # node 1 has featre [2.] etc.\n",
    "  # len(node_features) is the number of nodes.\n",
    "  node_features = jnp.array([[0.], [2.], [4.], [6.]])\n",
    "\n",
    "  # We will now specify 5 directed edges connecting the nodes we defined above.\n",
    "  # We define this with `senders` (source node indices) and `receivers`\n",
    "  # (destination node indices).\n",
    "  # For example, to add an edge from node 0 to node 1, we append 0 to senders,\n",
    "  # and 1 to receivers.\n",
    "  # We can do the same for all 5 edges:\n",
    "  # 0 -> 1\n",
    "  # 1 -> 2\n",
    "  # 2 -> 0\n",
    "  # 3 -> 0\n",
    "  # 0 -> 3\n",
    "  senders = jnp.array([0, 1, 2, 3, 0])\n",
    "  receivers = jnp.array([1, 2, 0, 0, 3])\n",
    "\n",
    "  # You can optionally add edge attributes to the 5 edges.\n",
    "  edges = jnp.array([[5.], [6.], [7.], [8.], [8.]])\n",
    "\n",
    "  # We then save the number of nodes and the number of edges.\n",
    "  # This information is used to make running GNNs over multiple graphs\n",
    "  # in a GraphsTuple possible.\n",
    "  n_node = jnp.array([4])\n",
    "  n_edge = jnp.array([5])\n",
    "\n",
    "  # Optionally you can add `global` information, such as a graph label.\n",
    "  global_context = jnp.array([[1]]) # Same feature dims as nodes and edges.\n",
    "  graph = jraph.GraphsTuple(\n",
    "      nodes=node_features,\n",
    "      edges=edges,\n",
    "      senders=senders,\n",
    "      receivers=receivers,\n",
    "      n_node=n_node,\n",
    "      n_edge=n_edge,\n",
    "      globals=global_context\n",
    "      )\n",
    "  return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PD6WUk6YkHDn"
   },
   "source": [
    "#### Recap: Zachary's Karate Graph\n",
    "On [Zachary's karate club](https://en.wikipedia.org/wiki/Zachary%27s_karate_club) we will optimize the assignments of student to master nodes using GAT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g7DoCGSuj8EA"
   },
   "outputs": [],
   "source": [
    "\"\"\"Zachary's karate club example.\n",
    "From https://github.com/deepmind/jraph/blob/master/jraph/examples/zacharys_karate_club.py.\n",
    "Here we train a graph neural network to process Zachary's karate club.\n",
    "https://en.wikipedia.org/wiki/Zachary%27s_karate_club\n",
    "Zachary's karate club is used in the literature as an example of a social graph.\n",
    "Here we use a graphnet to optimize the assignments of the students in the\n",
    "karate club to two distinct karate instructors (Mr. Hi and John A).\n",
    "\"\"\"\n",
    "\n",
    "def get_zacharys_karate_club() -> jraph.GraphsTuple:\n",
    "  \"\"\"Returns GraphsTuple representing Zachary's karate club.\"\"\"\n",
    "  social_graph = [\n",
    "      (1, 0), (2, 0), (2, 1), (3, 0), (3, 1), (3, 2),\n",
    "      (4, 0), (5, 0), (6, 0), (6, 4), (6, 5), (7, 0), (7, 1),\n",
    "      (7, 2), (7, 3), (8, 0), (8, 2), (9, 2), (10, 0), (10, 4),\n",
    "      (10, 5), (11, 0), (12, 0), (12, 3), (13, 0), (13, 1), (13, 2),\n",
    "      (13, 3), (16, 5), (16, 6), (17, 0), (17, 1), (19, 0), (19, 1),\n",
    "      (21, 0), (21, 1), (25, 23), (25, 24), (27, 2), (27, 23),\n",
    "      (27, 24), (28, 2), (29, 23), (29, 26), (30, 1), (30, 8),\n",
    "      (31, 0), (31, 24), (31, 25), (31, 28), (32, 2), (32, 8),\n",
    "      (32, 14), (32, 15), (32, 18), (32, 20), (32, 22), (32, 23),\n",
    "      (32, 29), (32, 30), (32, 31), (33, 8), (33, 9), (33, 13),\n",
    "      (33, 14), (33, 15), (33, 18), (33, 19), (33, 20), (33, 22),\n",
    "      (33, 23), (33, 26), (33, 27), (33, 28), (33, 29), (33, 30),\n",
    "      (33, 31), (33, 32)]\n",
    "  # Add reverse edges.\n",
    "  social_graph += [(edge[1], edge[0]) for edge in social_graph]\n",
    "  n_club_members = 34\n",
    "\n",
    "  return jraph.GraphsTuple(\n",
    "      n_node=jnp.asarray([n_club_members]),\n",
    "      n_edge=jnp.asarray([len(social_graph)]),\n",
    "      # One-hot encoding for nodes, i.e. argmax(nodes) = node index.\n",
    "      nodes=jnp.eye(n_club_members),\n",
    "      # No edge features.\n",
    "      edges=None,\n",
    "      globals=None,\n",
    "      senders=jnp.asarray([edge[0] for edge in social_graph]),\n",
    "      receivers=jnp.asarray([edge[1] for edge in social_graph]))\n",
    "\n",
    "def get_ground_truth_assignments_for_zacharys_karate_club() -> jnp.ndarray:\n",
    "  \"\"\"Returns ground truth assignments for Zachary's karate club.\"\"\"\n",
    "  return jnp.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n",
    "                    0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
    "\n",
    "graph = get_zacharys_karate_club()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qq-csnqPcB9v"
   },
   "source": [
    "Helper function to optimize Karate Club dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FPLSP8R5cFoJ"
   },
   "outputs": [],
   "source": [
    "def optimize_club(network: hk.Transformed, num_steps: int) -> jnp.ndarray:\n",
    "  \"\"\"Solves the karate club problem by optimizing the assignments of students.\"\"\"\n",
    "  zacharys_karate_club = get_zacharys_karate_club()\n",
    "  labels = get_ground_truth_assignments_for_zacharys_karate_club()\n",
    "  params = network.init(jax.random.PRNGKey(42), zacharys_karate_club)\n",
    "\n",
    "  @jax.jit\n",
    "  def predict(params: hk.Params) -> jnp.ndarray:\n",
    "    decoded_graph = network.apply(params, zacharys_karate_club)\n",
    "    return jnp.argmax(decoded_graph.nodes, axis=1)\n",
    "\n",
    "  @jax.jit\n",
    "  def prediction_loss(params: hk.Params) -> jnp.ndarray:\n",
    "    decoded_graph = network.apply(params, zacharys_karate_club)\n",
    "    # We interpret the decoded nodes as a pair of logits for each node.\n",
    "    log_prob = jax.nn.log_softmax(decoded_graph.nodes)\n",
    "    # The only two assignments we know a-priori are those of Mr. Hi (Node 0)\n",
    "    # and John A (Node 33).\n",
    "    return -(log_prob[0, 0] + log_prob[33, 1])\n",
    "\n",
    "  opt_init, opt_update = optax.adam(1e-2)\n",
    "  opt_state = opt_init(params)\n",
    "\n",
    "  @jax.jit\n",
    "  def update(params: hk.Params, opt_state) -> Tuple[hk.Params, Any]:\n",
    "    \"\"\"Returns updated params and state.\"\"\"\n",
    "    g = jax.grad(prediction_loss)(params)\n",
    "    updates, opt_state = opt_update(g, opt_state)\n",
    "    return optax.apply_updates(params, updates), opt_state\n",
    "\n",
    "  @jax.jit\n",
    "  def accuracy(params: hk.Params) -> jnp.ndarray:\n",
    "    decoded_graph = network.apply(params, zacharys_karate_club)\n",
    "    return jnp.mean(jnp.argmax(decoded_graph.nodes, axis=1) == labels)\n",
    "\n",
    "  for step in range(num_steps):\n",
    "    print(f\"step {step} accuracy {accuracy(params).item():.2f}\")\n",
    "    params, opt_state = update(params, opt_state)\n",
    "\n",
    "  return predict(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tgFx6U0c425K"
   },
   "source": [
    "### Graph Attention Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PcPhh76g44mB"
   },
   "outputs": [],
   "source": [
    "def add_self_edges_fn(receivers: jnp.ndarray, senders: jnp.ndarray,\n",
    "                      total_num_nodes: int) -> Tuple[jnp.ndarray, jnp.ndarray]:\n",
    "  \"\"\"Adds self edges. Assumes self edges are not in the graph yet.\"\"\"\n",
    "  ################\n",
    "  # YOUR CODE HERE\n",
    "  # for each node, add a connection to itself, both from the sender and receiver perspective\n",
    "  # you can easily implement it in a pythonic and vectorized way, by knowing total_num_nodes.\n",
    "  # HINT: Copy paste code from previous colab :)\n",
    "  receivers = ...\n",
    "  senders = ...\n",
    "  ################\n",
    "  return receivers, senders\n",
    "\n",
    "def attention_logit_fn(sender_attr: jnp.ndarray, receiver_attr: jnp.ndarray,\n",
    "                       edges: jnp.ndarray) -> jnp.ndarray:\n",
    "  del edges\n",
    "  ################\n",
    "  # YOUR CODE HERE\n",
    "  # Step 1: Concatenate the sender and receiver attributes\n",
    "  # Step 2: Pass the concatenated output through MLP with output size 1\n",
    "  x = ...\n",
    "  x = ...\n",
    "  ################\n",
    "  return x\n",
    "\n",
    "# GAT implementation adapted from https://github.com/deepmind/jraph/blob/master/jraph/_src/models.py#L442.\n",
    "def GAT(attention_query_fn: Callable,\n",
    "        attention_logit_fn: Callable,\n",
    "        node_update_fn: Callable,\n",
    "        add_self_edges: bool = True) -> Callable:\n",
    "  \"\"\"Returns a method that applies a Graph Attention Network layer.\n",
    "\n",
    "  Graph Attention message passing as described in\n",
    "  https://arxiv.org/pdf/1710.10903.pdf. This model expects node features as a\n",
    "  jnp.array, may use edge features for computing attention weights, and\n",
    "  ignore global features. It does not support nests.\n",
    "  Args:\n",
    "    attention_query_fn: function that generates attention queries from sender\n",
    "      node features.\n",
    "    attention_logit_fn: function that converts attention queries into logits for\n",
    "      softmax attention.\n",
    "    node_update_fn: function that updates the aggregated messages. If None, will\n",
    "      apply leaky relu and concatenate (if using multi-head attention).\n",
    "\n",
    "  Returns:\n",
    "    A function that applies a Graph Attention layer.\n",
    "  \"\"\"\n",
    "  # pylint: disable=g-long-lambda\n",
    "\n",
    "  def _ApplyGAT(graph: jraph.GraphsTuple) -> jraph.GraphsTuple:\n",
    "    \"\"\"Applies a Graph Attention layer.\"\"\"\n",
    "    nodes, edges, receivers, senders, _, _, _ = graph\n",
    "\n",
    "    # Equivalent to the sum of n_node, but statically known.\n",
    "    try:\n",
    "      sum_n_node = nodes.shape[0]\n",
    "    except IndexError:\n",
    "      raise IndexError('GAT requires node features')\n",
    "\n",
    "    total_num_nodes = tree.tree_leaves(nodes)[0].shape[0]\n",
    "    if add_self_edges:\n",
    "      # We add self edges to the senders and receivers so that each node\n",
    "      # includes itself in aggregation.\n",
    "      receivers, senders = add_self_edges_fn(receivers, senders,\n",
    "                                             total_num_nodes)\n",
    "\n",
    "    ################\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # Pass nodes through the attention query function to transform\n",
    "    # node features, e.g. with an MLP.\n",
    "    nodes = ...\n",
    "\n",
    "    # We compute the softmax logits using a function that takes the\n",
    "    # embedded sender and receiver attributes.\n",
    "    sent_attributes = nodes[senders]\n",
    "    received_attributes = nodes[receivers]\n",
    "    att_softmax_logits = attention_logit_fn(sent_attributes,\n",
    "                                            received_attributes, edges)\n",
    "\n",
    "    # Compute the attention softmax weights on the entire tree.\n",
    "    # Hint: you can take advantage of segment softmax\n",
    "    # https://jraph.readthedocs.io/en/latest/api.html#jraph.segment_softmax\n",
    "    # att_weights = jraph.segment_softmax(\n",
    "    #     ???, segment_ids=???, num_segments=???)\n",
    "    att_weights = ...\n",
    "\n",
    "    # Multiple attention weights with `sent_attributes`.\n",
    "    messages = ...\n",
    "\n",
    "    # 4. Aggregate messages to nodes\n",
    "    # HINT: agg_messages = jax.ops.segment_sum(???, ???, num_segments=???).\n",
    "    agg_messages = ...\n",
    "\n",
    "    # 5a. Apply `node_update_fn` to the aggregated messages.\n",
    "    nodes = ...\n",
    "\n",
    "    ################\n",
    "    return graph._replace(nodes=nodes)\n",
    "\n",
    "  return _ApplyGAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NQ6Bv9GZ4xVL"
   },
   "outputs": [],
   "source": [
    "node_update_fn = lambda x: jnp.reshape(\n",
    "    jax.nn.leaky_relu(x), (x.shape[0], -1))\n",
    "\n",
    "gat_layer = GAT(\n",
    "    attention_query_fn=lambda n: hk.Linear(8)(n),\n",
    "    attention_logit_fn=attention_logit_fn,\n",
    "    node_update_fn=node_update_fn,\n",
    "    add_self_edges=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KcNTXNXrQB4J"
   },
   "source": [
    "### Test GAT Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m0Mq47pXQD4h"
   },
   "outputs": [],
   "source": [
    "graph = build_toy_graph()\n",
    "network = hk.without_apply_rng(hk.transform(gat_layer))\n",
    "params = network.init(jax.random.PRNGKey(42), graph)\n",
    "out_graph = network.apply(params, graph)\n",
    "out_graph.nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LPyi6mza4HF8"
   },
   "outputs": [],
   "source": [
    "def gat_2_layers(graph: jraph.GraphsTuple) -> jraph.GraphsTuple:\n",
    "  \"\"\"Defines a GAT network for the karate club node classification task.\n",
    "\n",
    "  Args:\n",
    "    graph: GraphsTuple the network processes.\n",
    "\n",
    "  Returns:\n",
    "    output graph with updated node values.\n",
    "  \"\"\"\n",
    "  # We implement a 2 layers GAT Network\n",
    "\n",
    "  # First Laeyer\n",
    "  gn = GAT(\n",
    "      attention_query_fn=lambda n: hk.Linear(8)(n),\n",
    "      attention_logit_fn=attention_logit_fn,\n",
    "      node_update_fn=node_update_fn,\n",
    "      add_self_edges=True)\n",
    "  graph = gn(graph)\n",
    "\n",
    "  ################\n",
    "  # YOUR CODE HERE\n",
    "  # node_update_fn of the second layer must be a linear projection to the 2 classes\n",
    "  gn = ...\n",
    "  ################\n",
    "\n",
    "  graph = gn(graph)\n",
    "  return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7TbmeAFK5v9O"
   },
   "source": [
    "### Node classification on Karate dataset with GAT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dbksYW204evm"
   },
   "source": [
    "Let's train the model!\n",
    "\n",
    "We expect the model to reach an accuracy of about 0.97.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b11nvf914Nin"
   },
   "outputs": [],
   "source": [
    "\n",
    "network = hk.without_apply_rng(hk.transform(gat_2_layers))\n",
    "result = optimize_club(network, num_steps=15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rW3GkO0R4jNS"
   },
   "source": [
    "The final node assignment predicted by the trained model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DeHttElp4iIQ"
   },
   "outputs": [],
   "source": [
    "\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XFwxMWx05jH_"
   },
   "outputs": [],
   "source": [
    "zacharys_karate_club = get_zacharys_karate_club()\n",
    "nx_graph = convert_jraph_to_networkx_graph(zacharys_karate_club)\n",
    "pos = nx.circular_layout(nx_graph)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 7))\n",
    "ax1 = fig.add_subplot(121)\n",
    "nx.draw(\n",
    "    nx_graph,\n",
    "    pos=pos,\n",
    "    with_labels=True,\n",
    "    node_size=500,\n",
    "    node_color=result.tolist(),\n",
    "    font_color='white')\n",
    "ax1.title.set_text('Predicted Node Assignments with GAT')\n",
    "\n",
    "gt_labels = get_ground_truth_assignments_for_zacharys_karate_club()\n",
    "ax2 = fig.add_subplot(122)\n",
    "nx.draw(\n",
    "    nx_graph,\n",
    "    pos=pos,\n",
    "    with_labels=True,\n",
    "    node_size=500,\n",
    "    node_color=gt_labels.tolist(),\n",
    "    font_color='white')\n",
    "ax2.title.set_text('Ground-Truth Node Assignments')\n",
    "fig.suptitle('Do you spot the difference? 😐', y=-0.01)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zVOG84qhMmAd"
   },
   "source": [
    "## Multiclass node classification on OGBN-arxiv citation network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ECmN1Lh7M1U6"
   },
   "source": [
    "Now that we are familiar with the node classification task, let's try to repeat the task on a medium-scale graph. We will use the paper citation graph provided by the public [ogbn-arxiv](https://ogb.stanford.edu/docs/nodeprop/#ogbn-arxiv) benchmark.\n",
    "\n",
    "The ogbn-arxiv dataset is a directed graph representing the citation network between all Computer Science (CS) arXiv papers indexed by Microsoft academic graph. Each node is an arXiv paper and each directed edge indicates that one paper cites another one. Each paper comes with a 128-dimensional feature vector obtained by averaging the embeddings of words in its title and abstract. The task is to predict the 40 subject areas of arXiv CS papers, e.g., cs.AI, cs.LG, and cs.OS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q7MzCbwuNG3i"
   },
   "outputs": [],
   "source": [
    "from ogb.nodeproppred import NodePropPredDataset\n",
    "import networkx as nx\n",
    "dataset_name = \"ogbn-arxiv\"\n",
    "dataset = NodePropPredDataset(name=dataset_name)\n",
    "rand_seed = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7qvsTjkEKiyc"
   },
   "source": [
    "### Create the jraph GraphsTuple for OGBN-Arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kdcySuABKoqB"
   },
   "outputs": [],
   "source": [
    "senders =  jnp.array(dataset[0][0]['edge_index'][0] , dtype=jnp.int32)\n",
    "receivers = jnp.array(dataset[0][0]['edge_index'][1] , dtype=jnp.int32)\n",
    "node_features = jnp.array(dataset[0][0]['node_feat'], dtype=jnp.float32)\n",
    "n_node = jnp.array([dataset[0][0]['num_nodes']])\n",
    "n_edge = jnp.array([len(receivers)])\n",
    "global_context = jnp.array([[1]], dtype=jnp.int32)\n",
    "labels = jnp.array(dataset[0][1], dtype=jnp.int32)\n",
    "graph = jraph.GraphsTuple(nodes=node_features, senders=senders, receivers=receivers, n_node=n_node, n_edge=n_edge, globals=global_context,edges= None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FXbODzJfKuDv"
   },
   "outputs": [],
   "source": [
    "print(f'Number of nodes: {graph.n_node[0]}')\n",
    "print(f'Number of edges: {graph.n_edge[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VEISrG7bNPbN"
   },
   "source": [
    "### Split into train, validation and test nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T66NjeqDK90k"
   },
   "source": [
    "The original OGBN benchmark considers a realistic data split based on the publication dates of the papers. The general setting is that the ML models are trained on existing papers and then used to predict the subject areas of newly-published papers, which supports the direct application of them into real-world scenarios, such as helping the arXiv moderators. Specifically, we train on papers published until 2017, validate on those published in 2018, and test on those published since 2019.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HMlXjn8INVai"
   },
   "outputs": [],
   "source": [
    "split_idx = dataset.get_idx_split()\n",
    "train_idx = split_idx[\"train\"]\n",
    "val_idx = split_idx[\"valid\"]\n",
    "test_idx = split_idx[\"test\"]\n",
    "\n",
    "train_labels = jnp.squeeze(labels[train_idx])\n",
    "val_labels = jnp.squeeze(labels[val_idx])\n",
    "test_labels = jnp.squeeze(labels[test_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IkIUzbCdMOhN"
   },
   "source": [
    "### GCN Model for OGBN-Arxiv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PEqi9u6lcuh1"
   },
   "source": [
    "\n",
    "\n",
    "Let's use the GraphConvolution implementation of the first part of the tutorial to compare the two layers in a multiclass node classification problem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J6HF8YMZcxMJ"
   },
   "outputs": [],
   "source": [
    "def GraphConvolution(update_node_fn: Callable,\n",
    "                     aggregate_nodes_fn: Callable = jax.ops.segment_sum,\n",
    "                     add_self_edges: bool = False,\n",
    "                     symmetric_normalization: bool = True) -> Callable:\n",
    "  \"\"\"Returns a method that applies a Graph Convolution layer.\n",
    "\n",
    "  Graph Convolutional layer as in https://arxiv.org/abs/1609.02907,\n",
    "  NOTE: This implementation does not add an activation after aggregation.\n",
    "  If you are stacking layers, you may want to add an activation between\n",
    "  each layer.\n",
    "  Args:\n",
    "    update_node_fn: function used to update the nodes. In the paper a single\n",
    "      layer MLP is used.\n",
    "    aggregate_nodes_fn: function used to aggregates the sender nodes.\n",
    "    add_self_edges: whether to add self edges to nodes in the graph as in the\n",
    "      paper definition of GCN. Defaults to False.\n",
    "    symmetric_normalization: whether to use symmetric normalization. Defaults to\n",
    "      True.\n",
    "\n",
    "  Returns:\n",
    "    A method that applies a Graph Convolution layer.\n",
    "  \"\"\"\n",
    "  ################\n",
    "  # YOUR CODE HERE\n",
    "  ### Just Copy paste block from previous colab :)\n",
    "  def _ApplyGCN(graph: jraph.GraphsTuple) -> jraph.GraphsTuple:\n",
    "    \"\"\"Applies a Graph Convolution layer.\"\"\"\n",
    "    ...\n",
    "    return graph._replace(nodes=nodes)\n",
    "  ################\n",
    "  return _ApplyGCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1egovRVyMUCH"
   },
   "outputs": [],
   "source": [
    "def gcn_fn(graph: jraph.GraphsTuple) -> jraph.GraphsTuple:\n",
    "  ################\n",
    "  # YOUR CODE HERE\n",
    "  # 1. Build the first GCN layer having a single non-linear projection\n",
    "  # with dimensionality 32 as update_node_fn.\n",
    "  # 2. Build the classification head of the model as a GCN layer that project nodes\n",
    "  # into 40 classes, without applying any non-linearity.\n",
    "  # HINT: Refer to last colab\n",
    "  ...\n",
    "  ################\n",
    "  return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iveXJJmjSqm-"
   },
   "source": [
    "### GAT Model for OGBN-Arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wdVrwczjSwcR"
   },
   "outputs": [],
   "source": [
    "def gat_fn(graph: jraph.GraphsTuple) -> jraph.GraphsTuple:\n",
    "  \"\"\"Defines a GAT network for the OGBN-Arxiv node classification task.\n",
    "\n",
    "  Args:\n",
    "    graph: GraphsTuple the network processes.\n",
    "\n",
    "  Returns:\n",
    "    output graph with updated node values.\n",
    "  \"\"\"\n",
    "  ################\n",
    "  # YOUR CODE HERE\n",
    "  # Implement a 2 layers GAT Network\n",
    "  # Copy code from above but this time the node_update_fn of the final layer\n",
    "  # must be a linear projection to the 40 classes.\n",
    "  ...\n",
    "  ################\n",
    "  return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pMNT0pfkMSnU"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xEaGVDDdNY-0"
   },
   "outputs": [],
   "source": [
    "n_classes = 40\n",
    "epochs = 30\n",
    "lr = 0.01\n",
    "one_hot_train_labels = jax.nn.one_hot(train_labels, n_classes)\n",
    "print(one_hot_train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TAmCyZtqOGxL"
   },
   "outputs": [],
   "source": [
    "def train_ogbn_arxiv(network, params, opt_state):\n",
    "  for epoch in range(epochs+1):\n",
    "      @jax.jit\n",
    "      def prediction_loss(params) -> jnp.ndarray:\n",
    "          out_graph = network.apply(params, graph)\n",
    "          res=  out_graph.nodes[train_idx]\n",
    "          res1=optax.softmax_cross_entropy( res, one_hot_train_labels).mean()\n",
    "          return res1\n",
    "\n",
    "      @jax.jit\n",
    "      def update(params, opt_state) -> Tuple[hk.Params, Any]:\n",
    "          loss_val, grad = jax.value_and_grad(prediction_loss)(params)\n",
    "          updates, opt_state = opt_update(grad, opt_state)\n",
    "          return optax.apply_updates(params, updates), loss_val, opt_state\n",
    "\n",
    "      # compute the loss only for the batch and update the parameters\n",
    "      params, loss_value, opt_state = update(params, opt_state)\n",
    "\n",
    "      if epoch%5==0:\n",
    "          out_graph = network.apply(params,graph)\n",
    "          acc = onp.sum(onp.argmax(out_graph.nodes[train_idx], axis=1) == train_labels)/len(train_labels)\n",
    "\n",
    "          out_graph = network.apply(params,graph)\n",
    "          val_acc = onp.sum(onp.argmax(out_graph.nodes[val_idx], axis=1) == val_labels)/len(val_labels)\n",
    "          print(f\"epoch {epoch} loss {jnp.mean(loss_value)} train acc {acc} val acc {val_acc} \")\n",
    "\n",
    "\n",
    "model_arch = \"gat\" # choices: (\"gcn\", \"gat\")\n",
    "if model_arch == \"gcn\":\n",
    "  network = hk.without_apply_rng(hk.transform(gcn_fn))\n",
    "else:  # \"gat\"\n",
    "  network = hk.without_apply_rng(hk.transform(gat_fn))\n",
    "\n",
    "params = network.init(jax.random.PRNGKey(rand_seed), graph)\n",
    "opt_init, opt_update = optax.adam(lr)\n",
    "opt_state = opt_init(params)\n",
    "\n",
    "train_ogbn_arxiv(network, params, opt_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VpBnjfSv8xJX"
   },
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XOuHdozL8xJX"
   },
   "source": [
    "**Question:** How does GCN and GAT compare on the medium-scale OGBN-Arxiv dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ECK9KGOvPSRG"
   },
   "source": [
    "**Question:** Note that a GCN epoch takes more time, why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G4-Xb68b8xJX"
   },
   "source": [
    "**Question:** How does the GCN performace change w/out self-edges? Feel free to play around with the model architecture and train longer to increase the accuracy further."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
