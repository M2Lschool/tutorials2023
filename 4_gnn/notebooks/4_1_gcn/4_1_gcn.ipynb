{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QuolPkoI7i9X"
   },
   "source": [
    "# Practical 1 - Introduction to Graph Neural Networks with jraph\n",
    "---\n",
    "\n",
    "**Tutorial overview:** In this tutorial, you will learn about the various components of Graph Neural Networks and how to implement a Graph Convolutional Network (GCN). We will train and test the GCN on a toy Karate dataset demonstrating how to train and evaluate GNNs on a node classification task.\n",
    "<br>\n",
    "\n",
    "**Tutorial outline:**\n",
    "\n",
    "- Introduction to graphs\n",
    "- Message propagation\n",
    "- Graph Convolutional Networks (GCN)\n",
    "- Node classification using GCNs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0xsQsjbk8DtK"
   },
   "source": [
    "## Theory recap\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fUomV3C9POr-"
   },
   "source": [
    "### Fundamental Graph Concepts\n",
    "A graph consists of a set of nodes and a set of edges, where edges form connections between nodes.\n",
    "\n",
    "More formally, a graph is defined as $ G = (V, E)$ where $V$ is the set of vertices / nodes, and $E$ is the set of edges.\n",
    "\n",
    "In an **undirected** graph, each edge is an unordered pair of two nodes $\\in V$. E.g. a friend network can be represented as an undirected graph, assuming that the relationship \"*A is friends with B*\" implies \"*B is friends with A*\".\n",
    "\n",
    "In a **directed** graph, each edge is an ordered pair of nodes $\\in V$. E.g. a citation network would be best represented with a directed graph, since the relationship \"*A cites B*\" does not imply \"*B cites A*\".\n",
    "\n",
    "The **degree** of a node is defined as the number of edges incident on it, i.e. the sum of incoming and outgoing edges for that node.\n",
    "\n",
    "The **in-degree** is the sum of incoming edges only, and the **out-degree** is the sum of outgoing edges only.\n",
    "\n",
    "There are several ways to represent $E$:\n",
    "1. As a **list of edges**: a list of pairs $(u,v)$, where $(u,v)$ means that there is an edge going from node $u$ to node $v$. <mark>NOTE: in Jraph we are going to use a similar structure!</mark>\n",
    "2. As an **adjacency matrix**: a binary square matrix $A$ of size $|V| \\times |V|$, where $A_{u,v}=1$ iff there is a connection between nodes $u$ and $v$.\n",
    "3. As an **adjacency list**: An array of $|V|$ unordered lists, where the $i$th list corresponds to the $i$th node, and contains all the nodes directly connected to node $i$.\n",
    "\n",
    "---\n",
    "\n",
    "Example: Below is a directed graph with four nodes and five edges.\n",
    "\n",
    "<center><image src=\"https://storage.googleapis.com/dm-educational/assets/graph-nets/toy_graph.png\" width=\"400px\"></center>\n",
    "\n",
    "The arrows on the edges indicate the direction of each edge, e.g. there is an edge going from node 0 to node 1. Between node 0 and node 3, there are two edges: one going from node 0 to node 3 and one from node 3 to node 0.\n",
    "\n",
    "Node 0 has out-degree of 2, since it has two outgoing edges, and an in-degree of 2, since it has two incoming edges.\n",
    "\n",
    "The list of edges is:\n",
    "$$[(0, 1), (0, 3), (1, 2), (2, 0), (3, 0)]$$\n",
    "\n",
    "As adjacency matrix:\n",
    "\n",
    "$$\\begin{array}{l|llll}\n",
    " source \\setminus dest    & n_0 & n_1 & n_2 & n_3 \\\\ \\hline\n",
    "n_0 & 0    & 1    & 0    & 1    \\\\\n",
    "n_1 & 0    & 0    & 1    & 0    \\\\\n",
    "n_2 & 1    & 0    & 0    & 0    \\\\\n",
    "n_3 & 1    & 0    & 0    & 0\n",
    "\\end{array}$$\n",
    "\n",
    "As adjacency list:\n",
    "\n",
    "$$[\\{1, 3\\}, \\{2\\}, \\{0\\}, \\{0\\}]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zy0VRbrTNafY"
   },
   "source": [
    "\n",
    "### Graph Prediction Tasks\n",
    "What are the kinds of problems we want to solve on graphs?\n",
    "\n",
    "\n",
    "The tasks fall into roughly three categories:\n",
    "\n",
    "1. **Node Classification**: E.g. what is the topic of a paper given a citation network of papers?\n",
    "2. **Link Prediction / Edge Classification**: E.g. are two people in a social network friends?\n",
    "3. **Graph Classification**: E.g. is this protein molecule (represented as a graph) likely going to be effective?\n",
    "\n",
    "<center><image src=\"https://storage.googleapis.com/dm-educational/assets/graph-nets/graph_tasks.png\" width=\"700px\"></center>\n",
    "\n",
    "*The three main graph learning tasks. Image source: Petar Veličković.*\n",
    "\n",
    "Which examples of graph prediction tasks come to your mind? Which task types do they correspond to?\n",
    "\n",
    "We will create and train models on all three task types in this tutorial.\n",
    "\n",
    "<!--\n",
    "If applicable, re-iterate on the maths, schematics and high-level ideas that are necessary in the subsequent Practicals. Although we would expect the Lectures to cover the theory in-depth, we should aim to make the tutorials self-contained. However, feel free to add relevant resources as links for further reading for completeness.\n",
    "\n",
    "This should not be the main focus initially - best to first work on the Practicals and code, and then write the Theory that is needed. -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RbgVROxnPIl3"
   },
   "source": [
    "### Graph Convolutional Network (GCN) Layer\n",
    "\n",
    "A GCN aims to emulate the learning process that a traditional convolutional neural network (CNN) does on images. However, differently from images, the structure of graphs is not constrained to a regular grid, and this prevents the use of traditional convolutional filters.\n",
    "\n",
    "The regularity of grids reflects two properties that are crucial to applying traditional convolutional filters: (i) pixels are ordered, (ii) pixels have fixed-size neighborhoods (each pixel is surrounded by 8 pixels). Because of these properties, the content of images is *invariant* or *equivariant* to translation (shift operation), allowing the shifting of a local convolutional filter (e.g., 3x3) across the different parts of a larger image (e.g., 64x64). Note that a filter performs a weighted sum aggregation over the inputs (pixels) with learnable weights, where each filter input has its own weight.\n",
    "\n",
    "In graphs, there is no ordering of nodes, and the edges determine the size of each node neighborhood. Hence, we cannot learn a weighted sum of the neighbors as such weights depend on the neighbors' order that is not present. Instead, we need to apply a learnable filter invariant to the neighbors' order. The simplest solution is to apply the same weight to all neighbors without learning the aggregation function. In other words, each neighbor contributes equally. This type of convolutional filter identifies GCNs that are *isotropic*.\n",
    "\n",
    "\n",
    "<center><image src=\"https://storage.googleapis.com/dm-educational/assets/graph-nets/cnn_vs_gnn.png\" width=\"400px\"></center>\n",
    "\n",
    "Comparison of CNN and GCN filters.\n",
    "Image source: https://arxiv.org/pdf/1901.00596.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FcLxmzi0W9yw"
   },
   "source": [
    "### Message-Passing\n",
    "GCNs can be defined using different paradigms. The most widely adopted and easy to understand is *Message-Passing (MP)*. The basic idea of MP is to perform local convolution through two steps:\n",
    "\n",
    "1. _Compute messages / update node features_: Create a feature vector $\\vec{h}_n$ for each node $n$ (e.g. with an MLP). This is going to be the message that this node will pass to neighboring nodes.\n",
    "2. _Message-passing / aggregate node features_: For each node, calculate a new feature vector $\\vec{h}'_n$ based on the messages (features) from the nodes in its neighborhood. In a directed graph, only nodes from incoming edges are counted as neighbors. The image below shows this aggregation step. There are multiple options for aggregation; in *isotropic* GCNs, common aggregations are the mean, the sum, the min, or the max, while in *anisotropic* GCNs, the aggregation depends on variable edge weights (either learned or fixed).\n",
    "\n",
    "<center><image src=\"https://storage.googleapis.com/dm-educational/assets/graph-nets/graph_conv.png\" width=\"500px\"></center>\n",
    "\n",
    "*\\\"A generic overview of a graph convolution operation, highlighting the relevant information for deriving the next-level features for every node in the graph.\\\"* Image source: Petar Veličković (https://github.com/PetarV-/TikZ)\n",
    "\n",
    "In the following steps, we will implement the Graph Convolutional Network as introduced by Kipf et al. (2017) in https://arxiv.org/abs/1609.02907, which is one of the basic graph network architectures. We start by building its core building block, the graph convolutional layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SCk2L626WV_W"
   },
   "source": [
    "### Symmetric normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fuCypzq3UlEi"
   },
   "source": [
    "\n",
    "Graph nodes may have different numbers of neighbors/degrees, possibly leading to instabilities during neural network training, e.g., exploding or vanishing gradients. To address that, normalization is a commonly used method. In this case, we will normalize by node degrees.\n",
    "\n",
    "As a first attempt, we could count the number of incoming edges (including self-edge) and divide by that value.\n",
    "\n",
    "More formally, let $A$ be the adjacency matrix defining the edges of the graph.\n",
    "\n",
    "Then we define the degree matrix $D$ as a diagonal matrix with $D_{ii} = \\sum_jA_{ij}$ (the degree of node $i$)\n",
    "\n",
    "\n",
    "Now we can normalize $AH$ by dividing it by the node degrees:\n",
    "$${D}^{-1}AH$$\n",
    "Note that, with this normalization, we are computing the mean over the neighbors' messages.\n",
    "\n",
    "Instead, if we want to compute a more dynamic aggregation than the mere mean, we can take into\n",
    "account both the in and out degrees by computing a *symmetric normalization*, which is also what Kipf and Welling proposed in their [paper](https://arxiv.org/abs/1609.02907):\n",
    "$$D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}}H$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "puYBnCdfXrIk"
   },
   "source": [
    "## Implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cdptlndy2eqn"
   },
   "source": [
    "### Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oxZ9FwjUZouW"
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/deepmind/jraph.git\n",
    "!pip install flax\n",
    "!pip install dm-haiku\n",
    "!pip install networkx\n",
    "!pip install ogb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-lrshhVob7z6"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "%matplotlib inline\n",
    "import functools\n",
    "import matplotlib.pyplot as plt\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.tree_util as tree\n",
    "import jraph\n",
    "import flax\n",
    "import haiku as hk\n",
    "import optax\n",
    "import pickle\n",
    "import numpy as onp\n",
    "import networkx as nx\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kxD2FnvmTHbn"
   },
   "source": [
    "---\n",
    "## Intro to the jraph Library\n",
    "\n",
    "In the following sections, we will learn how to represent graphs and build GNNs in Python. We will use\n",
    "[jraph](https://github.com/deepmind/jraph), a lightweight library for working with GNNs in [JAX](https://github.com/google/jax).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7DKgb-NbE6Z"
   },
   "source": [
    "\n",
    "#### Representing a graph in jraph\n",
    "\n",
    "In jraph, a graph is represented with a `GraphsTuple` object. In addition to defining the graph structure of nodes and edges, you can also store node features, edge features and global graph features in a `GraphsTuple`.\n",
    "\n",
    "In the `GraphsTuple`, edges are represented in two aligned arrays of node indices: senders (source nodes) and receivers (destinaton nodes).\n",
    "Each index corresponds to one edge, e.g. edge `i` goes from `senders[i]` to `receivers[i]`. This corresponds to concatenating all the $(u, v)$ tuples, and then unzipping them into separete lists (`senders` are all the $u$, `receivers` are all the $v$). Each list will be stored in a JAX array of shape $1 \\times |E|$. Note that this way of representing the graph takes advantage of graph **sparsity** (the space required for `senders` and `receivers` is usually significantly lower than the space needed for the full adjacency matrix $|V| \\times |V|$).\n",
    "\n",
    "You can even store multiple graphs in one `GraphsTuple` object.\n",
    "\n",
    "We will start with creating a simple directed graph with 4 nodes and 5 edges. We will also add toy features to the nodes, using `2*node_index` as the feature.\n",
    "\n",
    "We will later use this toy graph in the GCN demo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BrLLaK1iFA3t"
   },
   "outputs": [],
   "source": [
    "def build_toy_graph() -> jraph.GraphsTuple:\n",
    "  \"\"\"Define a four node graph, each node has a scalar as its feature.\"\"\"\n",
    "\n",
    "  # Nodes are defined implicitly by their features.\n",
    "  # We will add four nodes, each with a feature, e.g.\n",
    "  # node 0 has feature [0.],\n",
    "  # node 1 has featre [2.] etc.\n",
    "  # len(node_features) is the number of nodes.\n",
    "  node_features = jnp.array([[0.], [2.], [4.], [6.]])\n",
    "\n",
    "  # We will now specify 5 directed edges connecting the nodes we defined above.\n",
    "  # We define this with `senders` (source node indices) and `receivers`\n",
    "  # (destination node indices).\n",
    "  # For example, to add an edge from node 0 to node 1, we append 0 to senders,\n",
    "  # and 1 to receivers.\n",
    "  # We can do the same for all 5 edges:\n",
    "  # 0 -> 1\n",
    "  # 1 -> 2\n",
    "  # 2 -> 0\n",
    "  # 3 -> 0\n",
    "  # 0 -> 3\n",
    "  senders = jnp.array([0, 1, 2, 3, 0])\n",
    "  receivers = jnp.array([1, 2, 0, 0, 3])\n",
    "\n",
    "  # You can optionally add edge attributes to the 5 edges.\n",
    "  edges = jnp.array([[5.], [6.], [7.], [8.], [8.]])\n",
    "\n",
    "  # We then save the number of nodes and the number of edges.\n",
    "  # This information is used to make running GNNs over multiple graphs\n",
    "  # in a GraphsTuple possible.\n",
    "  n_node = jnp.array([4])\n",
    "  n_edge = jnp.array([5])\n",
    "\n",
    "  # Optionally you can add `global` information, such as a graph label.\n",
    "  global_context = jnp.array([[1]]) # Same feature dims as nodes and edges.\n",
    "  graph = jraph.GraphsTuple(\n",
    "      nodes=node_features,\n",
    "      edges=edges,\n",
    "      senders=senders,\n",
    "      receivers=receivers,\n",
    "      n_node=n_node,\n",
    "      n_edge=n_edge,\n",
    "      globals=global_context\n",
    "      )\n",
    "  return graph\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qC5dS4J8cASF"
   },
   "outputs": [],
   "source": [
    "graph = build_toy_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5PQbh1BLhHB4"
   },
   "source": [
    "**Question**: How many nodes does our graph have? Do all nodes have features? How many ? What about edges ? Do we graph-level features?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ieQyV_icLcs"
   },
   "source": [
    "#### Visualizing the Graph\n",
    "To visualize the graph structure of the graph we created above, we will use the [`networkx`](networkx.org) library because it already has functions for drawing graphs.\n",
    "\n",
    "We first convert the `jraph.GraphsTuple` to a `networkx.DiGraph`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xNWZQ7yPcL7s"
   },
   "outputs": [],
   "source": [
    "# Utils for visualization\n",
    "def convert_jraph_to_networkx_graph(jraph_graph: jraph.GraphsTuple) -> nx.Graph:\n",
    "  nodes, edges, receivers, senders, _, _, _ = jraph_graph\n",
    "  nx_graph = nx.DiGraph()\n",
    "  if nodes is None:\n",
    "    for n in range(jraph_graph.n_node[0]):\n",
    "      nx_graph.add_node(n)\n",
    "  else:\n",
    "    for n in range(jraph_graph.n_node[0]):\n",
    "      nx_graph.add_node(n, node_feature=nodes[n])\n",
    "  if edges is None:\n",
    "    for e in range(jraph_graph.n_edge[0]):\n",
    "      nx_graph.add_edge(int(senders[e]), int(receivers[e]))\n",
    "  else:\n",
    "    for e in range(jraph_graph.n_edge[0]):\n",
    "      nx_graph.add_edge(\n",
    "          int(senders[e]), int(receivers[e]), edge_feature=edges[e])\n",
    "  return nx_graph\n",
    "\n",
    "def draw_jraph_graph_structure(jraph_graph: jraph.GraphsTuple) -> None:\n",
    "  nx_graph = convert_jraph_to_networkx_graph(jraph_graph)\n",
    "  pos = nx.spring_layout(nx_graph)\n",
    "  nx.draw(\n",
    "      nx_graph, pos=pos, with_labels=True, node_size=500, font_color='yellow')\n",
    "\n",
    "draw_jraph_graph_structure(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JLifmSL8gv2A"
   },
   "source": [
    "**Question**: Why is there only one arrow per edge ? What kind of matrix would have arrows in both sides ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s5VCpHT1E_-F"
   },
   "source": [
    "#### Helpers functions: `segment_sum`, `segment_max`\n",
    "Before starting with the GCN implementation, let's take some time to familiarize with JAX functions that we need for Jraph sparse graph representation. Specifically, we inspect `jax.ops.segment_<fn>` functions, e.g., `segment_sum` and `segment_max`.\n",
    "These functions allows the application of `<fn>` to chunks (segments) of data present in a single array. They have two arguments, the array on which we want to apply `<fn>`, and a vector of the same size defining the segments; each segment is identified by an integer value.\n",
    "\n",
    "**SPOILER ALERT** This function is useful for us to to aggregate information for the neighbors of a node! This is an essential step for Message propagation.\n",
    "\n",
    "See the following examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c-DxTxM8dB8h"
   },
   "outputs": [],
   "source": [
    "# -- DOCUMENTATION\n",
    "# Check how `jax.ops.segment_sum` can simplify your life with jraph sparse structures!\n",
    "# https://jax.readthedocs.io/en/latest/_autosummary/jax.ops.segment_sum.html\n",
    "# See also TF documentation for a visual illutration:\n",
    "# https://www.tensorflow.org/api_docs/python/tf/math/segment_sum\n",
    "data = jnp.arange(5) #  Array([0, 1, 2, 3, 4])\n",
    "segment_ids = jnp.array([0, 0, 1, 1, 2])\n",
    "jax.ops.segment_sum(data, segment_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oBUKe7BQfrbE"
   },
   "outputs": [],
   "source": [
    "# -- EXAMPLE: segment_sum\n",
    "\n",
    "# let's define a simple vector of 10 values\n",
    "dummy_v = jnp.arange(10)\n",
    "\n",
    "# now let's create three segments for the array dummy_v\n",
    "dummy_seg = jnp.array([0,0,0,0,1,1,1,2,2,2])\n",
    "\n",
    "# now we apply the summation over segments\n",
    "print(jax.ops.segment_sum(dummy_v, dummy_seg, num_segments=3))\n",
    "\n",
    "# note that the segments vector does not have to be sorted nor\n",
    "# the segment positions need to be consecutives\n",
    "dummy_seg = jnp.array([0,1,1,2,1,1,1,2,2,0])\n",
    "\n",
    "# now we apply the summation over segments\n",
    "print(jax.ops.segment_sum(dummy_v, dummy_seg, num_segments=3))\n",
    "\n",
    "# -- EXAMPLE: segment_max #\n",
    "logits = jnp.arange(5)\n",
    "segment_ids = jnp.array([0, 0, 0, 1, 1])\n",
    "print(jax.ops.segment_max(logits, segment_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5PPC2nWqOix1"
   },
   "source": [
    "### Implement a Graph Convolutional Network (GCN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PeAiCfYyjqnC"
   },
   "source": [
    "#### Simple GCN Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LyVrZLZ2TeLc"
   },
   "source": [
    "Now let us implement our first graph convolutional network (GCN)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xsO_WBCMOiDd"
   },
   "outputs": [],
   "source": [
    "def apply_simplified_gcn(graph: jraph.GraphsTuple) -> jraph.GraphsTuple:\n",
    "  # Unpack GraphsTuple\n",
    "  nodes, _, receivers, senders, _, _, _ = graph\n",
    "\n",
    "  # 1. Update node features (compute messages)\n",
    "  # For simplicity, we will first use an identify function here,\n",
    "  # and replace it with a trainable MLP block later.\n",
    "  update_node_fn = lambda nodes: nodes\n",
    "  nodes = update_node_fn(nodes)\n",
    "\n",
    "  # 2. Aggregate node features over nodes in neighborhood (Message-Passing)\n",
    "  # Equivalent to jnp.sum(n_node), but jittable\n",
    "  total_num_nodes = tree.tree_leaves(nodes)[0].shape[0]\n",
    "  aggregate_nodes_fn = jax.ops.segment_sum\n",
    "\n",
    "  # Compute new node features by aggregating messages from neighboring nodes\n",
    "  # -- take all nodes sending a message with x[senders],\n",
    "  # -- and aggregate by receiver with `aggregate_nodes_fn`\n",
    "\n",
    "  ################\n",
    "  # YOUR CODE HERE\n",
    "\n",
    "  # HINT please take a look at tree.tree_map and fill in the variables\n",
    "  # nodes = tree.tree_map(lambda x: aggregate_nodes_fn(???, ???, ???), nodes)\n",
    "  nodes = ...\n",
    "  ################\n",
    "\n",
    "  # Create a new graph with same structure and the updated nodes representation\n",
    "  out_graph = graph._replace(nodes=nodes)\n",
    "  return out_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WC5m-BmjP81R"
   },
   "outputs": [],
   "source": [
    "graph = build_toy_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HI16Dom1P_SE"
   },
   "outputs": [],
   "source": [
    "draw_jraph_graph_structure(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pbOlLBQnQBAd"
   },
   "outputs": [],
   "source": [
    "out_graph = apply_simplified_gcn(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hir-kjzWQDKA"
   },
   "source": [
    "Since we used the identity function for updating nodes and sum aggregation, we can verify the results pretty easily. As a reminder, in this toy graph, the node features are the same as the node index.\n",
    "\n",
    "Node 0: sum of features from node 2 and node 3 $\\rightarrow$ 10.\n",
    "\n",
    "Node 1: sum of features from node 0 $\\rightarrow$ 0.\n",
    "\n",
    "Node 2: sum of features from node 1 $\\rightarrow$ 2.\n",
    "\n",
    "Node 3: sum of features from node 0 $\\rightarrow$ 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DOx-SZDoQF7E"
   },
   "outputs": [],
   "source": [
    "out_graph.nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VSkciGUxQJqf"
   },
   "source": [
    "#### Add Trainable Parameters to GCN layer\n",
    "So far our graph convolution operation doesn't have any learnable parameters.\n",
    "Let's add an MLP block to the update function to make it trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "za2d_okZQJ_-"
   },
   "outputs": [],
   "source": [
    "class MLP(hk.Module):\n",
    "  def __init__(self, features: jnp.ndarray):\n",
    "    super().__init__()\n",
    "    self.features = features\n",
    "\n",
    "  def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n",
    "    layers = []\n",
    "    for feat in self.features[:-1]:\n",
    "      layers.append(hk.Linear(feat))\n",
    "      layers.append(jax.nn.relu)\n",
    "    layers.append(hk.Linear(self.features[-1]))\n",
    "    mlp = hk.Sequential(layers)\n",
    "    return mlp(x)\n",
    "\n",
    "################\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# HINT: instantiate an MLP with dimensions [8,4] and create a function from it\n",
    "# update_node_fn = lambda x: ???\n",
    "update_node_fn = ...\n",
    "################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0zYRKC69Qbh1"
   },
   "source": [
    "#### Check outputs of `update_node_fn` with MLP Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8knuZjFXQd4W"
   },
   "outputs": [],
   "source": [
    "graph = build_toy_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0c8IpqvVQh4n"
   },
   "outputs": [],
   "source": [
    "update_node_module = hk.without_apply_rng(hk.transform(update_node_fn))\n",
    "params = update_node_module.init(jax.random.PRNGKey(42), graph.nodes)\n",
    "out = update_node_module.apply(params, graph.nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9mqePnXQkhW"
   },
   "source": [
    "As output, we expect the updated node features. We should see one array of dim 4 for each of the 4 nodes, which is the result of applying a single MLP block to the features of each node individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VfvlfIq5QnH-"
   },
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3FNFeygGQrg-"
   },
   "source": [
    "#### Add Self-Edges (Edges connecting a node to itself)\n",
    "For each node, add an edge of the node onto itself. This way, nodes will include themselves in the aggregation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ugQe7d8DQsKH"
   },
   "outputs": [],
   "source": [
    "def add_self_edges_fn(receivers: jnp.ndarray, senders: jnp.ndarray,\n",
    "                      total_num_nodes: int) -> Tuple[jnp.ndarray, jnp.ndarray]:\n",
    "  \"\"\"Adds self edges. Assumes self edges are not in the graph yet.\"\"\"\n",
    "  ################\n",
    "  # YOUR CODE HERE\n",
    "  # for each node, add a connection to itself, both from the sender and receiver perspective\n",
    "  # you can easily implement it in a pythonic and vectorized way, by knowing `total_num_nodes`.\n",
    "\n",
    "  # Hint: Use jnp.concatenate to add indices [0, 1, ... `total_num_nodes-1`] to existing receivers and senders.\n",
    "  receivers = ...\n",
    "  senders = ...\n",
    "  ################\n",
    "  return receivers, senders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jRLc9R_5QvVh"
   },
   "source": [
    "#### Add Symmetric Normalization\n",
    "\n",
    "**Implementation tip**:\n",
    "Keep in mind that we are using sparse matrix operations; thus, we never explicit the adjacency matrix $A$ in its dense form.\n",
    "We will directly pre-normalize the feature of each node present in each edge by the square root of the in-degrees, and later we will post-normalize by the square root of the out-degrees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3bXXzfS_Qybj"
   },
   "source": [
    "#### General GCN Layer\n",
    "Now we can write a more general and configurable version of the Graph Convolution layer, allowing the caller to specify:\n",
    "\n",
    "*   **`update_node_fn`**: Function to use to update node features (e.g. the MLP block version we just implemented)\n",
    "*   **`aggregate_nodes_fn`**: Aggregation function to use to aggregate messages from neighbourhood.\n",
    "*  **`add_self_edges`**: Whether to add self edges for aggregation step.\n",
    "* **`symmetric_normalization`**: Whether to add symmetric normalization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w2MfUTRDQ0Kp"
   },
   "outputs": [],
   "source": [
    "# Adapted from https://github.com/deepmind/jraph/blob/master/jraph/_src/models.py#L506\n",
    "def GraphConvolution(update_node_fn: Callable,\n",
    "                     aggregate_nodes_fn: Callable = jax.ops.segment_sum,\n",
    "                     add_self_edges: bool = False,\n",
    "                     symmetric_normalization: bool = True) -> Callable:\n",
    "  \"\"\"Returns a method that applies a Graph Convolution layer.\n",
    "\n",
    "  Graph Convolutional layer as in https://arxiv.org/abs/1609.02907,\n",
    "  NOTE: This implementation does not add an activation after aggregation.\n",
    "  If you are stacking layers, you may want to add an activation between\n",
    "  each layer.\n",
    "  Args:\n",
    "    update_node_fn: function used to update the nodes. In the paper a single\n",
    "      layer MLP is used.\n",
    "    aggregate_nodes_fn: function used to aggregates the sender nodes.\n",
    "    add_self_edges: whether to add self edges to nodes in the graph as in the\n",
    "      paper definition of GCN. Defaults to False.\n",
    "    symmetric_normalization: whether to use symmetric normalization. Defaults to\n",
    "      True.\n",
    "\n",
    "  Returns:\n",
    "    A method that applies a Graph Convolution layer.\n",
    "  \"\"\"\n",
    "\n",
    "  def _ApplyGCN(graph: jraph.GraphsTuple) -> jraph.GraphsTuple:\n",
    "    \"\"\"Applies a Graph Convolution layer.\"\"\"\n",
    "    nodes, _, receivers, senders, _, _, _ = graph\n",
    "\n",
    "    ################\n",
    "    # YOUR CODE HERE\n",
    "    # Pass the nodes through the `update_node_fn`.\n",
    "    nodes = ...\n",
    "    ################\n",
    "\n",
    "    # Equivalent to jnp.sum(n_node), but jittable\n",
    "    total_num_nodes = tree.tree_leaves(nodes)[0].shape[0]\n",
    "    # --\n",
    "\n",
    "    if add_self_edges:\n",
    "      # We add self edges to the senders and receivers so that each node\n",
    "      # includes itself in aggregation.\n",
    "      # In principle, a `GraphsTuple` should partition by n_edge, but in\n",
    "      # this case it is not required since a GCN is agnostic to whether\n",
    "      # the `GraphsTuple` is a batch of graphs or a single large graph.\n",
    "\n",
    "      ################\n",
    "      # YOUR CODE HERE\n",
    "      # Pass the receiver and sender nodes through `add_self_edges_fn`\n",
    "      conv_receivers, conv_senders = ...\n",
    "      ################\n",
    "    else:\n",
    "      # Do nothing if no self edges to be added.\n",
    "      conv_senders = senders\n",
    "      conv_receivers = receivers\n",
    "\n",
    "    # pylint: disable=g-long-lambda\n",
    "    if symmetric_normalization:\n",
    "      # Calculate the normalization values.\n",
    "\n",
    "      # -- Compute in-degree (sender_degree) and out-degree (receiver_degree) of each node\n",
    "      # -- Remember: The in-degree is the sum of incoming edges only,\n",
    "      # and the out-degree is the sum of outgoing edges only.\n",
    "\n",
    "      ################\n",
    "      # YOUR CODE HERE\n",
    "      # Tip: all the operations should be jittable!\n",
    "      # -- take again advantage of `jax.ops.segment_sum`\n",
    "\n",
    "      # HINT: count_edges_fn = lambda x: jax.ops.segment_sum(\n",
    "      #    jnp.ones_like(conv_senders), ???, ???)\n",
    "      count_edges_fn = ...\n",
    "      ################\n",
    "\n",
    "      sender_degree = count_edges_fn(conv_senders)\n",
    "      receiver_degree = count_edges_fn(conv_receivers)\n",
    "\n",
    "      # 1. Pre normalize by sqrt sender degree:\n",
    "      # Avoid dividing by 0 by taking maximum of (degree, 1).\n",
    "      # check https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.rsqrt.html\n",
    "      nodes = tree.tree_map(\n",
    "          lambda x: x * jax.lax.rsqrt(jnp.maximum(sender_degree, 1.0))[:, None],\n",
    "          nodes,\n",
    "      )\n",
    "\n",
    "      ################\n",
    "      # YOUR CODE HERE\n",
    "      # 2. Aggregate the pre-normalized nodes.\n",
    "      # HINT: Revist the aggregation function above.\n",
    "      nodes = ...\n",
    "      ################\n",
    "\n",
    "      # 3. Post normalize by sqrt receiver degree.\n",
    "      # Avoid dividing by 0 by taking maximum of (degree, 1).\n",
    "      nodes = tree.tree_map(\n",
    "          lambda x:\n",
    "          (x * jax.lax.rsqrt(jnp.maximum(receiver_degree, 1.0))[:, None]),\n",
    "          nodes,\n",
    "      )\n",
    "\n",
    "    else:\n",
    "      # If no normalization is applied, just aggregate\n",
    "\n",
    "      ################\n",
    "      # YOUR CODE HERE\n",
    "      # HINT: Same implementation as the aggregation functon above.\n",
    "      nodes = ...\n",
    "      ################\n",
    "\n",
    "    return graph._replace(nodes=nodes)\n",
    "\n",
    "  return _ApplyGCN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M80oxWgtQ6UX"
   },
   "source": [
    "#### Test General GCN Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ouOFThinQ7LE"
   },
   "outputs": [],
   "source": [
    "gcn_layer = GraphConvolution(\n",
    "    update_node_fn=lambda n: MLP(features=[8, 4])(n),\n",
    "    aggregate_nodes_fn=jax.ops.segment_sum,\n",
    "    add_self_edges=True,\n",
    "    symmetric_normalization=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y5g7u5QTQ9sh"
   },
   "outputs": [],
   "source": [
    "graph = build_toy_graph()\n",
    "network = hk.without_apply_rng(hk.transform(gcn_layer))\n",
    "params = network.init(jax.random.PRNGKey(42), graph)\n",
    "out_graph = network.apply(params, graph)\n",
    "out_graph.nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8x8AI7xjRBuE"
   },
   "source": [
    "#### Build GCN Model with Multiple Layers\n",
    "With a single GCN layer, a node's representation after the GCN layer is only\n",
    "influenced by its direct neighbourhood. However, we may want to consider larger neighbourhoods, i.e. more than just 1 hop away. To achieve that, we can stack\n",
    "multiple GCN layers, similar to how stacking CNN layers expands the input region.\n",
    "\n",
    "We will define a network with two GCN layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ubgYLGGRG1a"
   },
   "outputs": [],
   "source": [
    "def gcn_fn(graph: jraph.GraphsTuple) -> jraph.GraphsTuple:\n",
    "  \"\"\"Defines a graph neural network with 3 GCN layers.\n",
    "  Args:\n",
    "    graph: GraphsTuple the network processes.\n",
    "\n",
    "  Returns:\n",
    "    output graph with updated node values.\n",
    "  \"\"\"\n",
    "\n",
    "  ################\n",
    "  # YOUR CODE HERE\n",
    "  # 1. Build the first GCN layer having a single non-linear projection\n",
    "  # with dimensionality 8 as update_node_fn.\n",
    "  # gn = GraphConvolution(update_node_fn=???, add_self_edges=???)\n",
    "  gn = ...\n",
    "  graph = gn(graph)\n",
    "\n",
    "  # 2. Build the classification head of the model as a GCN layer that project nodes\n",
    "  # into 2 classes, without applying any non-linearity.\n",
    "  # gn = GraphConvolution(update_node_fn=???)\n",
    "  gn = ...\n",
    "  graph = gn(graph)\n",
    "\n",
    "  ################\n",
    "  return graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jQn-Mpt2RLjQ"
   },
   "outputs": [],
   "source": [
    "graph = build_toy_graph()\n",
    "network = hk.without_apply_rng(hk.transform(gcn_fn))\n",
    "params = network.init(jax.random.PRNGKey(42), graph)\n",
    "out_graph = network.apply(params, graph)\n",
    "out_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z2aZMHWCVYTD"
   },
   "outputs": [],
   "source": [
    "out_graph.nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HTRAUS7xj5m-"
   },
   "source": [
    "### Node Classification: Train the GCN on a tiny Karate club dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PD6WUk6YkHDn"
   },
   "source": [
    "#### Zachary's Karate Club Dataset\n",
    "\n",
    "[Zachary's karate club](https://en.wikipedia.org/wiki/Zachary%27s_karate_club) is a small dataset commonly used as an example for a social graph. It's great for demo purposes, as it's easy to visualize and quick to train a model on it.\n",
    "\n",
    "A node represents a student or instructor in the club. An edge means that those two people have interacted outside of the class. There are two instructors in the club.\n",
    "\n",
    "Each student is assigned to one of two instructors.\n",
    "\n",
    "#### Optimizing the GCN on the Karate Club Node Classification Task\n",
    "\n",
    "The task is to predict the assignment of students to instructors, given the social graph and only knowing the assignment of two nodes (the two instructors) a priori.\n",
    "\n",
    "In other words, out of the 34 nodes, only two nodes are labeled, and we are trying to optimize the assignment of the other 32 nodes, by **maximizing the log-likelihood of the two known node assignments**.\n",
    "\n",
    "We will compute the accuracy of our node assignments by comparing to the ground-truth assignments. **Note that the ground-truth for the 32 student nodes is not used in the loss function itself.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g7DoCGSuj8EA"
   },
   "outputs": [],
   "source": [
    "\"\"\"Zachary's karate club example.\n",
    "From https://github.com/deepmind/jraph/blob/master/jraph/examples/zacharys_karate_club.py.\n",
    "Here we train a graph neural network to process Zachary's karate club.\n",
    "https://en.wikipedia.org/wiki/Zachary%27s_karate_club\n",
    "Zachary's karate club is used in the literature as an example of a social graph.\n",
    "Here we use a graphnet to optimize the assignments of the students in the\n",
    "karate club to two distinct karate instructors (Mr. Hi and John A).\n",
    "\"\"\"\n",
    "\n",
    "def get_zacharys_karate_club() -> jraph.GraphsTuple:\n",
    "  \"\"\"Returns GraphsTuple representing Zachary's karate club.\"\"\"\n",
    "  social_graph = [\n",
    "      (1, 0), (2, 0), (2, 1), (3, 0), (3, 1), (3, 2),\n",
    "      (4, 0), (5, 0), (6, 0), (6, 4), (6, 5), (7, 0), (7, 1),\n",
    "      (7, 2), (7, 3), (8, 0), (8, 2), (9, 2), (10, 0), (10, 4),\n",
    "      (10, 5), (11, 0), (12, 0), (12, 3), (13, 0), (13, 1), (13, 2),\n",
    "      (13, 3), (16, 5), (16, 6), (17, 0), (17, 1), (19, 0), (19, 1),\n",
    "      (21, 0), (21, 1), (25, 23), (25, 24), (27, 2), (27, 23),\n",
    "      (27, 24), (28, 2), (29, 23), (29, 26), (30, 1), (30, 8),\n",
    "      (31, 0), (31, 24), (31, 25), (31, 28), (32, 2), (32, 8),\n",
    "      (32, 14), (32, 15), (32, 18), (32, 20), (32, 22), (32, 23),\n",
    "      (32, 29), (32, 30), (32, 31), (33, 8), (33, 9), (33, 13),\n",
    "      (33, 14), (33, 15), (33, 18), (33, 19), (33, 20), (33, 22),\n",
    "      (33, 23), (33, 26), (33, 27), (33, 28), (33, 29), (33, 30),\n",
    "      (33, 31), (33, 32)]\n",
    "  # Add reverse edges.\n",
    "  social_graph += [(edge[1], edge[0]) for edge in social_graph]\n",
    "  n_club_members = 34\n",
    "\n",
    "  return jraph.GraphsTuple(\n",
    "      n_node=jnp.asarray([n_club_members]),\n",
    "      n_edge=jnp.asarray([len(social_graph)]),\n",
    "      # One-hot encoding for nodes, i.e. argmax(nodes) = node index.\n",
    "      nodes=jnp.eye(n_club_members),\n",
    "      # No edge features.\n",
    "      edges=None,\n",
    "      globals=None,\n",
    "      senders=jnp.asarray([edge[0] for edge in social_graph]),\n",
    "      receivers=jnp.asarray([edge[1] for edge in social_graph]))\n",
    "\n",
    "def get_ground_truth_assignments_for_zacharys_karate_club() -> jnp.ndarray:\n",
    "  \"\"\"Returns ground truth assignments for Zachary's karate club.\"\"\"\n",
    "  return jnp.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n",
    "                    0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s8CTfR0bldus"
   },
   "outputs": [],
   "source": [
    "graph = get_zacharys_karate_club()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gl6SmYoVlfzU"
   },
   "outputs": [],
   "source": [
    "print(f'Number of nodes: {graph.n_node[0]}')\n",
    "print(f'Number of edges: {graph.n_edge[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UyAd6cxLlji4"
   },
   "source": [
    "Visualize the karate club graph with circular node layout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Aa8j-7QRliZu"
   },
   "outputs": [],
   "source": [
    "nx_graph = convert_jraph_to_networkx_graph(graph)\n",
    "pos = nx.circular_layout(nx_graph)\n",
    "plt.figure(figsize=(6, 6))\n",
    "nx.draw(nx_graph, pos=pos, with_labels = True, node_size=500, font_color='yellow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dAFLZxpFloVq"
   },
   "source": [
    "Training and evaluation code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FCYqTnDUlqET"
   },
   "outputs": [],
   "source": [
    "def optimize_club(network: hk.Transformed, num_steps: int) -> jnp.ndarray:\n",
    "  \"\"\"Solves the karate club problem by optimizing the assignments of students.\"\"\"\n",
    "  zacharys_karate_club = get_zacharys_karate_club()\n",
    "  labels = get_ground_truth_assignments_for_zacharys_karate_club()\n",
    "  params = network.init(jax.random.PRNGKey(42), zacharys_karate_club)\n",
    "\n",
    "  @jax.jit\n",
    "  def predict(params: hk.Params) -> jnp.ndarray:\n",
    "    decoded_graph = network.apply(params, zacharys_karate_club)\n",
    "    return jnp.argmax(decoded_graph.nodes, axis=1)\n",
    "\n",
    "  @jax.jit\n",
    "  def prediction_loss(params: hk.Params) -> jnp.ndarray:\n",
    "    decoded_graph = network.apply(params, zacharys_karate_club)\n",
    "    # We interpret the decoded nodes as a pair of logits for each node.\n",
    "    log_prob = jax.nn.log_softmax(decoded_graph.nodes)\n",
    "    # The only two assignments we know a-priori are those of Mr. Hi (Node 0)\n",
    "    # and John A (Node 33).\n",
    "    return -(log_prob[0, 0] + log_prob[33, 1])\n",
    "\n",
    "  opt_init, opt_update = optax.adam(1e-2)\n",
    "  opt_state = opt_init(params)\n",
    "\n",
    "  @jax.jit\n",
    "  def update(params: hk.Params, opt_state) -> Tuple[hk.Params, Any]:\n",
    "    \"\"\"Returns updated params and state.\"\"\"\n",
    "    g = jax.grad(prediction_loss)(params)\n",
    "    updates, opt_state = opt_update(g, opt_state)\n",
    "    return optax.apply_updates(params, updates), opt_state\n",
    "\n",
    "  @jax.jit\n",
    "  def accuracy(params: hk.Params) -> jnp.ndarray:\n",
    "    decoded_graph = network.apply(params, zacharys_karate_club)\n",
    "    return jnp.mean(jnp.argmax(decoded_graph.nodes, axis=1) == labels)\n",
    "\n",
    "  for step in range(num_steps):\n",
    "    print(f\"step {step} accuracy {accuracy(params).item():.2f}\")\n",
    "    params, opt_state = update(params, opt_state)\n",
    "\n",
    "  return predict(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X5tu3Ae8lMVu"
   },
   "source": [
    "#### Train the GCN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3zVy1dW2luMI"
   },
   "source": [
    "Let's train the GCN! We expect this model reach an accuracy of about 0.91."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gUKQVhE4ltFT"
   },
   "outputs": [],
   "source": [
    "network = hk.without_apply_rng(hk.transform(gcn_fn))\n",
    "result = optimize_club(network, num_steps=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ekiqqz1l3nh"
   },
   "source": [
    "Visualize ground truth and predicted node assignments:\n",
    "\n",
    "What do you think of the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3AcTHVJQlNrm"
   },
   "outputs": [],
   "source": [
    "zacharys_karate_club = get_zacharys_karate_club()\n",
    "nx_graph = convert_jraph_to_networkx_graph(zacharys_karate_club)\n",
    "pos = nx.circular_layout(nx_graph)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 7))\n",
    "ax1 = fig.add_subplot(121)\n",
    "nx.draw(\n",
    "    nx_graph,\n",
    "    pos=pos,\n",
    "    with_labels=True,\n",
    "    node_size=500,\n",
    "    node_color=result.tolist(),\n",
    "    font_color='white')\n",
    "ax1.title.set_text('Predicted Node Assignments with GCN')\n",
    "\n",
    "gt_labels = get_ground_truth_assignments_for_zacharys_karate_club()\n",
    "ax2 = fig.add_subplot(122)\n",
    "nx.draw(\n",
    "    nx_graph,\n",
    "    pos=pos,\n",
    "    with_labels=True,\n",
    "    node_size=500,\n",
    "    node_color=gt_labels.tolist(),\n",
    "    font_color='white')\n",
    "ax2.title.set_text('Ground-Truth Node Assignments')\n",
    "fig.suptitle('Do you spot the difference? 😐', y=-0.01)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gJQuyKbT0SAj"
   },
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qTF5T6Oa0SAj"
   },
   "source": [
    "**Question:** How does the network training behave if we omit the effect of normalization? What alternatives do we have?"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
