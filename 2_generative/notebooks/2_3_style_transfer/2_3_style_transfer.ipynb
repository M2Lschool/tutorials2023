{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_xnMOsbqHz61"
   },
   "source": [
    "# Practical 3: Conditional GANs for image-to-image translation \n",
    "---\n",
    "\n",
    "### **Authors:**\n",
    "\n",
    "Original tutorial by **Luigi Celona** and **Flavio Piccoli**, modified by **[Nemanja Rakicevic](https://nemanja-rakicevic.github.io/)** and **[Manos Kirtas](https://scholar.google.com/citations?user=EyaKPkwAAAAJ&hl=en)**\n",
    "\n",
    "\n",
    "### **Tutorial overview:**\n",
    "\n",
    "In this tutorial you will implement, train and analyse the results of a conditional GAN model for converting building facades to real buildings. This tutorial is adapted from \"Image to image translation using conditional GANs\", as described in [Image-to-Image Translation with Conditional Adversarial Networks](https://arxiv.org/abs/1611.07004).\n",
    "\n",
    "\n",
    "We will use the [CMP Facade Database](http://cmp.felk.cvut.cz/~tylecr1/facade/), helpfully provided by the [Center for Machine Perception](http://cmp.felk.cvut.cz/) at the [Czech Technical University in Prague](https://www.cvut.cz/). To keep our example short, we will use a preprocessed [copy](https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/) of this dataset, created by the authors of the [paper](https://arxiv.org/abs/1611.07004) above.\n",
    "\n",
    "Each epoch takes around 15 seconds on a single V100 GPU.\n",
    "\n",
    "Below is the output generated after training the model for 200 epochs.\n",
    "\n",
    "![sample output_1](https://www.tensorflow.org/images/gan/pix2pix_1.png)\n",
    "![sample output_2](https://www.tensorflow.org/images/gan/pix2pix_2.png)\n",
    "\n",
    "\n",
    "### **Tutorial outline:**\n",
    "- [Theory recap](#theory-recap)\n",
    "- [Setup](#setup)\n",
    "  - Install and Import Packages\n",
    "  - Dataset\n",
    "  - Helper Functions\n",
    "  - Input Pipeline\n",
    "- [Implementing conditional GAN components](#implement-components)\n",
    "  - Generator\n",
    "  - Discriminator\n",
    "  - Loss functions\n",
    "- [Training and Visualisation](#training)\n",
    "  - Train Utils\n",
    "  - Main Train Loop\n",
    "  - Run Training\n",
    "- [Analysis](#analysis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory recap <a class=\"anchor\" id=\"theory-recap\"></a>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditional Generative Adversarial Networks (cGANs) are a type of GAN that can be used to generate images conditioned on some input data. This makes them well-suited for image-to-image translation tasks, where the goal is to translate an image from one domain to another, such as from sketches to photos, or from black and white to color.\n",
    "\n",
    "cGANs work by training two neural networks against each other: a generator and a discriminator. The generator takes as input the image to be translated, as well as some additional conditioning information, and produces a translated image. The discriminator is trained to distinguish between real images from the target domain and fake images generated by the generator.\n",
    "\n",
    "![trainings setup](https://github.com/M2Lschool/tutorials2023/raw/main/2_generative/images/img2img_translation.png)\n",
    "[Image credit [Image-to-image translation with conditional adversarial networks, Isola et al. (2017)](https://arxiv.org/abs/1611.07004)]\n",
    "\n",
    "During training, the generator and discriminator are pitted against each other in a minimax game. The generator tries to produce images that are indistinguishable from real images, while the discriminator tries to get better at detecting fake images. This adversarial process forces the generator to produce increasingly realistic images.\n",
    "\n",
    "Once the cGAN is trained, it can be used to translate images from the source domain to the target domain. This is done by simply feeding the generator an image from the source domain, along with any necessary conditioning information. The generator will then produce a translated image in the target domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e1_Y75QXJS6h"
   },
   "source": [
    "## Setup  <a class=\"anchor\" id=\"setup\"></a>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install and Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TmG8rR8D1fZa"
   },
   "outputs": [],
   "source": [
    "!pip install ipdb &> /dev/null\n",
    "!pip install git+https://github.com/deepmind/dm-haiku &> /dev/null\n",
    "!pip install -U tensorboard &> /dev/null\n",
    "!pip install git+https://github.com/deepmind/optax.git &> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YfIk2es3hJEd"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import functools\n",
    "import numpy as np\n",
    "\n",
    "# Dataset libraries.\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "import haiku as hk\n",
    "import jax\n",
    "import optax  # Package for optimizer.\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# Plotting libraries.\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "from typing import Mapping, Optional, Tuple, NamedTuple, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Hyperparameters\n",
    "\n",
    "BUFFER_SIZE = 400  #@param\n",
    "BATCH_SIZE = 1  #@param\n",
    "IMG_WIDTH = 256  #@param\n",
    "IMG_HEIGHT = 256  #@param\n",
    "TRAIN_INIT_RANDOM_SEED = 1729  #@param\n",
    "LAMBDA = 100  #@param\n",
    "EPOCHS = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iYn4MdZnKCey"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "You can download this dataset and similar datasets from [here](https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets). As mentioned in the [paper](https://arxiv.org/abs/1611.07004) we apply random jittering and mirroring to the training dataset.\n",
    "\n",
    "* In random jittering, the image is resized to `286 x 286` and then randomly cropped to `256 x 256`\n",
    "* In random mirroring, the image is randomly flipped horizontally i.e left to right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kn-k8kTXuAlv"
   },
   "outputs": [],
   "source": [
    "_URL = 'http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/facades.tar.gz'\n",
    "path_to_zip = tf.keras.utils.get_file('facades.tar.gz',\n",
    "                                      origin=_URL,\n",
    "                                      extract=True)\n",
    "PATH = os.path.join(os.path.dirname(path_to_zip), 'facades/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZjMDvmR_j2GA"
   },
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2CbTEt448b4R"
   },
   "outputs": [],
   "source": [
    "# We need a random key for initialization.\n",
    "rng = jax.random.PRNGKey(TRAIN_INIT_RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aO9ZAGH5K3SY"
   },
   "outputs": [],
   "source": [
    "#@title Dataset loading and preprocessing\n",
    "# We use tensorflow readers; JAX does not have support for input data reading\n",
    "# and pre-processing.\n",
    "def load(image_file):\n",
    "    image = tf.io.read_file(image_file)\n",
    "    image = tf.image.decode_jpeg(image)\n",
    "\n",
    "    w = tf.shape(image)[1]\n",
    "\n",
    "    w = w // 2\n",
    "    real_image = image[:, :w, :]\n",
    "    input_image = image[:, w:, :]\n",
    "\n",
    "    input_image = tf.cast(input_image, tf.float32)\n",
    "    real_image = tf.cast(real_image, tf.float32)\n",
    "\n",
    "    return input_image, real_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4OLHMpsQ5aOv"
   },
   "outputs": [],
   "source": [
    "inp, re = load(PATH + 'train/100.jpg')\n",
    "# Casting to int for matplotlib to show the image.\n",
    "plt.figure()\n",
    "plt.imshow(inp/255.0)\n",
    "plt.figure()\n",
    "plt.imshow(re/255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rwwYQpu9FzDu"
   },
   "outputs": [],
   "source": [
    "def resize(input_image, real_image, height, width):\n",
    "    input_image = tf.image.resize(input_image, [height, width],\n",
    "                                  method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    real_image = tf.image.resize(real_image, [height, width],\n",
    "                                 method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "\n",
    "    return input_image, real_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yn3IwqhiIszt"
   },
   "outputs": [],
   "source": [
    "def random_crop(input_image, real_image):\n",
    "    stacked_image = tf.stack([input_image, real_image], axis=0)\n",
    "    cropped_image = tf.image.random_crop(\n",
    "      stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 3])\n",
    "\n",
    "    return cropped_image[0], cropped_image[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "muhR2cgbLKWW"
   },
   "outputs": [],
   "source": [
    "# Normalizes the input images to [-1, 1].\n",
    "def normalize(input_image, real_image):\n",
    "    input_image = (input_image / 127.5) - 1\n",
    "    real_image = (real_image / 127.5) - 1\n",
    "\n",
    "    return input_image, real_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G2RHtcYPj2GC"
   },
   "source": [
    "Random jittering as described in the paper is composed of the following steps:\n",
    "1. Resize an image to a bigger height and width\n",
    "2. Randomly crop to the target size\n",
    "3. Randomly flip the image horizontally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fVQOjcPVLrUc"
   },
   "outputs": [],
   "source": [
    "#@title Data augmentation { form-width: \"40%\"}\n",
    "@tf.function()\n",
    "def random_jitter(input_image, real_image):\n",
    "    # Resizing to 286 x 286 x 3.\n",
    "    input_image, real_image = resize(input_image, real_image, 286, 286)\n",
    "\n",
    "    # Randomly cropping to 256 x 256 x 3.\n",
    "    input_image, real_image = random_crop(input_image, real_image)\n",
    "\n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        # Random mirroring.\n",
    "        input_image = tf.image.flip_left_right(input_image)\n",
    "        real_image = tf.image.flip_left_right(real_image)\n",
    "\n",
    "    return input_image, real_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n0OGdi6D92kM"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "for i in range(4):\n",
    "    rj_inp, rj_re = random_jitter(inp, re)\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.imshow(rj_inp / 255.0)\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tyaP4hLJ8b4W"
   },
   "outputs": [],
   "source": [
    "def load_image_train(image_file):\n",
    "    input_image, real_image = load(image_file)\n",
    "    input_image, real_image = random_jitter(input_image, real_image)\n",
    "    input_image, real_image = normalize(input_image, real_image)\n",
    "\n",
    "    return input_image, real_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VB3Z6D_zKSru"
   },
   "outputs": [],
   "source": [
    "def load_image_test(image_file):\n",
    "    input_image, real_image = load(image_file)\n",
    "    input_image, real_image = resize(input_image, real_image,\n",
    "                                     IMG_HEIGHT, IMG_WIDTH)\n",
    "    input_image, real_image = normalize(input_image, real_image)\n",
    "\n",
    "    return input_image, real_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PIGN6ouoQxt3"
   },
   "source": [
    "### Input Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SQHmYSmk8b4b"
   },
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.list_files(PATH + 'train/*.jpg')\n",
    "train_dataset = train_dataset.map(load_image_train,\n",
    "                                  num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MS9J0yA58b4g"
   },
   "outputs": [],
   "source": [
    "test_dataset = tf.data.Dataset.list_files(PATH + 'test/*.jpg')\n",
    "test_dataset = test_dataset.map(load_image_test)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing conditional GAN components <a class=\"anchor\" id=\"implement-components\"></a>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THY-sZMiQ4UV"
   },
   "source": [
    "### Generator  \n",
    "\n",
    "The architecture of the generator is a modified U-Net [[U-net: Convolutional networks for biomedical image segmentation, Ronneberger et al (2015)](https://arxiv.org/abs/1505.04597)]. The U-Net is an encoder-decoder with skip connections between mirrored layers in the encoder and decoder stacks. The skip connections allow to circumvent the bottleneck that causes the loss of low-level information (e.g. location of prominent edges).\n",
    "\n",
    "\n",
    "![trainings setup](https://github.com/M2Lschool/tutorials2023/raw/main/2_generative/images/generator_architecture.png)\n",
    "[Image credit [Image-to-image translation with conditional adversarial networks, Isola et al. (2017)](https://arxiv.org/abs/1611.07004)]\n",
    "\n",
    "  * Each block in the encoder is (Conv -> Batchnorm -> Leaky ReLU)\n",
    "  * Each block in the decoder is (Transposed Conv -> Batchnorm -> Dropout (applied to the first 3 blocks) -> ReLU)\n",
    "  * There are skip connections between the encoder and decoder (as in U-net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hk.Conv2D?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hk.BatchNorm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.nn.leaky_relu?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3R09ATE_SH9P"
   },
   "outputs": [],
   "source": [
    "#@title Encoder definition (Conv -> Batchnorm -> Leaky ReLU) { form-width: \"40%\" }\n",
    "\n",
    "class Encoder(hk.Module):\n",
    "    def __init__(self,\n",
    "                 channels: int,\n",
    "                 size: int,\n",
    "                 apply_batchnorm=True):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.size = size\n",
    "        self.initializer = hk.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
    "        self.apply_batchnorm = apply_batchnorm\n",
    "\n",
    "    def __call__(self, inputs, is_training):\n",
    "        ##################################################################\n",
    "        #  YOUR CODE HERE:\n",
    "        \n",
    "        # Encoder steps:\n",
    "        # 1. Apply hk.Conv2D layer (channels, size, stride=2, init, pad='SAME', nobias) to inputs.\n",
    "\n",
    "        # 2. Apply hk.BatchNorm if flag is active.\n",
    "\n",
    "        # 3. Apply jax.nn.leaky_relu (negative_slop=0.2) on output.\n",
    "        ##################################################################\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hk.Conv2DTranspose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hk.BatchNorm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hk.dropout?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nhgDsHClSQzP"
   },
   "outputs": [],
   "source": [
    "#@title Decoder definition (Transposed Conv -> Batchnorm -> Dropout (applied to the first 3 blocks) -> ReLU)  { form-width: \"40%\" }\n",
    "\n",
    "class Decoder(hk.Module):\n",
    "    def __init__(self,\n",
    "                 channels: int,\n",
    "                 size: int,\n",
    "                 apply_dropout=False):\n",
    "        super().__init__()\n",
    "        self.initializer = hk.initializers.RandomNormal(mean=0.0,\n",
    "                                                        stddev=0.02)\n",
    "        self.channels = channels\n",
    "        self.size = size\n",
    "        self.apply_dropout = apply_dropout\n",
    "\n",
    "    def __call__(self, inputs, is_training):\n",
    "        ##################################################################\n",
    "        #  YOUR CODE HERE:\n",
    "        \n",
    "        # Decoder steps:\n",
    "        # 1. Apply transpose conv layer (channels, size, stride=2, init, pad='SAME', nobias) to inputs.\n",
    "        \n",
    "        # 2. Apply batch_norm if flag is active.\n",
    "\n",
    "        # 3. dropout\n",
    "\n",
    "        # 4. ReLU\n",
    "        ##################################################################\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lFPI4Nu-8b4q"
   },
   "outputs": [],
   "source": [
    "class Generator(hk.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # In comment the output size of each block. `bs` is the batch size.\n",
    "        self.down_stack = [\n",
    "            Encoder(64, 4, apply_batchnorm=False),  # (bs, 128, 128, 64)\n",
    "            Encoder(128, 4),  # (bs, 64, 64, 128)\n",
    "            Encoder(256, 4),  # (bs, 32, 32, 256)\n",
    "            Encoder(512, 4),  # (bs, 16, 16, 512)\n",
    "            Encoder(512, 4),  # (bs, 8, 8, 512)\n",
    "            Encoder(512, 4),  # (bs, 4, 4, 512)\n",
    "            Encoder(512, 4),  # (bs, 2, 2, 512)\n",
    "            Encoder(512, 4),  # (bs, 1, 1, 512)\n",
    "        ]\n",
    "\n",
    "        self.up_stack = [\n",
    "            Decoder(512, 4, apply_dropout=True),  # (bs, 2, 2, 1024)\n",
    "            Decoder(512, 4, apply_dropout=True),  # (bs, 4, 4, 1024)\n",
    "            Decoder(512, 4, apply_dropout=True),  # (bs, 8, 8, 1024)\n",
    "            Decoder(512, 4),  # (bs, 16, 16, 1024)\n",
    "            Decoder(256, 4),  # (bs, 32, 32, 512)\n",
    "            Decoder(128, 4),  # (bs, 64, 64, 256)\n",
    "            Decoder(64, 4),  # (bs, 128, 128, 128)\n",
    "        ]\n",
    "\n",
    "        initializer = hk.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
    "        self.last = hk.Conv2DTranspose(3, 4,\n",
    "                                       stride=2,\n",
    "                                       padding='SAME',\n",
    "                                       w_init=initializer)  # (bs, 256, 256, 3)\n",
    "\n",
    "    def __call__(self, x, is_training):\n",
    "        # Downsampling through the model\n",
    "        skips = []\n",
    "        for down in self.down_stack:\n",
    "            x = down(x, is_training)\n",
    "            ##################################################################\n",
    "            #  YOUR CODE HERE:\n",
    "            \n",
    "            # Add encoder outputs to the list of skips.\n",
    "            \n",
    "            ##################################################################\n",
    "\n",
    "\n",
    "        # Upsampling and establishing the skip connections\n",
    "        skips = reversed(skips[:-1])\n",
    "        for up, skip in zip(self.up_stack, skips):\n",
    "            x = up(x, is_training)\n",
    "            \n",
    "            ##################################################################\n",
    "            #  YOUR CODE HERE:\n",
    "            \n",
    "            # Concatenate the skip and the previous step output.\n",
    "            \n",
    "            ##################################################################\n",
    "\n",
    "        x = self.last(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TlB-XMY5Awj9"
   },
   "source": [
    "![Generator Update Image](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/images/gen.png?raw=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZTKZfoaoEF22"
   },
   "source": [
    "## Build the Discriminator\n",
    "\n",
    "The Discriminator is a PatchGAN. It works by classifying small patches of an image as real or fake, rather than classifying the entire image at once. \n",
    "This discriminator is run convolutionally across the image, averaging all responses to provide the ultimate output of D. Such a discriminator effectively models the image as a Markov random field, assuming independence between pixels separated by more than a patch diameter. \n",
    "This makes it more robust to global changes in the image, such as changes in lighting or color, and it can be understood as a type of texture/style loss.\n",
    "\n",
    "  * Each block in the discriminator is (Conv -> BatchNorm -> Leaky ReLU)\n",
    "  * The shape of the output after the last layer is (batch_size, 30, 30, 1)\n",
    "  * Each 30x30 patch of the output classifies a 70x70 portion of the input image (such an architecture is called a PatchGAN).\n",
    "  * Discriminator receives 2 inputs.\n",
    "    * Input image and the target image, which it should classify as real.\n",
    "    * Input image and the generated image (output of generator), which it should classify as fake.\n",
    "    * We concatenate these 2 inputs together in the code (`jax.numpy.concatenate([inp, tar], axis=-1)`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ll6aNeQx8b4v"
   },
   "outputs": [],
   "source": [
    "class Discriminator(hk.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        initializer = hk.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
    "\n",
    "        self.down1 = Encoder(64, 4, apply_batchnorm=False)\n",
    "        self.down2 = Encoder(128, 4)\n",
    "        self.down3 = Encoder(256, 4)\n",
    "\n",
    "        self.conv = hk.Conv2D(512, 4, stride=1, w_init=initializer,\n",
    "                              padding='VALID', with_bias=False)\n",
    "        self.bn = hk.BatchNorm(create_scale=True, create_offset=True,\n",
    "                               decay_rate=0.999, eps=0.001)\n",
    "        self.last = hk.Conv2D(1, 4, stride=1, padding='VALID',\n",
    "                              w_init=initializer)\n",
    "\n",
    "    def __call__(self, x, is_training):  # (bs, 256, 256, channels*2)\n",
    "        x = self.down1(x, is_training)  # (bs, 128, 128, 64)\n",
    "        x = self.down2(x, is_training)  # (bs, 64, 64, 128)\n",
    "        x = self.down3(x, is_training)  # (bs, 32, 32, 256)\n",
    "        x = jnp.pad(x, ((0, 0), (1, 1), (1, 1), (0, 0)))  # (bs, 34, 34, 256)\n",
    "        x = self.conv(x)  # (bs, 31, 31, 512)\n",
    "        x = self.bn(x, is_training)\n",
    "        x = jax.nn.leaky_relu(x, negative_slope=0.2)\n",
    "        x = jnp.pad(x, ((0, 0), (1, 1), (1, 1), (0, 0)))  # (bs, 33, 33, 256)\n",
    "        x = self.last(x)  # (bs, 30, 30, 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dpDPEQXIAiQO"
   },
   "source": [
    "#### Generator loss\n",
    "\n",
    "  * It is a sigmoid cross entropy loss of the generated images and an **array of ones**.\n",
    "  * The [paper](https://arxiv.org/abs/1611.07004) also includes L1 loss which is MAE (mean absolute error) between the generated image and the target image.\n",
    "  * This allows the generated image to become structurally similar to the target image.\n",
    "  * The formula to calculate the total generator loss = gan_loss + LAMBDA * l1_loss, where LAMBDA = 100. This value was decided by the authors of the [paper](https://arxiv.org/abs/1611.07004)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XuzZAKFjqzqA"
   },
   "outputs": [],
   "source": [
    "# Computes binary cross entropy for classification.\n",
    "\n",
    "def bce_w_logits(\n",
    "    logits: jnp.ndarray,\n",
    "    target: jnp.ndarray\n",
    ") -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    Binary Cross Entropy Loss\n",
    "    :param logits: Input tensor\n",
    "    :param target: Target tensor\n",
    "    :return: Scalar value\n",
    "    \"\"\"\n",
    "    ##################################################################\n",
    "    #  YOUR CODE HERE:\n",
    "\n",
    "    # Refer to the first tutorial.\n",
    "    max_val = jnp.clip(logits, 0, None)\n",
    "    loss = logits - logits * target + max_val + \\\n",
    "    jnp.log(jnp.exp(-max_val) + jnp.exp((-logits - max_val)))\n",
    "    ##################################################################\n",
    "\n",
    "    return jnp.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "90BIcCKcDMxz"
   },
   "outputs": [],
   "source": [
    "def generator_loss(\n",
    "    disc_generated_output: jnp.ndarray,\n",
    "    gen_output: jnp.ndarray,\n",
    "    target: jnp.ndarray\n",
    ") -> Tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray]:\n",
    "    \"\"\"Computes the generator loss for the given batch.\"\"\"\n",
    "    ##################################################################\n",
    "    #  YOUR CODE HERE:\n",
    "\n",
    "    # Pass the discriminator output as logits and the target is array of ones of the same shape.\n",
    "    \n",
    "    ##################################################################\n",
    "\n",
    "    # Mean absolute error.\n",
    "    l1_loss = jnp.mean(jnp.abs(target - gen_output))\n",
    "    \n",
    "    \n",
    "    ##################################################################\n",
    "    #  YOUR CODE HERE:\n",
    "\n",
    "    # Calculate total generator loss as the GAN loss + scaled L1 loss.\n",
    "    \n",
    "    ##################################################################\n",
    "\n",
    "    return total_gen_loss, gan_loss, l1_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOqg1dhUAWoD"
   },
   "source": [
    "#### Discriminator loss\n",
    "  * The discriminator loss function takes 2 inputs; **real images, generated images**\n",
    "  * real_loss is a sigmoid cross entropy loss of the **real images** and an **array of ones (since these are the real images)**\n",
    "  * generated_loss is a sigmoid cross entropy loss of the **generated images** and an **array of zeros (since these are the fake images)**\n",
    "  * Then the total_loss is the sum of real_loss and the generated_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wkMNfBWlT-PV"
   },
   "outputs": [],
   "source": [
    "def discriminator_loss(disc_real_output, disc_generated_output):\n",
    "    real_loss = bce_w_logits(disc_real_output,\n",
    "                             jnp.ones_like(disc_real_output))\n",
    "    generated_loss = bce_w_logits(disc_generated_output,\n",
    "                                  jnp.zeros_like(disc_generated_output))\n",
    "    total_disc_loss = real_loss + generated_loss\n",
    "    return total_disc_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ede4p2YELFa"
   },
   "source": [
    "The training procedure for the discriminator is shown below.\n",
    "\n",
    "To learn more about the architecture and the hyperparameters you can refer the [paper](https://arxiv.org/abs/1611.07004)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IS9sHa-1BoAF"
   },
   "source": [
    "![Discriminator Update Image](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/images/dis.png?raw=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NLKOG55MErD0"
   },
   "source": [
    "## Model Training and Visualisation <a class=\"anchor\" id=\"training\"></a>  \n",
    "\n",
    "* For each example input generates an output.\n",
    "* The discriminator receives the input image and the generated image as the first input. The second input is the input image and the target image.\n",
    "* Next, we calculate the generator and the discriminator loss.\n",
    "* Then, we calculate the gradients of loss with respect to both the generator and the discriminator variables (inputs) and apply those to the optimizer.\n",
    "* Last, we log the losses to TensorBoard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0FMYgY_mPfTi"
   },
   "source": [
    "### Define the Checkpoint-saver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7micePl8XVtF"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "if not os.path.exists(checkpoint_prefix):\n",
    "    os.makedirs(checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the main model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hh0Q_XuscND5"
   },
   "outputs": [],
   "source": [
    "class P2PTuple(NamedTuple):\n",
    "    gen: Any\n",
    "    disc: Any\n",
    "\n",
    "\n",
    "class P2PState(NamedTuple):\n",
    "    params: P2PTuple\n",
    "    states: P2PTuple\n",
    "    opt_state: P2PTuple\n",
    "\n",
    "\n",
    "class Pix2Pix:\n",
    "    \"\"\"Pix2Pix model.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.gen_transform = hk.transform_with_state(\n",
    "            lambda *args: Generator()(*args)\n",
    "        )\n",
    "        self.disc_transform = hk.transform_with_state(\n",
    "            lambda *args: Discriminator()(*args)\n",
    "        )\n",
    "\n",
    "        # Build the optimizers.\n",
    "        self.gen_optimizer = optax.adam(2e-4, b1=0.5, b2=0.999)\n",
    "        self.disc_optimizer = optax.adam(2e-4, b1=0.5, b2=0.999)\n",
    "\n",
    "    @functools.partial(jax.jit, static_argnums=0)\n",
    "    def initial_state(self,\n",
    "                      rng: jnp.ndarray,\n",
    "                      batch: Tuple[jnp.ndarray, jnp.ndarray]):\n",
    "        \"\"\"Returns the initial parameters and optimize states of the generator.\n",
    "        \"\"\"\n",
    "        rng, gen_rng, disc_rng = jax.random.split(rng, 3)\n",
    "        gen_params, gen_state = self.gen_transform.init(gen_rng, batch[0], True)\n",
    "        disc_params, disc_state = \\\n",
    "            self.disc_transform.init(disc_rng,\n",
    "                                     jnp.concatenate(batch, axis=-1),\n",
    "                                     True)\n",
    "        params = P2PTuple(gen=gen_params, disc=disc_params)\n",
    "        states = P2PTuple(gen=gen_state, disc=disc_state)\n",
    "\n",
    "        # Initialize the optimizers.\n",
    "        opt_state = P2PTuple(gen=self.gen_optimizer.init(params.gen),\n",
    "                             disc=self.disc_optimizer.init(params.disc)\n",
    "                             )\n",
    "        return P2PState(params=params, states=states, opt_state=opt_state)\n",
    "\n",
    "    def generate_images(self,\n",
    "                        params: P2PTuple,\n",
    "                        state: P2PTuple,\n",
    "                        test_input):\n",
    "        # Note: The `training=True` is intentional here since\n",
    "        #       we want the batch statistics while running the model\n",
    "        #       on the test dataset. If we use training=False, we will get\n",
    "        #       the accumulated statistics learned from the training dataset\n",
    "        #       (which we don't want)\n",
    "        prediction, _ = self.gen_transform.apply(\n",
    "            params, state, None, test_input, True\n",
    "        )\n",
    "\n",
    "        return prediction\n",
    "\n",
    "    def gen_loss(self,\n",
    "                 gen_params: P2PTuple,\n",
    "                 gen_state: P2PTuple,\n",
    "                 batch: Tuple[jnp.ndarray, jnp.ndarray],\n",
    "                 disc_params: P2PTuple,\n",
    "                 disc_state: P2PTuple,\n",
    "                 rng_gen, rng_disc):\n",
    "        \"\"\"Computes a regularized loss for the given batch.\"\"\"\n",
    "\n",
    "        input, target = batch\n",
    "        \n",
    "        # Apply the generator to the input\n",
    "        output, gen_state = self.gen_transform.apply(\n",
    "            gen_params, gen_state, rng_gen, input, True\n",
    "        )\n",
    "\n",
    "        # Evaluate using the discriminator.\n",
    "        ##################################################################\n",
    "        #  YOUR CODE HERE:\n",
    "\n",
    "        # Apply the disc_transform (like in the step above) to the concatenated input and output.\n",
    "        # disc_generated_output, disc_state = ...\n",
    "        \n",
    "        ##################################################################\n",
    "\n",
    "\n",
    "        states = P2PTuple(gen=gen_state, disc=disc_state)\n",
    "\n",
    "        # Compute the loss.\n",
    "        total_loss, gan_loss, l1_loss = generator_loss(\n",
    "            disc_generated_output, output, target\n",
    "            )\n",
    "\n",
    "        return total_loss, (output, states, gan_loss, l1_loss)\n",
    "\n",
    "    def disc_loss(self,\n",
    "                  params: P2PTuple,\n",
    "                  state: P2PTuple,\n",
    "                  batch: Tuple[jnp.ndarray, jnp.ndarray],\n",
    "                  gen_output: jnp.ndarray, rng):\n",
    "        \"\"\"Computes a regularized loss for the given batch.\"\"\"\n",
    "        input, target = batch\n",
    "        real_output, state = self.disc_transform.apply(\n",
    "            params, state, rng, jnp.concatenate([input, target], axis=-1), True\n",
    "        )\n",
    "\n",
    "            \n",
    "        ##################################################################\n",
    "        #  YOUR CODE HERE:\n",
    "\n",
    "        # Apply the disc_transform (like in the step above) to the concatenated input and generated output.\n",
    "        # generated_output, state = ...\n",
    "        \n",
    "        ##################################################################\n",
    "\n",
    "        # Compute discriminator loss.\n",
    "        loss = discriminator_loss(real_output, generated_output)\n",
    "        return loss, state\n",
    "\n",
    "    @functools.partial(jax.jit, static_argnums=0)\n",
    "    def update(self, rng, p2p_state, batch):\n",
    "        \"\"\" Performs a parameter update. \"\"\"\n",
    "        rng, gen_rng, disc_rng = jax.random.split(rng, 3)\n",
    "\n",
    "        # Update the generator.\n",
    "        (gen_loss, gen_aux), gen_grads = \\\n",
    "            jax.value_and_grad(self.gen_loss,\n",
    "                               has_aux=True)(\n",
    "            p2p_state.params.gen,\n",
    "            p2p_state.states.gen,\n",
    "            batch,\n",
    "            p2p_state.params.disc,\n",
    "            p2p_state.states.disc,\n",
    "            gen_rng, disc_rng)\n",
    "\n",
    "        generated_output, states, gan_loss, l1_loss = gen_aux\n",
    "        gen_update, gen_opt_state = self.gen_optimizer.update(\n",
    "            gen_grads, p2p_state.opt_state.gen)\n",
    "        gen_params = optax.apply_updates(p2p_state.params.gen, gen_update)\n",
    "\n",
    "        # Update the discriminator.\n",
    "        (disc_loss, disc_state), disc_grads = \\\n",
    "            jax.value_and_grad(self.disc_loss,\n",
    "                               has_aux=True)(\n",
    "            p2p_state.params.disc,\n",
    "            states.disc,\n",
    "            batch,\n",
    "            generated_output,\n",
    "            disc_rng)\n",
    "\n",
    "        disc_update, disc_opt_state = self.disc_optimizer.update(\n",
    "            disc_grads, p2p_state.opt_state.disc)\n",
    "        disc_params = optax.apply_updates(p2p_state.params.disc, disc_update)\n",
    "\n",
    "        params = P2PTuple(gen=gen_params, disc=disc_params)\n",
    "        states = P2PTuple(gen=states.gen, disc=disc_state)\n",
    "        opt_state = P2PTuple(gen=gen_opt_state, disc=disc_opt_state)\n",
    "        p2p_state = P2PState(params=params, states=states, opt_state=opt_state)\n",
    "\n",
    "        return p2p_state, gen_loss, disc_loss, gan_loss, l1_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lYVSnyvwjCUb"
   },
   "outputs": [],
   "source": [
    "# The model.\n",
    "net = Pix2Pix()\n",
    "\n",
    "# Initialize the network and optimizer.\n",
    "for input, target in train_dataset.take(1):\n",
    "    net_state = net.initial_state(rng, (jnp.asarray(input),\n",
    "                                        jnp.asarray(target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xNNMDBNH12q-"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "log_dir = \"logs/\"\n",
    "\n",
    "summary_writer = tf.summary.create_file_writer(\n",
    "  log_dir + \"fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hx7s-vBHFKdh"
   },
   "source": [
    "### The training loop\n",
    "\n",
    "* Iterates over the number of epochs.\n",
    "* On each epoch it clears the display, and runs `generate_images` to show it's progress.\n",
    "* On each epoch it iterates over the training dataset, printing a '.' for each example.\n",
    "* It saves a checkpoint every 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2M7LmLtGEMQJ"
   },
   "outputs": [],
   "source": [
    "def fit(train_ds, epochs, test_ds, net_state):\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        display.clear_output(wait=True)\n",
    "\n",
    "        for example_input, example_target in test_ds.take(1):\n",
    "            prediction = net.generate_images(net_state.params.gen,\n",
    "                                             net_state.states.gen,\n",
    "                                             jnp.asarray(example_input))\n",
    "            plt.figure(figsize=(15, 15))\n",
    "\n",
    "            display_list = [example_input[0], example_target[0], prediction[0]]\n",
    "            title = ['Input Image', 'Ground Truth', 'Predicted Image']\n",
    "\n",
    "            for i in range(3):\n",
    "                plt.subplot(1, 3, i+1)\n",
    "                plt.title(title[i])\n",
    "                # Getting the pixel values between [0, 1] to plot it.\n",
    "                plt.imshow(display_list[i] * 0.5 + 0.5)\n",
    "                plt.axis('off')\n",
    "                plt.show()\n",
    "\n",
    "        print(\"Epoch: \", epoch)\n",
    "\n",
    "        # Train loop.\n",
    "        for n, (input_image, target) in train_ds.enumerate():\n",
    "            # Take a training step.\n",
    "            print('.', end='')\n",
    "            if (n+1) % 100 == 0:\n",
    "                print()\n",
    "\n",
    "            # Main update step\n",
    "            net_state, gen_total_loss, disc_loss, \\\n",
    "            gen_gan_loss, gen_l1_loss = \\\n",
    "                net.update(rng, net_state,\n",
    "                           (jnp.asarray(input_image), jnp.asarray(target)))\n",
    "\n",
    "            with summary_writer.as_default():\n",
    "                tf.summary.scalar('gen_total_loss', gen_total_loss, step=epoch)\n",
    "                tf.summary.scalar('gen_gan_loss', gen_gan_loss, step=epoch)\n",
    "                tf.summary.scalar('gen_l1_loss', gen_l1_loss, step=epoch)\n",
    "                tf.summary.scalar('disc_loss', disc_loss, step=epoch)\n",
    "        \n",
    "        print()\n",
    "\n",
    "        # Save (checkpoint) the model every 20 epochs.\n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            with open(\n",
    "                os.path.join(checkpoint_prefix, 'pix2pix_params.pkl'),\n",
    "                    'wb') as handle:\n",
    "                pickle.dump(net_state.params, handle,\n",
    "                            protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "            with open(\n",
    "                os.path.join(checkpoint_prefix, 'pix2pix_states.pkl'),\n",
    "                    'wb') as handle:\n",
    "                pickle.dump(net_state.states, handle,\n",
    "                            protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        print('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n",
    "                                                           time.time()-start))\n",
    "\n",
    "    # Save the last checkpoint.\n",
    "    with open(\n",
    "        os.path.join(checkpoint_prefix, 'pix2pix_params.pkl'),\n",
    "            'wb') as handle:\n",
    "        pickle.dump(net_state.params, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    with open(\n",
    "        os.path.join(checkpoint_prefix, 'pix2pix_states.pkl'),\n",
    "            'wb') as handle:\n",
    "        pickle.dump(net_state.states, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wozqyTh2wmCu"
   },
   "source": [
    "This training loop saves logs you can easily view in TensorBoard to monitor the training progress. Working locally you would launch a separate tensorboard process. In a notebook, if you want to monitor with TensorBoard it's easiest to launch the viewer before starting the training.\n",
    "\n",
    "To launch the viewer run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ot22ujrlLhOd"
   },
   "outputs": [],
   "source": [
    "#docs_infra: no_execute\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {log_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pe0-8Bzg22ox"
   },
   "source": [
    "Now run the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a1zZmKmvOH85"
   },
   "outputs": [],
   "source": [
    "fit(train_dataset, EPOCHS, test_dataset, net_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oeq9sByu86-B"
   },
   "source": [
    "If you want to share the TensorBoard results _publicly_ you can upload the logs to [TensorBoard.dev](https://tensorboard.dev/) by copying the following into a code-cell.\n",
    "\n",
    "Note: This requires a Google account.\n",
    "\n",
    "```\n",
    "!tensorboard dev upload --logdir  {log_dir}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l-kT7WHRKz-E"
   },
   "source": [
    "Caution: This command does not terminate. It's designed to continuously upload the results of long-running experiments. Once your data is uploaded you need to stop it using the \"interrupt execution\" option in your notebook tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-lGhS_LfwQoL"
   },
   "source": [
    "You can view the [results of a previous run](https://tensorboard.dev/experiment/lZ0C6FONROaUMfjYkVyJqw) of this notebook on [TensorBoard.dev](https://tensorboard.dev/).\n",
    "\n",
    "TensorBoard.dev is a managed experience for hosting, tracking, and sharing ML experiments with everyone.\n",
    "\n",
    "It can also included inline using an `<iframe>`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8IS4c93guQ8E"
   },
   "outputs": [],
   "source": [
    "display.IFrame(\n",
    "    src=\"https://tensorboard.dev/experiment/lZ0C6FONROaUMfjYkVyJqw\",\n",
    "    width=\"100%\",\n",
    "    height=\"1000px\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DMTm4peo3cem"
   },
   "source": [
    "Interpreting the logs from a GAN is more subtle than a simple classification or regression model. Things to look for:\n",
    "\n",
    "* Check that neither model has \"won\". If either the `gen_gan_loss` or the `disc_loss` gets very low it's an indicator that this model is dominating the other, and you are not successfully training the combined model.\n",
    "* The value `log(2) = 0.69` is a good reference point for these losses, as it indicates a perplexity of 2: That the discriminator is on average equally uncertain about the two options.\n",
    "* For the `disc_loss` a value below `0.69` means the discriminator is doing better than random, on the combined set of real + generated images.\n",
    "* For the `gen_gan_loss` a value below `0.69` means the generator is doing better than random at fooling the descriminator.\n",
    "* As training progresses the `gen_l1_loss` should go down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kz80bY3aQ1VZ"
   },
   "source": [
    "## Restore the latest checkpoint and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HSSm4kfvJiqv"
   },
   "outputs": [],
   "source": [
    "!ls {checkpoint_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4t4x69adQ5xb"
   },
   "outputs": [],
   "source": [
    "# Restore the latest checkpoint in checkpoint_dir.\n",
    "with open(\n",
    "    os.path.join(checkpoint_prefix, 'pix2pix_params.pkl'),\n",
    "        'rb') as handle:\n",
    "    params = pickle.load(handle)\n",
    "\n",
    "with open(\n",
    "    os.path.join(checkpoint_prefix, 'pix2pix_states.pkl'),\n",
    "        'rb') as handle:\n",
    "    states = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1RGysMU_BZhx"
   },
   "source": [
    "## Generate using test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jzDgOHSHzSFG"
   },
   "source": [
    "* We pass images from the test dataset to the generator.\n",
    "* The generator will then translate the input image into the output.\n",
    "* Last step is to plot the predictions and **voila!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KUgSnmy2nqSP"
   },
   "outputs": [],
   "source": [
    "# Run the trained model on a few examples from the test dataset\n",
    "\n",
    "for test_input, tar in test_dataset.take(5):\n",
    "    prediction = net.generate_images(\n",
    "        params.gen, states.gen,jnp.asarray(test_input))\n",
    "    display_list = [test_input[0], tar[0], prediction[0]]\n",
    "    title = ['Input Image', 'Ground Truth', 'Predicted Image']\n",
    "    for i in range(3):\n",
    "        plt.subplot(1, 3, i+1)\n",
    "        plt.title(title[i])\n",
    "        # getting the pixel values between [0, 1] to plot it.\n",
    "        plt.imshow(display_list[i] * 0.5 + 0.5)\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis <a class=\"anchor\" id=\"analysis\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) What is the minimal number of EPOCHS necessary to train for, in order to get meaningful image generation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. How does removing skip connection affect the generated image quality?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. How does changing $\\lambda$ of the L1 loss affect the training?"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ComputerVisionPart3_solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
