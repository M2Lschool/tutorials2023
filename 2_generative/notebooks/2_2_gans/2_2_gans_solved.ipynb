{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5vZHV2nPZtHx"
   },
   "source": [
    "# Practical 2: Generative Adversarial Networks (GAN)\n",
    "---\n",
    "\n",
    "**Tutorial overview**\n",
    "In this tutorial you will implement, train and analyse the results of a Generative Adversarial Network.\n",
    "\n",
    "\n",
    "**Tutorial outline**\n",
    "- [Theory recap](#theory-recap)\n",
    "- [Setup](#setup)\n",
    "  - Install and Import Packages\n",
    "  - Dataset\n",
    "  - Helper Functions\n",
    "- [Implementing GAN components](#implement-gan)\n",
    "  - Generator and Discriminator\n",
    "  - Loss functions\n",
    "- [Training and Visualisation](#training)\n",
    "  - Train Utils\n",
    "  - Main Train Loop\n",
    "  - Run Training\n",
    "- [Analysis](#analysis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BBeaBXnIZtH5"
   },
   "source": [
    "## Theory recap <a class=\"anchor\" id=\"theory-recap\"></a>\n",
    "---\n",
    "\n",
    "\n",
    "Generative Adversarial Networks (GANs) are a type of machine learning model that can be used to generate realistic data, such as images, text, and audio. GANs work by training two neural networks against each other: a generator and a discriminator.\n",
    "\n",
    "The generator is responsible for generating new data, while the discriminator is responsible for distinguishing between real and fake data. During training, the generator tries to fool the discriminator by generating data that is indistinguishable from real data. The discriminator, in turn, tries to get better at detecting fake data. This adversarial process forces the generator to produce increasingly realistic data.\n",
    "\n",
    "How GANs work in detail:\n",
    "\n",
    "- The generator takes as input a random noise vector and produces a synthetic data sample.\n",
    "- The discriminator takes as input a data sample (either real or fake) and outputs a probability that the sample is real.\n",
    "- The generator and discriminator are trained alternately.\n",
    "  - The generator is trained to maximize the probability that the discriminator classifies its output as real,    \n",
    "  - The discriminator is trained to maximize the probability that it correctly classifies real and fake data.\n",
    "- This training process continues until the generator is able to produce data that is indistinguishable from real data.\n",
    "\n",
    "![gan_architecture](https://miro.medium.com/v2/resize:fit:1400/1*ZKUo2QtHasnr8-RiqeJ_YA.png)\n",
    "[Image source [Saul Dobilas Medium](https://towardsdatascience.com/gans-generative-adversarial-networks-an-advanced-solution-for-data-generation-2ac9756a8a99)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "va0ZVK24ZtH-"
   },
   "source": [
    "## Setup <a class=\"anchor\" id=\"setup\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTES:**\n",
    "<br>\n",
    "- If the following error is appeared 'AttributeError: module 'numpy' has no attribute '_no_nep50_warning', please restart the kernel and re-run the cells.\n",
    "- Please use the GPU kernel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JvxKB5LGZtIB"
   },
   "source": [
    "### Install and Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uKeg5TrDnE22",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#@title Install Packages\n",
    "! pip install numpy==1.25.2\n",
    "! pip install chex -q\n",
    "! pip install optax -q\n",
    "! pip install distrax -q\n",
    "! pip install dm_haiku -q\n",
    "! pip install absl-py -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lE7Dgv4IZtIH"
   },
   "outputs": [],
   "source": [
    "! pip install typing_extensions -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VY7XMJHVZtIJ"
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from time import time\n",
    "\n",
    "import haiku as hk\n",
    "import jax\n",
    "import matplotlib.pyplot as plt\n",
    "import optax\n",
    "from haiku.initializers import Constant, RandomNormal\n",
    "from jax import jit\n",
    "from jax import numpy as jnp\n",
    "from jax import random\n",
    "from jax import value_and_grad as vgrad\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1QqUFhs8ZtIK"
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4v2ZQgYZtIL"
   },
   "source": [
    "In this tutorial we will use the [MNIST dataset](https://keras.io/api/datasets/mnist/).\n",
    "<br>\n",
    "The other datasets are left for homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MKV2F0kjZtIM"
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_images_mnist(batch_size=128, seed=0):\n",
    "    def prepare_dataset(X):\n",
    "        X = tf.cast(X, tf.float32)\n",
    "        # Normalization, pixels in [-1, 1]\n",
    "        X = (X / 255.0) * 2.0 - 1.0\n",
    "        X = tf.expand_dims(X, axis=-1)\n",
    "        # shape=(batch_size, 28, 28, 1)\n",
    "        return X\n",
    "\n",
    "    (X_train, _), (X_test, _) = tf.keras.datasets.mnist.load_data()\n",
    "    X = tf.concat([X_train, X_test], axis=0)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "    dataset = dataset.cache().shuffle(buffer_size=len(X), seed=seed)\n",
    "    dataset = dataset.batch(batch_size).prefetch(buffer_size=-1)\n",
    "    dataset = dataset.map(prepare_dataset)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def load_images_cifar10(batch_size=128, seed=0):\n",
    "    def prepare_dataset(X):\n",
    "        X = tf.cast(X, tf.float32)\n",
    "        # Normalization, pixels in [-1, 1]\n",
    "        X = (X / 255.0) * 2.0 - 1.0\n",
    "        # shape=(batch_size, 32, 32, 3)\n",
    "        return X\n",
    "\n",
    "    (X_train, _), (X_test, _) = tf.keras.datasets.cifar10.load_data()\n",
    "    X = tf.concat([X_train, X_test], axis=0)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "    dataset = dataset.cache().shuffle(buffer_size=len(X), seed=seed)\n",
    "    dataset = dataset.batch(batch_size).prefetch(buffer_size=-1)\n",
    "    dataset = dataset.map(prepare_dataset)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "    return iter(tfds.as_numpy(ds))\n",
    "\n",
    "\n",
    "def load_images_celeba_64(batch_size=128, seed=0, path='data/CelebA/'):\n",
    "    def generate_data():\n",
    "        for f_name in os.listdir(path):\n",
    "            img = cv2.imread(os.path.join(path, f_name))\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)[20:-20, :, :]\n",
    "            img = cv2.resize(img, (64, 64))\n",
    "            img = tf.constant(img, dtype=tf.float32)\n",
    "            img = (img / 255.0) * 2.0 - 1.0\n",
    "            yield img\n",
    "\n",
    "    dataset = tf.data.Dataset.from_generator(generate_data,\n",
    "                                             output_types=tf.float32,\n",
    "                                             output_shapes=(64, 64, 3))\n",
    "    dataset = dataset.shuffle(buffer_size=202_600, seed=seed)\n",
    "    dataset = dataset.batch(batch_size).prefetch(buffer_size=-1)\n",
    "    dataset.__len__ = lambda: tf.constant(202_599 // batch_size + 1,\n",
    "                                          dtype=tf.int64)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def load_images_celeba_128(batch_size=128, seed=0, path='data/CelebA/'):\n",
    "    def generate_data():\n",
    "        for f_name in os.listdir(path):\n",
    "            img = cv2.imread(os.path.join(path, f_name))\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)[20:-20, :, :]\n",
    "            img = cv2.resize(img, (128, 128))\n",
    "            img = tf.constant(img, dtype=tf.float32)\n",
    "            img = (img / 255.0) * 2.0 - 1.0\n",
    "            yield img\n",
    "\n",
    "    dataset = tf.data.Dataset.from_generator(generate_data,\n",
    "                                             output_types=tf.float32,\n",
    "                                             output_shapes=(128, 128, 3))\n",
    "    dataset = dataset.shuffle(buffer_size=202_600, seed=seed)\n",
    "    dataset = dataset.batch(batch_size).prefetch(buffer_size=-1)\n",
    "    dataset.__len__ = lambda: tf.constant(202_599 // batch_size + 1,\n",
    "                                          dtype=tf.int64)\n",
    "    return dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NX8zQPIWZtIN"
   },
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZemaXab9ZtIO"
   },
   "source": [
    "Some helper functions for:\n",
    "- A class that keeps track of the moving average of the inputs\n",
    "- Plotting functions for the images and losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Vy3J5LIl5bT"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Mean(object):\n",
    "    \"\"\" Compute dynamic mean of given inputs. \"\"\"\n",
    "    def __init__(self):\n",
    "        self.val = 0.0\n",
    "        self.count = 0\n",
    "        # Keep the history of **given inputs**\n",
    "        self.history = []\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0.0\n",
    "        self.count = 0\n",
    "\n",
    "    def reset_history(self):\n",
    "        self.history = []\n",
    "\n",
    "    def __call__(self, val):\n",
    "        if isinstance(val, jnp.ndarray):\n",
    "            val = val.item()\n",
    "        # Keep the history of **given inputs**\n",
    "        self.history.append(val)\n",
    "        self.val = (self.val * self.count + val) / (self.count + 1)\n",
    "        self.count += 1\n",
    "        return self.val\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.val)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return repr(self.val)\n",
    "\n",
    "    def __format__(self, *args, **kwargs):\n",
    "        return self.val.__format__(*args, **kwargs)\n",
    "\n",
    "\n",
    "def input_func(key, batch_size, zdim):\n",
    "    \"\"\" Input of generator ( = \"noise\" in classic GAN). \"\"\"\n",
    "    return random.normal(key, (batch_size, zdim))\n",
    "\n",
    "\n",
    "# Plotting\n",
    "\n",
    "def plot_tensor_images(images, num_images=(10, 10), cmap='gray'):\n",
    "    # Normalize to [0, 1]\n",
    "    if images.min() < 0:\n",
    "        images = (images + 1.0) / 2.0\n",
    "        images = jnp.clip(images, 0.0, 1.0)\n",
    "    h, w = images.shape[1:3]\n",
    "    nh, nw = num_images\n",
    "    if len(images) < nh * nw:\n",
    "        raise ValueError(\"Not enough images to show (number of images \"\n",
    "                         f\"received: {len(images)}, number of image \"\n",
    "                         f\"needed : {nh}x{nw}.\")\n",
    "    image_grid = images[:nh * nw].reshape(nh, nw, h, w, -1)\n",
    "    image_grid = jnp.transpose(image_grid, (0, 2, 1, 3, 4))\n",
    "    image_grid = image_grid.reshape(nh * h, nw * w, -1)\n",
    "    plt.grid(False)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image_grid, cmap=cmap)\n",
    "\n",
    "\n",
    "def plot_curves(history, n_epochs):\n",
    "    loss_gen, loss_disc = history['loss_gen'], history['loss_disc']\n",
    "    loss_gen, loss_disc = jnp.array(loss_gen), jnp.array(loss_disc)\n",
    "    # Downsample the points to reduce the length of the plots\n",
    "    len_gen, len_disc = min(1000, len(loss_gen)), min(1000, len(loss_disc))\n",
    "    time_gen = jnp.linspace(0, n_epochs, len_gen)\n",
    "    time_disc = jnp.linspace(0, n_epochs, len_disc)\n",
    "    loss_gen = jnp.interp(time_gen, jnp.linspace(0, n_epochs, len(loss_gen)),\n",
    "                          loss_gen)\n",
    "    loss_disc = jnp.interp(time_disc, jnp.linspace(0, n_epochs,\n",
    "                                                   len(loss_disc)), loss_disc)\n",
    "\n",
    "    plt.plot(time_gen, loss_gen, color='#ff9100', label='generator loss')\n",
    "    plt.plot(time_disc, loss_disc, color='#00aaff', label='discriminator loss')\n",
    "    plt.ylim([0, 5.0])\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend(loc='best')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mxgWxyqDZtIO"
   },
   "source": [
    "## Implementing GAN components <a class=\"anchor\" id=\"implement-gan\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator\n",
    "\n",
    "The code below implements the Generator Network using hk.Conv2DTranspose, and the Discriminator Network using hk.Conv2D.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UPkD7uu3ZtIP"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Generator(hk.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.channels = (256, 128, 64, 1)\n",
    "        self.ker_shapes = (3, 4, 3, 4)\n",
    "        self.strides = (2, 1, 2, 2)\n",
    "        self.padding = (0, 0, 0, 0)\n",
    "        self.n_layers = len(self.channels)\n",
    "\n",
    "        if isinstance(self.ker_shapes, int):\n",
    "            self.ker_shapes = [self.ker_shapes] * self.n_layers\n",
    "        if isinstance(self.strides, int):\n",
    "            self.strides = [self.strides] * self.n_layers\n",
    "        if isinstance(self.padding, int):\n",
    "            self.padding = [self.padding] * self.n_layers\n",
    "\n",
    "        self.layers = [\n",
    "            hk.Conv2DTranspose(\n",
    "                self.channels[i],\n",
    "                kernel_shape=self.ker_shapes[i],\n",
    "                stride=self.strides[i],\n",
    "                padding='VALID' if self.padding[i] == 0 else 'SAME',\n",
    "                with_bias=False,\n",
    "                w_init=RandomNormal(stddev=0.02, mean=0.0))\n",
    "            for i in range(self.n_layers)\n",
    "        ]\n",
    "\n",
    "        self.batch_norms = [\n",
    "            hk.BatchNorm(False, False, 0.99) for _ in range(self.n_layers - 1)\n",
    "        ]\n",
    "\n",
    "    def __call__(self, z, is_training=jnp.asarray([True])):\n",
    "        x = jnp.reshape(z, (-1, 1, 1, z.shape[-1]))\n",
    "        for i in range(self.n_layers - 1):\n",
    "            x = self.layers[i](x)\n",
    "            x = self.batch_norms[i](x, is_training)\n",
    "            x = jax.nn.relu(x)\n",
    "        x = self.layers[-1](x)\n",
    "        x = jnp.tanh(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Discriminator(hk.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.channels = (16, 32, 1)\n",
    "        self.ker_shapes = 4\n",
    "        self.strides = 2\n",
    "        self.padding = (0, 0, 0, 0)\n",
    "        self.n_layers = len(self.channels)\n",
    "\n",
    "        if isinstance(self.ker_shapes, int):\n",
    "            self.ker_shapes = [self.ker_shapes] * self.n_layers\n",
    "        if isinstance(self.strides, int):\n",
    "            self.strides = [self.strides] * self.n_layers\n",
    "        if isinstance(self.padding, int):\n",
    "            self.padding = [self.padding] * self.n_layers\n",
    "\n",
    "        self.layers = [\n",
    "            hk.Conv2D(self.channels[i],\n",
    "                      kernel_shape=self.ker_shapes[i],\n",
    "                      stride=self.strides[i],\n",
    "                      padding='VALID' if self.padding[i] == 0 else 'SAME',\n",
    "                      w_init=RandomNormal(stddev=0.02, mean=0.0),\n",
    "                      b_init=Constant(0.0)) for i in range(self.n_layers)\n",
    "        ]\n",
    "        self.batch_norms = [\n",
    "            hk.BatchNorm(True, True, 0.99) for _ in range(self.n_layers - 1)\n",
    "        ]\n",
    "\n",
    "    def __call__(self, x, is_training=jnp.asarray([True])):\n",
    "\n",
    "        if x.ndim == 3:\n",
    "            x = jnp.expand_dims(x, axis=-1)\n",
    "        for i in range(self.n_layers - 1):\n",
    "            x = self.layers[i](x)\n",
    "            x = self.batch_norms[i](x, is_training)\n",
    "            x = jax.nn.leaky_relu(x, 0.2)\n",
    "\n",
    "        x = self.layers[-1](x)\n",
    "        x = jnp.squeeze(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MZbCgcLvZtIR"
   },
   "outputs": [],
   "source": [
    "\n",
    "@hk.without_apply_rng\n",
    "@hk.transform_with_state\n",
    "def gen_fwd(z, is_training):\n",
    "    \"\"\" (transformed) Forward pass of generator. \"\"\"\n",
    "\n",
    "    generator = Generator()\n",
    "    X_fake = generator(z, is_training=is_training)\n",
    "    return X_fake\n",
    "\n",
    "\n",
    "@hk.without_apply_rng\n",
    "@hk.transform_with_state\n",
    "def disc_fwd(X, is_training):\n",
    "    \"\"\" (transformed) Discriminator pass of generator. \"\"\"\n",
    "\n",
    "    discriminator = Discriminator()\n",
    "    y_pred = discriminator(X, is_training=is_training)\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def init_generator(key, config, z):\n",
    "    \"\"\" Initialize the generator parameters/states\n",
    "    and its optimizer.\"\"\"\n",
    "    params_gen, state_gen = gen_fwd.init(key,\n",
    "                                         z,\n",
    "                                         is_training=jnp.asarray([True]))\n",
    "\n",
    "    opt_gen = optax.adam(learning_rate=config['lr'],\n",
    "                         b1=config['beta1'],\n",
    "                         b2=config['beta2'])\n",
    "    opt_state_gen = opt_gen.init(params_gen)\n",
    "    return state_gen, opt_gen, opt_state_gen, params_gen\n",
    "\n",
    "\n",
    "def init_discriminator(key, config, x):\n",
    "    \"\"\" Initialize the discriminator parameters/states\n",
    "    and its optimizer.\"\"\"\n",
    "    params_disc, state_disc = disc_fwd.init(\n",
    "        key,\n",
    "        x,\n",
    "        is_training=jnp.asarray([True]),\n",
    "    )\n",
    "\n",
    "    opt_disc = optax.adam(learning_rate=config['lr'],\n",
    "                          b1=config['beta1'],\n",
    "                          b2=config['beta2'])\n",
    "    opt_state_disc = opt_disc.init(params_disc)\n",
    "    return state_disc, opt_disc, opt_state_disc, params_disc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1RVzK41ZtIR"
   },
   "source": [
    "## Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j3sbvj_OZtIR"
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def cross_entropy(logits, labels):\n",
    "    return optax.sigmoid_binary_cross_entropy(logits, labels)\n",
    "\n",
    "\n",
    "\n",
    "def fwd_loss_gen(params_gen, params_disc, state_gen, state_disc, z,\n",
    "                 is_training):\n",
    "    \"\"\" Computes the loss of the generator over one batch. \"\"\"\n",
    "\n",
    "    ############################################################\n",
    "    # Generate fake images X_fake, state_gen = gen_fwd.apply(params_gen, state_gen, ???, is_training=is_training)\n",
    "    \n",
    "    X_fake, state_gen = gen_fwd.apply(params_gen,\n",
    "                                      state_gen,\n",
    "                                      z,\n",
    "                                      is_training=is_training)\n",
    "    \n",
    "    ############################################################\n",
    "\n",
    "    \n",
    "    ############################################################    \n",
    "    # Discriminate fake images X_fake, state_gen = disc_fwd.apply(params_disc, state_disc, ???, is_training=is_training)\n",
    "\n",
    "    y_pred_fake, state_disc = disc_fwd.apply(params_disc,\n",
    "                                             state_disc,\n",
    "                                             X_fake,\n",
    "                                             is_training=is_training)\n",
    "    \n",
    "    #################################################################\n",
    "\n",
    "    \n",
    "    #################################################################\n",
    "    # Run cross-entroy\n",
    "    \n",
    "    loss_gen = cross_entropy(y_pred_fake, jnp.ones_like(y_pred_fake))\n",
    "    \n",
    "    #################################################################\n",
    "    loss_gen = jnp.mean(loss_gen)\n",
    "    return loss_gen, (state_gen, state_disc)\n",
    "\n",
    "\n",
    "def fwd_loss_disc(params_disc, params_gen, state_disc, state_gen, z, X_real,\n",
    "                  is_training):\n",
    "    \"\"\" Computes the loss of the discriminator over one batch. \"\"\"\n",
    "    X_fake, state_gen = gen_fwd.apply(params_gen,\n",
    "                                      state_gen,\n",
    "                                      z,\n",
    "                                      is_training=is_training)\n",
    "\n",
    "    #################################################################\n",
    "    # Predict fake data y_pred_fake, state_disc = disc_fwd.apply(params_disc, state_disc, ???, is_training=is_training)\n",
    "    y_pred_fake, state_disc = disc_fwd.apply(params_disc,\n",
    "                                             state_disc,\n",
    "                                             X_fake,\n",
    "                                             is_training=is_training)\n",
    "\n",
    "    # Predict real data y_pred_real, state_disc = disc_fwd.apply(params_disc, state_disc, ???, is_training=is_training)\n",
    "    y_pred_real, state_disc = disc_fwd.apply(params_disc,\n",
    "                                             state_disc,\n",
    "                                             X_real,\n",
    "                                             is_training=is_training)\n",
    "    #################################################################\n",
    "\n",
    "    # Smooth label (+/- 0.1)\n",
    "    fake_loss = cross_entropy(y_pred_fake, jnp.zeros_like(y_pred_fake) + 0.1)\n",
    "    real_loss = cross_entropy(y_pred_real, jnp.ones_like(y_pred_real) - 0.1)\n",
    "    loss_disc = ((fake_loss + real_loss) / 2.0)\n",
    "    loss_disc = jnp.mean(loss_disc)\n",
    "    return loss_disc, (state_disc, state_gen)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "plel48FLZtIS"
   },
   "source": [
    "## Training and Visualisations <a class=\"anchor\" id=\"training\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ZzlSPr7ZtIS"
   },
   "source": [
    "### Train utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l8nxIJNJZtIS"
   },
   "outputs": [],
   "source": [
    "@partial(jit, static_argnums=(4, 7))\n",
    "def train_gen(\n",
    "    params_gen,\n",
    "    params_disc,\n",
    "    state_gen,\n",
    "    state_disc,\n",
    "    opt_gen,\n",
    "    opt_state_gen,\n",
    "    z,\n",
    "    is_training,\n",
    "):\n",
    "    \"\"\" (jit) Update the generator parameters/states and\n",
    "    its optimizer over one batch. \"\"\"\n",
    "    (loss_gen, (state_gen, state_disc)), grads = vgrad(fwd_loss_gen,\n",
    "                                                       has_aux=True)(\n",
    "                                                           params_gen,\n",
    "                                                           params_disc,\n",
    "                                                           state_gen,\n",
    "                                                           state_disc,\n",
    "                                                           z,\n",
    "                                                           is_training,\n",
    "                                                       )\n",
    "\n",
    "    updates, opt_state_gen = opt_gen.update(grads, opt_state_gen, params_gen)\n",
    "    params_gen = optax.apply_updates(params_gen, updates)\n",
    "    return params_gen, state_gen, state_disc, opt_state_gen, loss_gen\n",
    "\n",
    "\n",
    "@partial(jit, static_argnums=(4, 8))\n",
    "def train_disc(params_disc, params_gen, state_disc, state_gen, opt_disc,\n",
    "               opt_state_disc, z, X_real, is_training):\n",
    "    \"\"\" (jit) Update the discriminator parameters/states and\n",
    "    its optimizer over one batch. \"\"\"\n",
    "    (loss_disc, (state_disc, state_gen)), grads = vgrad(fwd_loss_disc,\n",
    "                                                        has_aux=True)(\n",
    "                                                            params_disc,\n",
    "                                                            params_gen,\n",
    "                                                            state_disc,\n",
    "                                                            state_gen,\n",
    "                                                            z,\n",
    "                                                            X_real,\n",
    "                                                            is_training,\n",
    "                                                        )\n",
    "    updates, opt_state_disc = opt_disc.update(grads, opt_state_disc,\n",
    "                                              params_disc)\n",
    "    params_disc = optax.apply_updates(params_disc, updates)\n",
    "    return params_disc, state_disc, state_gen, opt_state_disc, loss_disc\n",
    "\n",
    "\n",
    "def cycle_train(X_real, key, params_gen, params_disc, state_gen, state_disc,\n",
    "                opt_gen, opt_state_gen, opt_disc, opt_state_disc,\n",
    "                mean_loss_gen, mean_loss_disc, config):\n",
    "    \"\"\" Train the generator and the discriminator and update\n",
    "    the means (mean_loss_gen and mean_loss_disc).\n",
    "    \"\"\"\n",
    "    X_real = jnp.array(X_real)\n",
    "    batch_size = X_real.shape[0]  # (can change at the end of epoch)\n",
    "    key, *keys = random.split(key, 1 + config['disc_cycle_train'])\n",
    "\n",
    "    # Train generator\n",
    "    z = input_func(key, batch_size, config['z_dim'])\n",
    "    (params_gen, state_gen, state_disc, opt_state_gen, loss_gen) = train_gen(\n",
    "        params_gen,\n",
    "        params_disc,\n",
    "        state_gen,\n",
    "        state_disc,\n",
    "        opt_gen,\n",
    "        opt_state_gen,\n",
    "        z,\n",
    "        True,\n",
    "    )\n",
    "    mean_loss_gen(loss_gen)\n",
    "\n",
    "    # Train discriminator (cylce_train_disc times)\n",
    "    for k in range(config['disc_cycle_train']):\n",
    "        z = input_func(keys[k], batch_size, config['z_dim'])\n",
    "\n",
    "        (params_disc, state_disc, state_gen, opt_state_disc,\n",
    "         loss_disc) = train_disc(\n",
    "             params_disc,\n",
    "             params_gen,\n",
    "             state_disc,\n",
    "             state_gen,\n",
    "             opt_disc,\n",
    "             opt_state_disc,\n",
    "             z,\n",
    "             X_real,\n",
    "             True,\n",
    "         )\n",
    "        mean_loss_disc(loss_disc)\n",
    "\n",
    "    return (params_gen, params_disc, state_gen, state_disc, opt_state_gen,\n",
    "            opt_state_disc, mean_loss_gen, mean_loss_disc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B9FlcnqzZtIT"
   },
   "source": [
    "### Main training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kMfbbXlrZtIU"
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    dataset,\n",
    "    key,\n",
    "    config,\n",
    "):\n",
    "\n",
    "    X_real = jnp.array(dataset.take(1).as_numpy_iterator().next())\n",
    "    batch_size = X_real.shape[0]\n",
    "    # Initialize generator and discriminator\n",
    "    z = input_func(key, batch_size, config['z_dim'])\n",
    "    state_gen, opt_gen, opt_state_gen, params_gen = init_generator(\n",
    "        key, config, z)\n",
    "    state_disc, opt_disc, opt_state_disc, params_disc = init_discriminator(\n",
    "        key, config, X_real)\n",
    "    print('Initialization succeeded.')\n",
    "\n",
    "    mean_loss_gen, mean_loss_disc = Mean(), Mean()\n",
    "    len_ds = int(dataset.__len__())\n",
    "    itr = 0\n",
    "    start = time()\n",
    "    for ep in range(config['epochs']):\n",
    "        print('Epoch {}-{}'.format(ep + 1, config['epochs']))\n",
    "\n",
    "        for i_batch, X_real in enumerate(dataset):\n",
    "            t = time() - start\n",
    "            eta = t / (itr + 1) * (config['epochs'] * len_ds - itr - 1)\n",
    "            t_h, t_m, t_s = t // 3600, (t % 3600) // 60, t % 60\n",
    "            eta_h, eta_m, eta_s = eta // 3600, (eta % 3600) // 60, eta % 60\n",
    "            print(\n",
    "                f'  batch {i_batch + 1}/{len_ds} - '\n",
    "                f'gen loss:{mean_loss_gen: .5f} - '\n",
    "                f'disc loss:{mean_loss_disc: .5f} - '\n",
    "                f'time: {int(t_h)}h {int(t_m)}min {int(t_s)}sec - '\n",
    "                f'eta: {int(eta_h)}h {int(eta_m)}min {int(eta_s)}sec    ',\n",
    "                end='\\r')\n",
    "            key, subkey = random.split(key, 2)\n",
    "\n",
    "            # Training both generator and discriminator\n",
    "            (params_gen, params_disc, state_gen, state_disc, opt_state_gen,\n",
    "             opt_state_disc, mean_loss_gen, mean_loss_disc) = cycle_train(\n",
    "                 X_real, key, params_gen, params_disc, state_gen, state_disc,\n",
    "                 opt_gen, opt_state_gen, opt_disc, opt_state_disc,\n",
    "                 mean_loss_gen, mean_loss_disc, config)\n",
    "\n",
    "            itr += 1\n",
    "            # Plot images\n",
    "            if itr % config['display_step'] == 0:\n",
    "                z = input_func(\n",
    "                    subkey, config['num_images'][0] * config['num_images'][1],\n",
    "                    config['z_dim'])\n",
    "                X_fake, state_gen = gen_fwd.apply(params_gen,\n",
    "                                                  state_gen,\n",
    "                                                  z,\n",
    "                                                  is_training=False)\n",
    "\n",
    "                plot_tensor_images(X_fake, num_images=config['num_images'])\n",
    "                plt.title('Epoch {}/{} - iteration {}'.format(\n",
    "                    ep + 1, config['epochs'], itr),\n",
    "                          fontsize=15)\n",
    "                plt.show(block=False)\n",
    "\n",
    "\n",
    "        mean_loss_gen.reset(), mean_loss_disc.reset()\n",
    "        print()\n",
    "\n",
    "    history = {\n",
    "        'loss_gen': mean_loss_gen.history,\n",
    "        'loss_disc': mean_loss_disc.history\n",
    "    }\n",
    "\n",
    "    z = input_func(key, config['num_images'][0] * config['num_images'][1],\n",
    "                   config['z_dim'])\n",
    "    X_fake, state_gen = gen_fwd.apply(\n",
    "        params_gen,\n",
    "        state_gen,\n",
    "        z,\n",
    "        is_training=False,\n",
    "    )\n",
    "    plt.figure(figsize=(40, 20))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plot_tensor_images(X_fake, num_images=config['num_images'])\n",
    "    plt.title('Final images generation')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title('Loss curves')\n",
    "    plot_curves(history, config['epochs'])\n",
    "    plt.show()\n",
    "    return params_gen, state_gen, params_disc, state_disc, history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDAd2CkKZtIV"
   },
   "source": [
    "### Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'z_dim': 64,\n",
    "    'lr': 1e-4,\n",
    "    'beta1': 0.5,\n",
    "    'beta2': 0.999,\n",
    "    'batch_size': 128,\n",
    "    'epochs': 5,\n",
    "    'disc_cycle_train': 5,\n",
    "    'seed': 25,\n",
    "    'display_step': 500,\n",
    "    'num_images': (10, 10)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_eKrCo01ZtIV"
   },
   "outputs": [],
   "source": [
    "key = random.PRNGKey(config['seed'])\n",
    "dataset = load_images_mnist(batch_size=config['batch_size'],\n",
    "                            seed=config['seed'])\n",
    "\n",
    "params_gen, state_gen, params_disc, state_disc, history = train(\n",
    "    dataset=dataset,\n",
    "    key=key,\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08LVkzYOZtIV"
   },
   "source": [
    "## Analysis <a class=\"anchor\" id=\"analysis\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Find the correct leaerning rate and batch size?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s7c79F7NZtIV"
   },
   "source": [
    "2. Which hyperparameter affects GAN training the most?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Increase the num_epochs, how does this affect generated image quality? "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
