{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/M2Lschool/tutorials2023-dev/blob/main/2_generative/notebooks/2_1_vaes/2_1_vaes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YagUuibzdbNw"
      },
      "source": [
        "# Variational AutoEncoders (VAE)\n",
        "---\n",
        "\n",
        "**Tutorial overview**\n",
        "<br>\n",
        "In this tutorial you will implement, train and analyse the results of a Variational AutoEncoder.\n",
        "\n",
        "\n",
        "**Tutorial outline**\n",
        "- [Setup](#setup)\n",
        "  - Install and Import Packages\n",
        "  - Dataset\n",
        "  - Helper Functions\n",
        "- [Implementing VAE components](#implement-vae)\n",
        "  - Encoder\n",
        "  - Decoder\n",
        "  - Loss functions\n",
        "- [Training and Visualisation](#training)\n",
        "  - Train Utils\n",
        "  - Main Train Loop\n",
        "  - Run Training\n",
        "- [Analysis](#analysis)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTseo3k_8qaB"
      },
      "source": [
        "## Setup <a class=\"anchor\" id=\"setup\"></a>  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8TF94GG1Vg9"
      },
      "source": [
        "**NOTE:**\n",
        "<br>If the following error is appeared 'AttributeError: module 'numpy' has no attribute '_no_nep50_warning', please restart the kernel and re-run the cells."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXsvi3bsMQxS"
      },
      "source": [
        "### Install and Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCk0Fp7ElLDp"
      },
      "outputs": [],
      "source": [
        "#@title Install Packages\n",
        "! pip install numpy==1.25.2\n",
        "! pip install chex -q\n",
        "! pip install optax -q\n",
        "! pip install distrax -q\n",
        "! pip install dm_haiku -q\n",
        "! pip install absl-py -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LW6tbXlc9li0"
      },
      "outputs": [],
      "source": [
        "#@title Import Packages\n",
        "import dataclasses\n",
        "from typing import Mapping, NamedTuple, Sequence, Tuple, Iterator\n",
        "\n",
        "import haiku as hk\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import optax\n",
        "import tensorflow_datasets as tfds\n",
        "from matplotlib import pyplot as plt\n",
        "from tensorflow_probability.substrates import jax as tfp\n",
        "\n",
        "tfd = tfp.distributions\n",
        "\n",
        "PRNGKey = jnp.ndarray\n",
        "Batch = Mapping[str, np.ndarray]\n",
        "\n",
        "SAMPLE_SHAPE: Sequence[int] = (28, 28, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InCgNoZB-5R0"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "In this tutorial we will use a small subset of MNIST datasetColaboratory. We will load the standaert MNIST dataset from which we select a few celebrities,  and a small number of samples for each celebrity. This will allow us to train the models faster and visualize them easily.\n",
        "\n",
        "More specifically, we denote dataset as $\\mathcal{D}$ consisting of $N\\geq1$ datapoints:\n",
        "$$\n",
        "  \\mathcal{X} = \\{x^{(1)}, x^{(2)}, \\ldots, x^{(N)}\\} ≡ \\{x^{(i)}\\}\n",
        "_{i=1}^{N}.$$\n",
        "The datapoints are assumed to be intependednt sample from an unchanging underlying distribution. Formally, the observations $\\mathcal{X}=\\{x^{(i)}\\}\n",
        "_{1}^{N}$ are said to be independently and identically distributed. Under this assumption, the log probability assigned to the data by the model is therefore given by:\n",
        "$$\n",
        "\\log p_{\\theta}(\\mathcal{D}) = \\sum_{\\mathbb{x}\\in\\mathcal{X}}\\log p_{\\mathbb{\\theta}}(\\mathbb{x}),\n",
        "$$where $\\theta$ is the trainable parameters of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K--IJxGx_t8C"
      },
      "outputs": [],
      "source": [
        "# @title Dataset loading and visualisation helper functions\n",
        "# @markdown We can use TensorFlow data (`tfds`) to download MNIST from the cloud.\n",
        "# @markdown <br>[Chex](https://github.com/deepmind/chex) is a library of utilities helping to write more reliable JAX code.\n",
        "# @markdown Within `chex` you will find a `dataclass` object definition, which will automatically register new class instances into JAX, so you can easily apply JAX's tree utilities out of the box. We will use it to define a labelled data object type.\n",
        "\n",
        "\n",
        "def load_dataset(split: str, batch_size: int,\n",
        "                 random_seed: int) -> Iterator[Batch]:\n",
        "    ds = tfds.load(\n",
        "        \"binarized_mnist\",\n",
        "        split=split,\n",
        "        shuffle_files=True,\n",
        "        read_config=tfds.ReadConfig(shuffle_seed=random_seed),\n",
        "    )\n",
        "    ds = ds.shuffle(buffer_size=10 * batch_size, seed=random_seed)\n",
        "    ds = ds.batch(batch_size)\n",
        "    ds = ds.prefetch(buffer_size=5)\n",
        "    ds = ds.repeat()\n",
        "\n",
        "    return iter(tfds.as_numpy(ds))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXfJOl7Cffk9"
      },
      "source": [
        "## Variational Autoencoders <a class=\"anchor\" id=\"implement-vae\"></a>  \n",
        "\n",
        "\n",
        "Consider a joint distribution $p(x, z)$ over a set of latent variables $z \\in \\mathcal{Z}$ and observed variable $x \\in \\mathcal{X}$ (for instance, the images of our dataset).\n",
        "\n",
        "Inference over the observed variable $x$ involves computing the posterior distribution $p(z|x) = \\frac{p(x,z)}{p(x)}$ is often intractable to compute, as the _marginal likelihood_ $p(x) = \\int_z p(x, z)dz$ requires integrating over a potentially exponential number of configurations of $z$.\n",
        "\n",
        "In order to turn the deep latend-variable model's intractable posterior inference and learning problem into tractable problem, we introduce an inference model $q_\\phi(\\mathbf{z}|\\mathbf{x})$, which is called **encoder**., with $\\phi$ indicates the trainable parameters of the inference model.\n",
        "\n",
        "### Encoder - Posterior function modelling\n",
        "\n",
        "**Variational Inference (VI)** can be used to approximate the posterior $p(z|x)$ in a tractable fashion. VI casts the problem of computing the posterior as an optimization problem introducing a family of tractable (simpler) distribution $\\mathcal{Q}$ parametrized by $\\phi$. The objective is to find the best approximation of the true posterior $q_{\\phi^*} \\in \\mathcal{Q}$ that minimizes the Kullback-Leibler (KL) divergence with the exact prosterior:\n",
        "\n",
        "$$\n",
        "q_{\\phi^*}(z) = \\underset{q_{\\phi}}{arg min} \\ \\ D_{KL}(q_{\\phi}(z) || p(z|x))\n",
        "$$\n",
        "\n",
        "$q_{\\phi^*}(z)$ can serve as a proxy for the true posterior distribution. Note that the solution depends on the speciﬁc value of the observed (evidence) variables $x_i$ we are conditioning on, so computing the posterior requires solving an optimization problem for each sample independently.\n",
        "\n",
        "In this tutorial, we use a much more efficient approach. Rather than solving an optimization process per data point, we can **amortize the cost of inference** by leveraging the power of function approximation and learn a deterministic mapping to predict the distributional variables as a function of $x$. Specifically, the posterior parameters for $x_i$ will be the output of a *learned* function $f_\\theta(x_i)$, where $\\theta$ are parameters shared across all data points.\n",
        "\n",
        "\n",
        "<img src=\"https://lilianweng.github.io/posts/2018-08-12-vae/autoencoder-architecture.png\" alt=\"VAE\" width=\"800\"/>\n",
        "\n",
        "[Image credit: [lilianweng.github.io](https://lilianweng.github.io/posts/2018-08-12-vae/)]\n",
        "\n",
        "\n",
        "The simplest posterior model is a diagonal Gaussian\n",
        "$q_{\\phi}(z|x) = \\mathcal{N}(z|\\mu_{\\phi}(x), diag(\\sigma^2_{\\phi}(x)))$, where $q_{\\theta}$ can be modelled e.g. using a neural network, whose outputs define the parameters  $z = \\{\\mu_{\\phi}, \\sigma^2_{\\phi} \\}$ of the distribution.\n",
        "\n",
        "However, the Gaussian distribution is not allow as to return gradients. To this end, sampling from this Gaussian is achieved employing the **reparameterization trick** that disentangles the noise source, so that the gradients can be backpropagated:\n",
        "$$\n",
        "z \\sim \\mathcal{N}(z|\\mu, \\sigma^2) \\Leftrightarrow z = \\mu + \\sigma\u000f\\epsilon, \u000f \\epsilon \\sim \\mathcal{N}(0, 1)\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://lilianweng.github.io/posts/2018-08-12-vae/vae-gaussian.png\" alt=\"VAE reparameterisation trick\" width=\"800\"/>\n",
        "\n",
        "[Image credit: [lilianweng.github.io](https://lilianweng.github.io/posts/2018-08-12-vae/)]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkKjG9WL1VhI"
      },
      "outputs": [],
      "source": [
        "# Used functions:\n",
        "hk.Module?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hk.Linear?"
      ],
      "metadata": {
        "id": "9OjvFTeg3Kt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jax.random.normal?"
      ],
      "metadata": {
        "id": "KxEusegR3L3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mdOhDtUKNMD"
      },
      "outputs": [],
      "source": [
        "#@title Encoder Implementation\n",
        "class Encoder(hk.Module):\n",
        "    \"\"\"Encoder model.\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_size1: int = 50,\n",
        "        hidden_size2: int = 25,\n",
        "        latent_size: int = 10,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self._hidden_size1 = hidden_size1\n",
        "        self._hidden_size2 = hidden_size2\n",
        "        self._latent_size = latent_size\n",
        "        self.activation_fn = jax.nn.relu\n",
        "\n",
        "    def __call__(self, x: jnp.ndarray) -> Tuple[jnp.ndarray, jnp.ndarray]:https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#kullback-leibler\n",
        "\n",
        "        x = hk.Flatten()(x)\n",
        "\n",
        "\n",
        "\n",
        "        x = hk.Sequential([\n",
        "            ######################## CODE AFTER ########################\n",
        "            # Implement two hk.Linear layers using the corresponding hidden_size\n",
        "            # Each layer is followed by the activation function\n",
        "            # hint: hk.Linear -> https://dm-haiku.readthedocs.io/en/latest/api.html?highlight=linear#linear\n",
        "\n",
        "\n",
        "            ######################## CODE BEFORE ########################\n",
        "        ])(x)\n",
        "\n",
        "        ######################## CODE AFTER ########################\n",
        "        # Apply the linear layer to x to get the mean\n",
        "\n",
        "        mean =\n",
        "        ######################## CODE BEFORE ########################\n",
        "\n",
        "\n",
        "        ######################## CODE AFTER ########################\n",
        "        # Apply the linear layer to x to get the log_stddev\n",
        "\n",
        "        log_stddev =\n",
        "        ######################## CODE BEFORE ########################\n",
        "\n",
        "        stddev = jnp.exp(log_stddev)\n",
        "\n",
        "        ######################## CODE AFTER ########################\n",
        "        # Implement reparemeterization trick z = mean + std * epsilon. Epsilon is implemented with jax.random.normal\n",
        "        # hint: jax.random.normal -> https://jax.readthedocs.io/en/latest/_autosummary/jax.random.normal.html\n",
        "\n",
        "        z =\n",
        "        ######################## CODE BEFOR ########################\n",
        "\n",
        "        return z, mean, stddev"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbQji6GCM66O"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MoCIIQoN1VhK"
      },
      "outputs": [],
      "source": [
        "# Used functions:\n",
        "jax.nn.relu?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sVZEukDM113"
      },
      "outputs": [],
      "source": [
        "class Decoder(hk.Module):\n",
        "    \"\"\"Decoder model.\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_size: int = 512,\n",
        "        output_shape: Sequence[int] = SAMPLE_SHAPE,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self._hidden_size = hidden_size\n",
        "        self._output_shape = output_shape\n",
        "\n",
        "    def __call__(self, z: jnp.ndarray) -> jnp.ndarray:\n",
        "\n",
        "        ######################## CODE AFTER ########################\n",
        "        # Implement hk.Linear followd by jax.nn.relu. Apply both to z.\n",
        "\n",
        "        z =\n",
        "        z =\n",
        "        ######################## CODE BEFORE ########################\n",
        "\n",
        "        logits = hk.Linear(np.prod(self._output_shape))(z)\n",
        "        logits = jnp.reshape(logits, (-1, *self._output_shape))\n",
        "\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MVxYm-njgnC"
      },
      "outputs": [],
      "source": [
        "#@title Variational Autoencoder Implementation\n",
        "class VAEOutput(NamedTuple):\n",
        "    image: jnp.ndarray\n",
        "    mean: jnp.ndarray\n",
        "    stddev: jnp.ndarray\n",
        "    logits: jnp.ndarray\n",
        "\n",
        "\n",
        "@dataclasses.dataclass\n",
        "class VariationalAutoEncoder(hk.Module):\n",
        "    \"\"\"Main VAE model class, uses Encoder & Decoder under the hood.\"\"\"\n",
        "\n",
        "    encoder: Encoder\n",
        "    decoder: Decoder\n",
        "\n",
        "    def __call__(self, x: jnp.ndarray) -> VAEOutput:\n",
        "        x = x.astype(jnp.float32)\n",
        "\n",
        "        ######################## CODE AFTER ########################\n",
        "        # Get z, mean, stddev as outputs of self.encoder\n",
        "        z, mean, stddev =\n",
        "\n",
        "        # Get z, mean, stddev as outputs of self.encoder\n",
        "        logits =\n",
        "        ######################## CODE BEFORE ########################\n",
        "\n",
        "        p = jax.nn.sigmoid(logits)\n",
        "        image = jax.random.bernoulli(hk.next_rng_key(), p)\n",
        "\n",
        "        return VAEOutput(image, mean, stddev, logits)\n",
        "\n",
        "@hk.transform\n",
        "def model(x):\n",
        "    ######################## CODE AFTER ########################\n",
        "    # Instantiate VariationalAutoEncoder by passing the Encoder and Decoder.\n",
        "    # Check the Encoder and Decoder classes to see the arguments they need. For output_shape use SAMPLE_SHAPE.\n",
        "\n",
        "    vae = VariationalAutoEncoder(\n",
        "        encoder=,\n",
        "        decoder=,\n",
        "    )\n",
        "    ######################## CODE BEFORE ########################\n",
        "\n",
        "    return vae(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PGxGRrXMQxl"
      },
      "source": [
        "### Loss Functions\n",
        "\n",
        "We use maximum likelihood for training, that is, ideally we would like to maximize:\n",
        "\n",
        "$$\\mathbb{E}_{x \\sim P^*}\\log p_{\\theta}(x)$$\n",
        "\n",
        "Note that $p_{\\theta}(x)$ is the marginal probability distribution $p_{\\theta}(x) = \\int p_\\theta(x, z) dz$. We can rewrite this in familiar terms as $\\int p_\\theta(x|z) p(z) dz$. However, computing (and maximizing) the above marginal is computationally infeasible.\n",
        "\n",
        "Instead, we can show:\n",
        "\n",
        "$$\\log p_{\\theta}(x) \\ge \\mathbb{E}_{z \\sim q(z|x)} \\big[\\log p_\\theta(x | z)\\big] - \\mathbb{KL}\\big(q_\\phi(z | x) || p(z)\\big)$$\n",
        "\n",
        "This right hand side is called the evidence lower bound (ELBO). Broadly speaking the term variational methods, like variational inference, refers to this technique of using an approximate posterior distribution and the ELBO; this is where Variational Autoencoder gets its name from too.\n",
        "\n",
        "In order to try to maximize the likelihood, we maximize the lower bound (ELBO) instead, using e.g. Stochastic Gradient Descent. This yields the following loss used with Variational AutoEncoders:\n",
        "\n",
        "<font size=4>\n",
        "<!-- $$ \\\\mathcal{L}(X, z) = \\\\mathbb{E}\\\\big[\\\\log P(X|z)\\\\big] - D_{KL}\\\\big[Q(z|X) \\\\big|\\\\big| P(z)\\\\big].$$ -->\n",
        "$$\n",
        "\\mathcal{L}(x) = - \\Big( \\mathbb{E}_{z \\sim q(z|x)} \\big [ \\log p_\\theta(x | z)\\big] - \\mathbb{KL}\\big(q_\\phi(z | x) || p(z)\\big) \\Big)\n",
        "$$\n",
        "</font>\n",
        "\n",
        "<br>\n",
        "\n",
        "Therefore, training this model is called Stochastic Variational Inference.\n",
        "\n",
        "Observe that:\n",
        "* The first term encourages the model to reconstruct the input faithfully. This part is similar to the Vanilla AutoEncoder.\n",
        "* The second term can be seen as a *regularization term* of the encoder towards the prior.\n",
        "\n",
        "(The formula contains an expectation; in practice that would be approximated with one or more samples.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mewe-il21VhN"
      },
      "outputs": [],
      "source": [
        "# Used functions:\n",
        "jnp.sum?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "jnp.square?"
      ],
      "metadata": {
        "id": "3wobsMdn9eqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jnp.logaddexp?"
      ],
      "metadata": {
        "id": "UnxOqkTN9hdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RuiKNpWPdT-C"
      },
      "outputs": [],
      "source": [
        "def kl_gaussian(mean: jnp.ndarray, var: jnp.ndarray) -> jnp.ndarray:\n",
        "    r\"\"\"Calculate KL divergence between given and standard gaussian distributions.\n",
        "\n",
        "    KL(p, q) = H(p, q) - H(p) = -\\int p(x)log(q(x))dx - -\\int p(x)log(p(x))dx\n",
        "            = 0.5 * [log(|s2|/|s1|) - 1 + tr(s1/s2) + (m1-m2)^2/s2]\n",
        "            = 0.5 * [-log(|s1|) - 1 + tr(s1) + m1^2] (if m2 = 0, s2 = 1)\n",
        "\n",
        "    hint: https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#kullback-leibler\n",
        "\n",
        "    Args:\n",
        "        mean: mean vector of the first distribution\n",
        "        var: diagonal vector of covariance matrix of the first distribution\n",
        "\n",
        "    Returns:\n",
        "        A scalar representing KL divergence of the two Gaussian distributions.\n",
        "    \"\"\"\n",
        "    kl_coeff = 0.5\n",
        "\n",
        "    ######################## CODE AFTER ########################\n",
        "    # Implement KL loss following the equation and pseudocode above and using the jnp functions from the code block above.\n",
        "    # hints: jnp.sum, jnp.log, jnp.square\n",
        "\n",
        "    kl_loss =\n",
        "    ######################## CODE BEFORE ########################\n",
        "\n",
        "    return kl_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REcARgbLi3iz"
      },
      "outputs": [],
      "source": [
        "def binary_cross_entropy(x: jnp.ndarray, logits: jnp.ndarray) -> jnp.ndarray:\n",
        "    \"\"\"Calculate binary (logistic) cross-entropy from distribution logits.\n",
        "\n",
        "    Args:\n",
        "        x: input variable tensor, must be of same shape as logits\n",
        "        logits: log odds of a Bernoulli distribution, i.e. log(p/(1-p))\n",
        "\n",
        "        hint: https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#cross-entropy\n",
        "\n",
        "    Returns:\n",
        "        A scalar representing binary CE for the given Bernoulli distribution.\n",
        "    \"\"\"\n",
        "    if x.shape != logits.shape:\n",
        "        raise ValueError(\"inputs x and logits must be of the same shape\")\n",
        "\n",
        "    x = jnp.reshape(x, (x.shape[0], -1))\n",
        "    logits = jnp.reshape(logits, (logits.shape[0], -1))\n",
        "\n",
        "    ######################## CODE AFTER ########################\n",
        "    # Implement Binary cross-entropy loss following the equation and pseudocode above and using the jnp functions from 2 code blocks above.\n",
        "    # hint: jnp.sum, jnp.logandexp -> https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.logaddexp.html\n",
        "    bce_loss =\n",
        "    ######################## CODE BEFORE ########################\n",
        "\n",
        "    return bce_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zDu8pNOj23H"
      },
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def loss_fn(\n",
        "    params: hk.Params,\n",
        "    rng_key: PRNGKey,\n",
        "    batch: Batch,\n",
        ") -> jnp.ndarray:\n",
        "    \"\"\"ELBO: E_p[log(x)] - KL(d||q), where p ~ Be(0.5) and q ~ N(0,1).\"\"\"\n",
        "    outputs: VAEOutput = model.apply(params, rng_key, batch[\"image\"])\n",
        "\n",
        "    ######################## CODE AFTER ########################\n",
        "    # Calculate ELBO using the equations and pseudocode above.\n",
        "\n",
        "    log_likelihood = -binary_cross_entropy(batch['image'], outputs.logits)\n",
        "    kl = kl_gaussian(outputs.mean, jnp.square(outputs.stddev))\n",
        "\n",
        "    elbo = log_likelihood - kl\n",
        "    ######################## CODE BEFORE ########################\n",
        "\n",
        "    return -jnp.mean(elbo)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zez5sMLcMQxo"
      },
      "source": [
        "## Training and Visualisation  <a class=\"anchor\" id=\"training\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-AtkQ_v0ZHl"
      },
      "outputs": [],
      "source": [
        "class TrainingState(NamedTuple):\n",
        "    params: hk.Params\n",
        "    opt_state: optax.OptState\n",
        "    rng_key: jax.Array\n",
        "\n",
        "\n",
        "def get_config():\n",
        "  batch_size = 16 #@param\n",
        "  learning_rate = 0.001 #@param\n",
        "  training_steps = 2000 #@param\n",
        "  eval_frequency = 200 #@param\n",
        "  random_seed = 42 #@param\n",
        "  kappa_constrained = 0.1 #@param\n",
        "\n",
        "  return locals()\n",
        "\n",
        "\n",
        "config = get_config()\n",
        "\n",
        "######################## CODE AFTER ########################\n",
        "# You can choose one of the optimizers (https://optax.readthedocs.io/en/latest/api.html)\n",
        "\n",
        "optimizer =\n",
        "\n",
        "######################## CODE BEFORE ########################\n",
        "\n",
        "@jax.jit\n",
        "def update(state: TrainingState, batch: Batch) -> TrainingState:\n",
        "    \"\"\"Performs a single SGD step.\"\"\"\n",
        "    rng_key, next_rng_key = jax.random.split(state.rng_key)\n",
        "\n",
        "    gradients = jax.grad(loss_fn)(state.params, rng_key, batch)\n",
        "\n",
        "    updates, new_opt_state = optimizer.update(gradients, state.opt_state)\n",
        "\n",
        "    new_params = optax.apply_updates(state.params, updates)\n",
        "\n",
        "    return TrainingState(new_params, new_opt_state, next_rng_key)\n",
        "\n",
        "\n",
        "def train_simple_vae(config):\n",
        "    # Load datasets\n",
        "    train_ds = load_dataset(tfds.Split.TRAIN, config['batch_size'],\n",
        "                            config['random_seed'])\n",
        "    valid_ds = load_dataset(tfds.Split.TEST, config['batch_size'],\n",
        "                            config['random_seed'])\n",
        "\n",
        "    # Initialization\n",
        "    initial_rng_key = jax.random.PRNGKey(config['random_seed'])\n",
        "    initial_params = model.init(initial_rng_key, next(train_ds)['image'])\n",
        "    initial_opt_state = optimizer.init(initial_params)\n",
        "    state = TrainingState(initial_params, initial_opt_state, initial_rng_key)\n",
        "\n",
        "    img_dict = {'step': [], 'image': [], 'elbo': []}\n",
        "    loss_acc = []\n",
        "    for step in range(config['training_steps']):\n",
        "\n",
        "        state = update(\n",
        "            state,\n",
        "            next(train_ds),\n",
        "        )\n",
        "\n",
        "        if step % config['eval_frequency'] == 0:\n",
        "            batch = next(valid_ds)\n",
        "            val_loss = loss_fn(state.params, state.rng_key, batch)\n",
        "\n",
        "            print(\"Step: {}; Validation ELBO: {}\".format(step, val_loss))\n",
        "\n",
        "            outputs = model.apply(state.params, state.rng_key, batch[\"image\"])\n",
        "\n",
        "            rnd_idx = jax.random.randint(state.rng_key, (1, ), 0,\n",
        "                                         len(outputs))[0]\n",
        "\n",
        "            img_dict['step'].append(step)\n",
        "            img_dict['image'].append(outputs.image.astype(int)[rnd_idx])\n",
        "            img_dict['elbo'].append(val_loss.item())\n",
        "\n",
        "    return img_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqiWAyDyaMkr"
      },
      "outputs": [],
      "source": [
        "img_dict = train_simple_vae(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U56VkllEfVnz"
      },
      "outputs": [],
      "source": [
        "def plot_training_progress(img_dict):\n",
        "    fig, axes = plt.subplots(1, len(img_dict['step']), figsize=(50, 150))\n",
        "\n",
        "    for i in range(len(img_dict['step'])):\n",
        "\n",
        "        axes[i].set_title('Step: {}\\nELBO: {:.4f}'.format(\n",
        "            img_dict['step'][i], img_dict['elbo'][i]))\n",
        "        axes[i].imshow(img_dict['image'][i])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_loss(img_dict):\n",
        "  fig, ax = plt.subplots(1, 1)\n",
        "\n",
        "  ax.plot(img_dict['step'], np.array(img_dict['elbo']))\n",
        "\n",
        "  ax.set_title(\"Validation Loss\")\n",
        "  ax.set_ylabel('ELBO Loss')\n",
        "  ax.set_xlabel('Steps')\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "plot_training_progress(img_dict)\n",
        "plot_loss(img_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0unQt8RRJOS9"
      },
      "source": [
        "## Analysis  <a class=\"anchor\" id=\"analysis\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVXQ5Axr1VhR"
      },
      "source": [
        "1. What's the effect of changing the batch size regarding the performance and training efficiency? Can you comment on that?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Xl_gMNl1VhR"
      },
      "source": [
        "2. How the latent space size effects the performance of VAEs?  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMVnlrVl1VhR"
      },
      "source": [
        "3. Introduce coefficients to weight the KL divergence and cross entropy loss? How the training process is affected?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}