{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z2wQ87fHPtll"
   },
   "source": [
    "# Practical 2: Vector Quantization for Learning Representations\n",
    "---\n",
    "\n",
    "**Tutorial overview:**\n",
    "In this tutorial you will learn about and implement a Vector Quantization layer for learning discrete representations of images. This tutorial is adapted from the VQ-VAE example in the [Haiku repository](https://github.com/deepmind/dm-haiku).\n",
    "\n",
    "\n",
    "**Tutorial outline:**\n",
    "- [What is vector quantization?](#vq)\n",
    "- [Setup](#setup)\n",
    "  - Install and import packages\n",
    "  - Get MNIST dataset\n",
    "- [Implementing the model](#implementing-the-model)\n",
    "  - Implementing the VQ layer\n",
    "  - Implementing the encoder and decoder\n",
    "- [Training](#training)\n",
    "- [Analysis](#analysis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T31G5_Z_ngc7"
   },
   "source": [
    "## What is vector quantization? <a class=\"anchor\" id=\"what-is-vq\"></a>\n",
    "---\n",
    "\n",
    "[Vector quantization](https://en.wikipedia.org/wiki/Vector_quantization)  (VQ) is a technique used to map continuous vectors to a finite set of discrete vectors, called a \"codebook\". This technique and variants of it are common data compression techniques.\n",
    "\n",
    "In representation learning, we'll often train a model specifically for the purpose of extracting a representation that it implicitly learns (e.g., in one of its hidden layer).\n",
    "\n",
    "VQ has been used as a layer in deep learning in order to turn models that would otherwise learn continuous representations into ones that learn discrete ones, including for generative models like VAEs (variational autoencoders) and GANs (generative adversarial networks), which you'll see later this week. The 2017 paper [Neural Discrete Representation Learning](https://arxiv.org/abs/1711.00937) introduces VQ-VAE, which makes the encoder output discrete codes. In this practical, you will implement the VQ layer introduced in the paper.\n",
    "\n",
    "\n",
    "A lot of state-of-the-art image generation work relies on first learning discrete representations, such as [DALL-E](https://arxiv.org/abs/2102.12092) and [Phenaki](https://sites.research.google/phenaki/).  Discrete representations can then be modeled using powerful autoregressive language models. Something to think about: why else might a discrete representation be advantageous over a continuous one?\n",
    "\n",
    "In the following section we'll see how the VQ operation works and how to implement it.\n",
    "\n",
    "\n",
    "#### Quick note on autoencoders\n",
    "Below, you'll train a VQ-VAE, but you won't need to know anything about VAEs for this Colab (you will learn about them later this week). For now, just ensure you understand the plain autoencoder, where an **encoder** layer and a **decoder** are composed and trained to reconstruct its input $x$, as follows:\n",
    "$$\n",
    "\\begin{align*}\n",
    "x &= \\text{input (e.g. image)} \\\\\n",
    "\\text{bottleneck } b &= E(x)\n",
    "      &&\\text{for some appropriate encoder }E\\\\\n",
    "\\text{prediction (reconstruction) is } \\hat{x} &= D(b)\n",
    "      &&\\text{for some appropriate decoder }D \\\\\n",
    "loss &= \\mathcal L(x, \\hat x)  &&\\text{with some reconstruction loss} \\mathcal L\n",
    "\\end{align*}\n",
    "$$\n",
    "The loss $\\mathcal L$ encourages $\\hat x$ to be similar to $x$; for images this may simply be the squared error.\n",
    "\n",
    "After training, the value at the bottleneck, $b = E(x)$ can then be used for its learned representation in a downstream task. In this setup, without further changes, $b$ will be a continuous representation of $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uv9h5IGummhl"
   },
   "source": [
    "## Setup <a class=\"anchor\" id=\"setup\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8uVd2e-w27u"
   },
   "source": [
    "### Install and import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L1RvPCKUncPK"
   },
   "outputs": [],
   "source": [
    "!pip install -q dm-haiku optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "94TQXf3Zd_wP"
   },
   "outputs": [],
   "source": [
    "import haiku as hk\n",
    "import jax\n",
    "import matplotlib.pyplot as plt\n",
    "import optax\n",
    "from jax import jit\n",
    "from jax import numpy as jnp\n",
    "from jax import random\n",
    "import numpy as np\n",
    "# TensorFlow used only for datasets:\n",
    "from tensorflow.keras import datasets\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ihgyDi4tw8-q"
   },
   "source": [
    "### Get MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tHo68kCrw7oq"
   },
   "outputs": [],
   "source": [
    "def load_mnist():\n",
    "    (x_train, _), (x_test, _) = datasets.mnist.load_data()\n",
    "\n",
    "    # Rescale images to [-1, 1]\n",
    "    x_train = (x_train.astype(np.float32) / 255.0) * 2.0 - 1.0\n",
    "    x_train = np.expand_dims(x_train, axis=-1)\n",
    "    x_test = (x_test.astype(np.float32) / 255.0) * 2.0 - 1.0\n",
    "    x_test = np.expand_dims(x_test, axis=-1)\n",
    "\n",
    "    # For convenience later on, we'll pad MNIST from 28x28 to 32x32.\n",
    "    # Black pixels are -1.\n",
    "    pad_width = ((2, 2), (2, 2))\n",
    "    x_train = np.pad(x_train, ((0, 0),) + pad_width + ((0, 0),), mode='constant', constant_values=-1)\n",
    "    x_test = np.pad(x_test, ((0, 0),) + pad_width + ((0, 0),), mode='constant', constant_values=-1)\n",
    "\n",
    "    return x_train, x_test\n",
    "\n",
    "mnist_train, mnist_test = load_mnist()\n",
    "print(mnist_train.shape, mnist_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-BDRBzeuzeWq"
   },
   "outputs": [],
   "source": [
    "# Display the first few examples in the training set.\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(np.hstack(mnist_train[:10].squeeze()), cmap='Greys_r')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5IP5bP6txn9v"
   },
   "source": [
    "## Implementing the model <a class=\"anchor\" id=\"implementing-the-model\"></a>\n",
    "\n",
    "Haiku, like other neural network libraries, divides large computational graphs into _modules_ or _layers_ that together can be composed into an arbitrarily complex model. We'll start by implementing a module for vector quantization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AhjcHeXKzSjI"
   },
   "source": [
    "### Vector quantization layer\n",
    "\n",
    "The vector quantization layer keeps track of a set of embeddings for each discrete code in its codebook.\n",
    "\n",
    "On the forward pass, the layer finds the nearest embedding for each item in the input and replaces it with the corresponding discrete code.\n",
    "\n",
    "Since this operation is not differentiable, on the backward pass, the gradients are passed through the original continuous representations (bypassing the discrete embeddings), using the straight-through estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VSYSR3BNxcTT"
   },
   "outputs": [],
   "source": [
    "class VectorQuantizer(hk.Module):\n",
    "    def __init__(self, embedding_dim, num_embeddings, commitment_cost):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.commitment_cost = commitment_cost\n",
    "\n",
    "    def __call__(self, inputs, is_training):\n",
    "        flat_inputs = jnp.reshape(inputs, [-1, self.embedding_dim])\n",
    "        embeddings = hk.get_parameter(\n",
    "            \"embeddings\",\n",
    "            [self.embedding_dim, self.num_embeddings],\n",
    "            init=hk.initializers.RandomUniform())\n",
    "\n",
    "        # Quantization operation: compute (squared) distances, then find the\n",
    "        # indices of the nearest neighbors. The original vectors are then\n",
    "        # replaced with the corresponding entries in the codebook.\n",
    "\n",
    "        # START OF SECTION TO FILL IN\n",
    "        # Squared distance avoids the need to expand_dims on flat_inputs:\n",
    "        distances = (jnp.sum(jnp.square(flat_inputs), axis=1, keepdims=True) -\n",
    "                     2 * jnp.matmul(flat_inputs, embeddings) +\n",
    "                     jnp.sum(jnp.square(embeddings), axis=0, keepdims=True))\n",
    "\n",
    "        # Using one-hot lets us compute avg_probs below more easily\n",
    "        encoding_indices = jnp.argmin(distances, axis=1)\n",
    "        encodings = jax.nn.one_hot(encoding_indices, self.num_embeddings)\n",
    "        quantized = jnp.take(embeddings.T, encoding_indices, axis=0)\n",
    "        # alternative: quantized = embeddings[:, encoding_indices]\n",
    "\n",
    "        # Identity in the forward pass (== quantized), but forces gradient of\n",
    "        # quantized = gradient flat_inputs, i.e. straight-through estimator\n",
    "        quantized = flat_inputs + jax.lax.stop_gradient(quantized - flat_inputs)\n",
    "        # END OF SECTION TO FILL IN\n",
    "\n",
    "        # Losses: besides the VAE loss (already implemented below, as VAEs have\n",
    "        # not yet been covered - will see this later in the week), we need\n",
    "        # two more losses for the VQ operation:\n",
    "        # FILL IN THE BLANKS:\n",
    "        # - e_latent_loss: encourages the encoder's output (i.e., the continuous\n",
    "        #   representations) to be close to the quantized embeddings\n",
    "        e_latent_loss = jnp.mean(jnp.square(jax.lax.stop_gradient(quantized) - flat_inputs))\n",
    "        # - q_latent_loss: updates the learned embeddings in the codebook to\n",
    "        #   better represent the continuous representations.\n",
    "        q_latent_loss = jnp.mean(jnp.square(quantized - jax.lax.stop_gradient(flat_inputs)))\n",
    "\n",
    "        loss = q_latent_loss + self.commitment_cost * e_latent_loss\n",
    "\n",
    "        return {\n",
    "            \"quantize\": jnp.reshape(quantized, inputs.shape),\n",
    "            \"loss\": loss,\n",
    "            \"encodings\": encodings,\n",
    "            \"encoding_indices\": jnp.reshape(encoding_indices, inputs.shape[:-1])\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0zhe3bIOyNWG"
   },
   "source": [
    "### Convolutional encoder and decoder\n",
    "\n",
    "The architecture here closely follows that in the [VQ-VAE paper](https://arxiv.org/abs/1711.00937).\n",
    "\n",
    "The encoder is several convolutional layers, followed by some ResNet-style blocks. Similar for the decoder.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ndk6em72yMFS"
   },
   "outputs": [],
   "source": [
    "class ResidualStack(hk.Module):\n",
    "  def __init__(self, num_hiddens, num_residual_layers, num_residual_hiddens,\n",
    "               name=None):\n",
    "    super(ResidualStack, self).__init__(name=name)\n",
    "    self._num_hiddens = num_hiddens\n",
    "    self._num_residual_layers = num_residual_layers\n",
    "    self._num_residual_hiddens = num_residual_hiddens\n",
    "\n",
    "    self._layers = []\n",
    "    for i in range(num_residual_layers):\n",
    "      conv3 = hk.Conv2D(\n",
    "          output_channels=num_residual_hiddens,\n",
    "          kernel_shape=(3, 3),\n",
    "          stride=(1, 1),\n",
    "          name=\"res3x3_%d\" % i)\n",
    "      conv1 = hk.Conv2D(\n",
    "          output_channels=num_hiddens,\n",
    "          kernel_shape=(1, 1),\n",
    "          stride=(1, 1),\n",
    "          name=\"res1x1_%d\" % i)\n",
    "      self._layers.append((conv3, conv1))\n",
    "\n",
    "  def __call__(self, inputs):\n",
    "    h = inputs\n",
    "    for conv3, conv1 in self._layers:\n",
    "      conv3_out = conv3(jax.nn.relu(h))\n",
    "      conv1_out = conv1(jax.nn.relu(conv3_out))\n",
    "      h += conv1_out\n",
    "    return jax.nn.relu(h)  # Resnet V1 style\n",
    "\n",
    "\n",
    "class Encoder(hk.Module):\n",
    "  def __init__(self, num_hiddens, num_residual_layers, num_residual_hiddens,\n",
    "               name=None):\n",
    "    super(Encoder, self).__init__(name=name)\n",
    "    self._num_hiddens = num_hiddens\n",
    "    self._num_residual_layers = num_residual_layers\n",
    "    self._num_residual_hiddens = num_residual_hiddens\n",
    "\n",
    "    self._enc_1 = hk.Conv2D(\n",
    "        output_channels=self._num_hiddens // 2,\n",
    "        kernel_shape=(4, 4),\n",
    "        stride=(2, 2),\n",
    "        name=\"enc_1\")\n",
    "    self._enc_2 = hk.Conv2D(\n",
    "        output_channels=self._num_hiddens,\n",
    "        kernel_shape=(4, 4),\n",
    "        stride=(2, 2),\n",
    "        name=\"enc_2\")\n",
    "    self._enc_3 = hk.Conv2D(\n",
    "        output_channels=self._num_hiddens,\n",
    "        kernel_shape=(4, 4),\n",
    "        stride=(2, 2),  # was 1,1\n",
    "        name=\"enc_3\")\n",
    "    self._residual_stack = ResidualStack(\n",
    "        self._num_hiddens,\n",
    "        self._num_residual_layers,\n",
    "        self._num_residual_hiddens)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    h = jax.nn.relu(self._enc_1(x))\n",
    "    h = jax.nn.relu(self._enc_2(h))\n",
    "    h = jax.nn.relu(self._enc_3(h))\n",
    "    return self._residual_stack(h)\n",
    "\n",
    "\n",
    "class Decoder(hk.Module):\n",
    "  def __init__(self, num_hiddens, num_residual_layers, num_residual_hiddens,\n",
    "               name=None):\n",
    "    super(Decoder, self).__init__(name=name)\n",
    "    self._num_hiddens = num_hiddens\n",
    "    self._num_residual_layers = num_residual_layers\n",
    "    self._num_residual_hiddens = num_residual_hiddens\n",
    "\n",
    "    self._dec_1 = hk.Conv2DTranspose(\n",
    "        output_channels=self._num_hiddens,\n",
    "        kernel_shape=(4, 4),\n",
    "        stride=(2, 2),\n",
    "        name=\"dec_1\")\n",
    "    self._residual_stack = ResidualStack(\n",
    "        self._num_hiddens,\n",
    "        self._num_residual_layers,\n",
    "        self._num_residual_hiddens)\n",
    "    self._dec_2 = hk.Conv2DTranspose(\n",
    "        output_channels=self._num_hiddens // 2,\n",
    "        kernel_shape=(4, 4),\n",
    "        stride=(2, 2),\n",
    "        name=\"dec_2\")\n",
    "    self._dec_3 = hk.Conv2DTranspose(\n",
    "        output_channels=1,\n",
    "        kernel_shape=(4, 4),\n",
    "        stride=(2, 2),\n",
    "        name=\"dec_3\")\n",
    "\n",
    "  def __call__(self, x):\n",
    "    h = self._dec_1(x)\n",
    "    h = self._residual_stack(h)\n",
    "    h = jax.nn.relu(self._dec_2(h))\n",
    "    x_recon = self._dec_3(h)\n",
    "    return x_recon\n",
    "\n",
    "\n",
    "class VQVAEModel(hk.Module):\n",
    "  def __init__(self, encoder, decoder, vqvae, pre_vq_conv1,\n",
    "               data_variance, name=None):\n",
    "    super(VQVAEModel, self).__init__(name=name)\n",
    "    self._encoder = encoder\n",
    "    self._decoder = decoder\n",
    "    self._vqvae = vqvae\n",
    "    self._pre_vq_conv1 = pre_vq_conv1\n",
    "    self._data_variance = data_variance\n",
    "\n",
    "  def __call__(self, inputs, is_training):\n",
    "    z = self._pre_vq_conv1(self._encoder(inputs))\n",
    "    vq_output = self._vqvae(z, is_training=is_training)\n",
    "    x_recon = self._decoder(vq_output['quantize'])\n",
    "    recon_error = jnp.mean((x_recon - inputs) ** 2) / self._data_variance\n",
    "    loss = recon_error + vq_output['loss']\n",
    "    return {\n",
    "        'z': z,\n",
    "        'x_recon': x_recon,\n",
    "        'loss': loss,\n",
    "        'recon_error': recon_error,\n",
    "        'vq_output': vq_output,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g2vhVvz8z8kb"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_hiddens = 64\n",
    "num_residual_hiddens = 32\n",
    "num_residual_layers = 2\n",
    "embedding_dim = 32\n",
    "num_embeddings = 16\n",
    "commitment_cost = 0.25\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# For scaling reconstruction error\n",
    "train_data_variance = np.var(mnist_train)\n",
    "\n",
    "# Build modules.\n",
    "def forward(data, is_training):\n",
    "    encoder = Encoder(num_hiddens, num_residual_layers, num_residual_hiddens)\n",
    "    decoder = Decoder(num_hiddens, num_residual_layers, num_residual_hiddens)\n",
    "    pre_vq_conv1 = hk.Conv2D(\n",
    "        output_channels=embedding_dim,\n",
    "        kernel_shape=(1, 1),\n",
    "        stride=(1, 1),\n",
    "        name=\"to_vq\")\n",
    "\n",
    "    vq_vae = hk.nets.VectorQuantizer(\n",
    "        embedding_dim=embedding_dim,\n",
    "        num_embeddings=num_embeddings,\n",
    "        commitment_cost=commitment_cost)\n",
    "\n",
    "    model = VQVAEModel(encoder, decoder, vq_vae, pre_vq_conv1, data_variance=train_data_variance)\n",
    "\n",
    "    return model(data, is_training)\n",
    "\n",
    "forward = hk.transform_with_state(forward)\n",
    "optimizer = optax.adam(learning_rate)\n",
    "\n",
    "@jax.jit\n",
    "def train_step(params, state, opt_state, data):\n",
    "    def adapt_forward(params, state, data):\n",
    "        model_output, state = forward.apply(params, state, None, data, is_training=True)\n",
    "        loss = model_output['loss']\n",
    "        return loss, (model_output, state)\n",
    "\n",
    "    grads, (model_output, state) = jax.grad(adapt_forward, has_aux=True)(params, state, data)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, state, opt_state, model_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mFzU_uVy18Aa"
   },
   "source": [
    "## Training <a class=\"anchor\" id=\"training\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wYXtkO080Om5"
   },
   "outputs": [],
   "source": [
    "# Make minibatch iterators over the MNIST data using TensorFlow dataset API.\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(mnist_train).shuffle(10000).repeat().batch(batch_size)\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices(mnist_test).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OMKFGpWj0oy_"
   },
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "dummy_input = jnp.zeros((1, 32, 32, 1))\n",
    "params = forward.init(jax.random.PRNGKey(42), dummy_input, is_training=True)\n",
    "opt_state = optimizer.init(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DHC4EKe41FC0"
   },
   "source": [
    "Training for 20000 updates takes approximately 2 minutes with a T4 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M3muP2qA1xZh"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "num_training_updates = 20000\n",
    "\n",
    "# Lists to keep track of metrics\n",
    "train_losses = []\n",
    "train_recon_errors = []\n",
    "train_vqvae_loss = []\n",
    "\n",
    "# Initialization\n",
    "rng = jax.random.PRNGKey(42)\n",
    "train_dataset_iter = iter(train_dataset)\n",
    "\n",
    "# Initialize model parameters and optimizer state\n",
    "dummy_data = next(train_dataset_iter).numpy()\n",
    "params, state = forward.init(rng, dummy_data, is_training=True)\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "# Training loop\n",
    "for step in range(1, num_training_updates + 1):\n",
    "    data = next(train_dataset_iter).numpy()\n",
    "    params, state, opt_state, train_results = train_step(params, state, opt_state, data)\n",
    "\n",
    "    train_results = jax.device_get(train_results)\n",
    "    train_losses.append(train_results['loss'])\n",
    "    train_recon_errors.append(train_results['recon_error'])\n",
    "    train_vqvae_loss.append(train_results['vq_output']['loss'])\n",
    "\n",
    "    if step % 1000 == 0:\n",
    "        print(\n",
    "            f'[Step {step}/{num_training_updates}] ' +\n",
    "            f'train loss: {np.mean(train_losses[-100:]):.3f} ' +\n",
    "            f'recon_error: {np.mean(train_recon_errors[-100:]):.3f} ' +\n",
    "            f'vqvae loss: {np.mean(train_vqvae_loss[-100:]):.3f}'\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "13BmPwPL3Fs8"
   },
   "outputs": [],
   "source": [
    "def plot_reconstructions(originals, recons, n=8):\n",
    "    '''Plots original and reconstructed images side by side'''\n",
    "\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    for i in range(n):\n",
    "        # Display original\n",
    "        ax = plt.subplot(2, n, i + 1)\n",
    "        plt.imshow(np.clip(originals[i].squeeze(), 0, 1), cmap='gray')\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        ax.set_title('Original')\n",
    "\n",
    "        # Display reconstruction\n",
    "        ax = plt.subplot(2, n, i + 1 + n)\n",
    "        plt.imshow(np.clip(recons[i].squeeze(), 0, 1), cmap='gray')\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        ax.set_title('Reconstruction')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Obtain a batch from the test set\n",
    "test_batch = next(iter(valid_dataset)).numpy()\n",
    "\n",
    "# Get the reconstructions using the trained VQ-VAE\n",
    "rng = jax.random.PRNGKey(42)\n",
    "result, _ = forward.apply(params, state, rng, test_batch, is_training=False)\n",
    "reconstructions = result['x_recon']\n",
    "\n",
    "# Plot some inputs alongside reconstructions:\n",
    "plot_reconstructions(test_batch, reconstructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MVEmsqpl4ylE"
   },
   "outputs": [],
   "source": [
    "# Obtain the discrete representation of the first digit\n",
    "discrete_representation = result['vq_output']['encoding_indices'][0]  # Taking the first element\n",
    "\n",
    "# Display the discrete representation\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(discrete_representation, cmap='tab20', aspect='auto')\n",
    "plt.colorbar()\n",
    "plt.title('Discrete Representation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r600yK9IC6NY"
   },
   "source": [
    "## Analysis\n",
    "\n",
    "1. Fill in the blank above for the `VectorQuantization` class and make sure the code runs correctly. What is the shape of the learned discrete representation? (See visualization and/or try printing it from the `VectorQuantization` class.)\n",
    "2. The codebook consists of a fixed set of embeddings. How would the model behave if the size of this codebook (number of embeddings) is increased or decreased significantly?\n",
    "3. Discuss potential real-world applications where learned discrete representations may be advantageous."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "uv9h5IGummhl"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
