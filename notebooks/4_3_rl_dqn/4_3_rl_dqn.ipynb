{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Part III 🔥 Advanced: Deep Q-learning (DQN).\n",
        "---"
      ],
      "metadata": {
        "id": "oLyDPfq1INxu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "%%capture\n",
        "\n",
        "!pip install dm-haiku\n",
        "!pip install optax\n",
        "!pip install dm_env\n",
        "!pip install gym[accept-rom-license]\n",
        "!pip install dm-acme[envs]\n",
        "!pip install autorom[accept-rom-license]"
      ],
      "metadata": {
        "id": "bz9ZYSVK5D_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "\n",
        "import collections\n",
        "import random\n",
        "from typing import Sequence\n",
        "\n",
        "import chex\n",
        "import dm_env\n",
        "from dm_env import specs\n",
        "import haiku as hk\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from matplotlib import animation\n",
        "from matplotlib import rc\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import optax\n",
        "import gym\n",
        "from acme.wrappers import atari_wrapper, gym_wrapper\n",
        "from acme import wrappers\n",
        "\n",
        "rc('animation', html='jshtml')\n",
        "import warnings\n",
        "# warnings.filterwarnings(action='once')\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "JAlbDPgu5I1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Some helper functions copied from 4.1\n",
        "\n",
        "_ACTIONS = (-1, 0, 1)  # Move paddle left, no-op, move paddle right.\n",
        "\n",
        "class Catch(dm_env.Environment):\n",
        "  \"\"\"A Catch environment built on the dm_env.Environment class.\n",
        "\n",
        "  The agent must move a paddle to intercept falling balls. Falling balls only\n",
        "  move downwards on the column they are in.\n",
        "\n",
        "  The observation is an array with shape (rows, columns) containing binary\n",
        "  values: 0 if a space is empty; 1 if it contains the paddle and 2 for a ball.\n",
        "\n",
        "  The actions are discrete, and by default there are three available actions:\n",
        "  move left, stay, and move right.\n",
        "\n",
        "  The episode terminates when the ball reaches the bottom of the screen.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               rows: int = 10,\n",
        "               columns: int = 5,\n",
        "               discount: float = 1.0):\n",
        "    \"\"\"Initializes a new Catch environment.\n",
        "\n",
        "    Args:\n",
        "      rows: number of rows.\n",
        "      columns: number of columns.\n",
        "      discount: discount factor for calculating reward.\n",
        "    \"\"\"\n",
        "    self._rows = rows\n",
        "    self._columns = columns\n",
        "    self._discount = discount\n",
        "    self._board = np.zeros((rows, columns), dtype=np.float32)\n",
        "    self._ball_x = None\n",
        "    self._ball_y = None\n",
        "    self._paddle_x = None\n",
        "    self._reset_next_step = True\n",
        "\n",
        "  def reset(self) -> dm_env.TimeStep:\n",
        "    \"\"\"Returns the first `TimeStep` of a new episode.\"\"\"\n",
        "    self._reset_next_step = False\n",
        "    # Ball can drop from any column.\n",
        "    self._ball_x = np.random.randint(self._columns)\n",
        "    self._ball_y = 0  # Top of matrix.\n",
        "    self._paddle_x = self._columns // 2  # Centre.\n",
        "\n",
        "    return dm_env.restart(self._observation())\n",
        "\n",
        "  def step(self, action: int) -> dm_env.TimeStep:\n",
        "    \"\"\"Updates the environment according to the action.\"\"\"\n",
        "    if self._reset_next_step:\n",
        "      return self.reset()\n",
        "\n",
        "    # Move the paddle.\n",
        "    dx = _ACTIONS[action]  # Get action. dx = change in x position.\n",
        "    # Clip to keep paddle in bounds of the environment matrix.\n",
        "    self._paddle_x = np.clip(self._paddle_x + dx, 0, self._columns - 1)\n",
        "\n",
        "    # -----------------------------------#\n",
        "    # Drop the ball down one row: increase y coordinate of the ball by 1.\n",
        "    self._ball_y += 1\n",
        "    # -----------------------------------#\n",
        "\n",
        "    # Check for termination.\n",
        "    if self._ball_y == self._rows - 1:  # Ball has fallen below the rows.\n",
        "      # Reward depends on whether the paddle is on the ball (positions match).\n",
        "      reward = 1. if self._paddle_x == self._ball_x else -1.\n",
        "      self._reset_next_step = True\n",
        "      return dm_env.termination(reward=reward, observation=self._observation())\n",
        "\n",
        "    return dm_env.transition(reward=0., observation=self._observation(),\n",
        "                             discount=self._discount)\n",
        "\n",
        "  def observation_spec(self) -> specs.BoundedArray:\n",
        "    \"\"\"Returns the observation spec.\"\"\"\n",
        "    return specs.BoundedArray(\n",
        "        shape=self._board.shape,\n",
        "        dtype=self._board.dtype,\n",
        "        name='board',\n",
        "        minimum=0,\n",
        "        maximum=2)\n",
        "\n",
        "  def action_spec(self) -> specs.DiscreteArray:\n",
        "    \"\"\"Returns the action spec.\"\"\"\n",
        "    return specs.DiscreteArray(\n",
        "        dtype=int, num_values=len(_ACTIONS), name='action')\n",
        "\n",
        "  def _observation(self) -> np.ndarray:\n",
        "    self._board.fill(0.)\n",
        "    self._board[self._ball_y, self._ball_x] = 2.\n",
        "    self._board[self._rows - 1, self._paddle_x] = 1.\n",
        "\n",
        "    return self._board.copy()\n",
        "\n",
        "# Function to animate the observations\n",
        "def animate(data, interval=200):\n",
        "  fig = plt.figure(1)\n",
        "  img = plt.imshow(data[0])\n",
        "  plt.axis('off')\n",
        "\n",
        "  def animate(i):\n",
        "    img.set_data(data[i])\n",
        "\n",
        "  anim = animation.FuncAnimation(fig, animate, frames=len(data), interval=interval)\n",
        "  plt.close(1)\n",
        "  return anim\n",
        "\n",
        "Transition = collections.namedtuple(\n",
        "    'Transition', 'obs_tm1 a_tm1 r_t discount_t obs_t')\n",
        "\n",
        "class TransitionAccumulator:\n",
        "  \"\"\"Simple Python accumulator for transitions.\"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    self._prev = None\n",
        "    self._action = None\n",
        "    self._latest = None\n",
        "\n",
        "  def push(self, env_output, action):\n",
        "    self._prev = self._latest\n",
        "    self._action = action\n",
        "    self._latest = env_output\n",
        "\n",
        "  def sample(self):\n",
        "    return Transition(self._prev.observation, self._action, self._latest.reward,\n",
        "                      self._latest.discount, self._latest.observation)\n",
        "\n",
        "  def is_ready(self):\n",
        "    \"\"\"Checks if there is previous data stored.\"\"\"\n",
        "    return self._prev is not None"
      ],
      "metadata": {
        "cellView": "form",
        "id": "rRQi8dgsSYD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "So far, we considered look-up tables for value-based RL and neural network as a policy in policy-gradient method. Now we will look into Q-learning with neural network as a function apploximation because look-up table is only possible in this simple environment where the number of states is quite small. This is not scalable to situations where, say, the goal location changes or the obstacles are in different locations at every episode (consider how big the table should be in this situation). Imagine also playing ATARI from pixels where the observation is a frame and the number of possible frames is exponential in the number of pixels on the screen.\n",
        "\n",
        "<center><img width=\"200\" src=\"https://storage.googleapis.com/dm-educational/assets/reinforcement-learning-summer-school/atari.gif\"></center>\n",
        "\n"
      ],
      "metadata": {
        "id": "7eIVE38m-zN_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural net-based Q-learning\n",
        "\n",
        "What we want is just being able to compute the Q-value when fed with a particular $(s, a)$ pair. So we will have a function to do this instead of keeping a big table. We will use function approximation as a way to generalise Q-values over some representation of a very large state space, and we will train our approximator to output accurate Q-value estimates. In this section, we will explore Q-learning with function approximation, which, although theoretically proven to diverge for some degenerate MDPs, can yield impressive results in very large environments. [Playing Atari with Deep Reinforcement Learning](https://deepmind.com/research/publications/playing-atari-deep-reinforcement-learning)  introduced the first deep learning model to successfully learn control policies directly from high-dimensional pixel inputs using RL, and we're going to implement a simplified version of that agent here!\n",
        "\n",
        "So, we will predict $Q(s, a)$ using a neural network $f()$, which given a vector $s$, will output a vector of Q-values for all possible actions $a$. As a loss to optimise, we will use the same loss as in tabular setting: **TD error**. By training our neural network to output values such that the TD error is minimised, we will also satisfy the Bellman Optimality Equation, which is a good sufficient condition to enforce so that we may obtain an optimal policy.\n",
        "We will write the TD error as a loss (e.g., with an $L2$ loss, but others would work too), compute its gradient (which are now gradients with respect to individual parameters of the neural network) and slowly improve our Q-value approximation:\n",
        "\n",
        "$$Loss = \\mathbb{E}\\left[ \\left(r + \\gamma \\max_{a'} Q(s', a') − Q(s, a)  \\right)^2\\right].$$"
      ],
      "metadata": {
        "id": "1vcprG_iJYiD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ⭐ Exercise\n",
        "\n",
        "1) Fill in the gap to define the network function, similar to how it was done in the previous section 4.2. For our function approximator, we're going to use an [MLP](https://en.wikipedia.org/wiki/Multilayer_perceptron) that takes the observation and outputs Q values for each of the actions. We construct the MLP inside the `__init__` function. We are going to use [Jax](https://github.com/google/jax) and [Haiku](https://github.com/deepmind/dm-haiku) to implement and train our neural nets.\n",
        "\n",
        "2) Now we have an agent that uses an MLP to compute Q-values. But in its current state, the MLP params are just initialised randomly and not changed at all. We need to add a TD-Learning algorithm to our agent. `learner_step` will receive a collection of data that is collected from interacting with the environment using the `actor_step` function and then update the network parameters by computing the gradient of the loss function with respect to the network parameters. Complete the learner_step function to compute the gradient of the loss with respect to the neural network parameters and make a step. Again, this is very similar to the section 4.3, use that implementation as reference."
      ],
      "metadata": {
        "id": "5KMCcccmSSVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QlearningAgent(object):\n",
        "  \"\"\"Q-learning agent.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               action_spec: specs.DiscreteArray,\n",
        "               observation_spec: specs.DiscreteArray,\n",
        "               num_hiddens: Sequence[int] = [50],\n",
        "               epsilon: float = 0.01,\n",
        "               learning_rate: float = 0.005):\n",
        "    self._observation_spec = observation_spec\n",
        "    self._num_actions = action_spec.num_values\n",
        "    self._epsilon = epsilon\n",
        "    self._optimizer = optax.adam(learning_rate)\n",
        "\n",
        "    def network(obs):\n",
        "      \"\"\"Q network of the agent.\"\"\"\n",
        "      obs = jnp.expand_dims(obs, 0)\n",
        "      # -----------------------------------#\n",
        "\n",
        "      return\n",
        "      # -----------------------------------#\n",
        "\n",
        "    self._network = hk.without_apply_rng(hk.transform(network, apply_rng=True))\n",
        "    # Jitting for speed.\n",
        "    self.actor_step = jax.jit(self.actor_step)\n",
        "    self.learner_step = jax.jit(self.learner_step)\n",
        "\n",
        "  def initial_params(self, rng_key):\n",
        "    \"\"\"Initialises the agent params given the RNG key.\"\"\"\n",
        "    sample_input = self._observation_spec.generate_value()\n",
        "    sample_input = jnp.expand_dims(sample_input, 0)\n",
        "    return self._network.init(rng_key, sample_input)\n",
        "\n",
        "  def initial_learner_state(self, params):\n",
        "    return self._optimizer.init(params)\n",
        "\n",
        "  def actor_step(self, params, timestep, rng_key, evaluation):\n",
        "    \"\"\"Given the observation, computes the action using epsilon-greedy algorithm.\"\"\"\n",
        "    qvalues = self._network.apply(params, timestep.observation)\n",
        "    if np.random.random() > self._epsilon:\n",
        "      train_a = jnp.argmax(qvalues)\n",
        "    else:\n",
        "      train_a = jax.random.choice(rng_key, self._num_actions)\n",
        "\n",
        "    # If evaluating, return the greedy action. Otherwise, return the\n",
        "    # epsilon-greedy action.\n",
        "    return jax.lax.select(evaluation, jnp.argmax(qvalues), train_a)\n",
        "\n",
        "  def learner_step(self, params: hk.Params, data, learner_state, rng_key):\n",
        "    \"\"\"Computes loss, its gradient w.r.t. params, and runs an optimisation step.\"\"\"\n",
        "    # -----------------------------------#\n",
        "    # get the gradient of the loss and the loss of function _loss with respect to params on data\n",
        "\n",
        "    # get the updates and learner_step from _optimizer\n",
        "\n",
        "    # use optax to modify the params with updates and get a new set of params\n",
        "\n",
        "    # -----------------------------------#\n",
        "    return params, learner_state, loss\n",
        "\n",
        "  def _loss(self, params, obs_tm1, a_tm1, r_t, discount_t, obs_t):\n",
        "    \"\"\"Computes the TD error loss.\"\"\"\n",
        "    q_tm1 = self._network.apply(params, obs_tm1)[0,:] # use [0,:] to cut batch dimension\n",
        "    q_t = self._network.apply(params, obs_t)[0,:]\n",
        "\n",
        "    chex.assert_rank([q_tm1, a_tm1, r_t, discount_t, q_t], [1, 0, 0, 0, 1])\n",
        "    chex.assert_type([q_tm1, a_tm1, r_t, discount_t, q_t],\n",
        "                     [float, int, float, float, float])\n",
        "\n",
        "    target_tm1 = r_t + discount_t * jnp.max(q_t)\n",
        "    target_tm1 = jax.lax.stop_gradient(target_tm1)\n",
        "    td_error = target_tm1 - q_tm1[a_tm1]\n",
        "    loss = 0.5 * td_error ** 2\n",
        "    return loss, loss\n"
      ],
      "metadata": {
        "id": "6820CX42_H0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Loop\n",
        "Now we are ready to write the training loop. We're going to use the same transition accumulator from the tabular Q-learning. Before starting the training, we need to initialise the agent's and optimiser's parameters.\n"
      ],
      "metadata": {
        "id": "5YnL3zvq_PwL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_episodes = 1_001\n",
        "evaluate_every = 50\n",
        "eval_episodes = 10\n",
        "seed = 1221\n",
        "\n",
        "rng = hk.PRNGSequence(jax.random.PRNGKey(seed))\n",
        "\n",
        "# # Initialise the environment.\n",
        "env = Catch()\n",
        "\n",
        "timestep = env.reset()\n",
        "\n",
        "# Build and initialise the agent.\n",
        "agent = QlearningAgent(env.action_spec(),\n",
        "                       env.observation_spec(),\n",
        "                       num_hiddens=[20,20],\n",
        "                       epsilon=0.2,\n",
        "                       learning_rate=1e-3)\n",
        "params = agent.initial_params(next(rng))\n",
        "learner_state = agent.initial_learner_state(params)\n",
        "\n",
        "# Initialise the accumulator.\n",
        "accumulator = TransitionAccumulator()\n",
        "\n",
        "# Run loop.\n",
        "avg_returns = []\n",
        "losses = []\n",
        "\n",
        "for episode in range(train_episodes):\n",
        "\n",
        "  # Prepare agent, environment and accumulator for a new episode.\n",
        "  timestep = env.reset()\n",
        "  accumulator.push(timestep, None)\n",
        "\n",
        "  while not timestep.last():\n",
        "    # Acting.\n",
        "    action = agent.actor_step(params, timestep, next(rng), False)\n",
        "    # Agent-environment interaction.\n",
        "    timestep = env.step(int(action))\n",
        "    # Accumulate experience.\n",
        "    accumulator.push(timestep, action)\n",
        "\n",
        "    # Learning.\n",
        "    if accumulator.is_ready():\n",
        "      params, learner_state, loss = agent.learner_step(\n",
        "          params, accumulator.sample(), learner_state, next(rng))\n",
        "      losses.append(np.asarray(loss))\n",
        "\n",
        "  # Evaluation.\n",
        "  if not episode % evaluate_every:\n",
        "    returns = []\n",
        "    for _ in range(eval_episodes):\n",
        "      timestep = env.reset()\n",
        "      timesteps = [timestep]\n",
        "      while not timestep.last():\n",
        "        action = agent.actor_step(params, timestep, next(rng), True)\n",
        "        timestep = env.step(int(action))\n",
        "        timesteps.append(timestep)\n",
        "      returns.append(np.sum([item.reward for item in timesteps[1:]]))\n",
        "\n",
        "    avg_returns.append(np.mean(returns))\n",
        "    print(f\"Episode {episode:4d}: Average returns: {avg_returns[-1]:.2f}\")"
      ],
      "metadata": {
        "id": "5zgzQXXz_S4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ⭐ Exercise\n",
        "\n",
        "- How is the learning progresses? Plot both the loss function and the average return that the agent achieves (consider plotting the moving average with `np.convolve` if needed). How stable is the learning (e.g., does the loss go gradually down)?\n",
        "- Try replacing the Catch environment with CartPole from Policy gradient section. What happens then?"
      ],
      "metadata": {
        "id": "FbX-bvV7NHUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------#\n",
        "\n",
        "# -----------------------------------#"
      ],
      "metadata": {
        "id": "n3kJMemQBPpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0osEU_5DmR2"
      },
      "source": [
        "## DQN Agent\n",
        "\n",
        "The agent we implemented above, while very successful on some tasks like [TD-Gammon](https://en.wikipedia.org/wiki/TD-Gammon), suffers from divergence issues and is hard to train for more complicated tasks (you might have noticed the issues already in the simple environments that we considered).\n",
        "\n",
        "The [Deep Q-Learning Agent (DQN)](https://deepmind.com/research/publications/2019/playing-atari-deep-reinforcement-learning) that was first introduced to play Atari games from pixels improves on the Q-learning agent by incorporating two main ideas:\n",
        "\n",
        "*   `Replay buffer`: To alleviate the problems of correlated data and non-stationary distributions.\n",
        "*   `Target network`: Use of an iterative update that adjusts the action-values (Q) towards target values that are only periodically updated, thereby reducing correlations with the target\"."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Replay buffer\n",
        "\n",
        "We modify the `TransitionAccumulator` slightly by adding a queue to collect more than one transition. Notice also that `sample` function now takes `batch_size` parameter, similar to what we did in 4.2."
      ],
      "metadata": {
        "id": "sWhCG51oQD82"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIEh7hUoDvTS"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer(object):\n",
        "  \"\"\"A simple Python replay buffer.\"\"\"\n",
        "\n",
        "  def __init__(self, capacity, discount_factor=0.99):\n",
        "    self._discount_factor = discount_factor\n",
        "    self._prev = None\n",
        "    self._action = None\n",
        "    self._latest = None\n",
        "    self.buffer = collections.deque(maxlen=capacity)\n",
        "\n",
        "  def push(self, env_output, action):\n",
        "    self._prev = self._latest\n",
        "    self._action = action\n",
        "    self._latest = env_output\n",
        "\n",
        "    if action is not None:\n",
        "      self.buffer.append(\n",
        "          (self._prev.observation, self._action, self._latest.reward,\n",
        "           self._latest.discount, self._latest.observation))\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    obs_tm1, a_tm1, r_t, discount_t, obs_t = zip(\n",
        "        *random.sample(self.buffer, batch_size))\n",
        "    return (jnp.stack(obs_tm1), jnp.asarray(a_tm1), jnp.asarray(r_t),\n",
        "            jnp.asarray(discount_t) * self._discount_factor, jnp.stack(obs_t))\n",
        "\n",
        "  def is_ready(self, batch_size):\n",
        "    return batch_size <= len(self.buffer)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Target network\n",
        "\n",
        "The second ingredient we need is a *target network*.\n",
        "\n",
        "At each iteration `i`, the target network computes the DQN loss $L_i$ on the parameters $\\theta_i$, based on a the set of target parameters $\\theta_{i-1}$ and a given batch of sampled trajectories `sample`. As described in the manuscript, the loss function is defined as:\n",
        "\n",
        "$$L_i (\\theta_i) = \\mathbb{E}_{s,a \\sim \\rho(\\cdot)} \\left[ \\left( y_i - Q(s,a ;\\theta_i) \\right)^2\\right],$$\n",
        "\n",
        "where the target $y_i$ is computed using a bootstrap value computed from Q-value network with target parameters:\n",
        "\n",
        "$$ y_i = \\mathbb{E}_{s' \\sim S} \\left[r + \\gamma \\max_{a' \\in A} Q(s', a' ; \\theta^{\\text{target}}_i) \\; | \\; s, a \\right]. $$\n"
      ],
      "metadata": {
        "id": "por9wAbjC69D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Two sets of parameters, for online and for target network\n",
        "Params = collections.namedtuple(\"Params\", \"online target\")\n",
        "# Learner state includes the counter of the learning steps and optimiser state\n",
        "LearnerState = collections.namedtuple(\"LearnerState\", \"count opt_state\")\n",
        "\n",
        "class DQNAgent(object):\n",
        "  \"\"\"Q-learning agent.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               action_spec: specs.DiscreteArray,\n",
        "               observation_spec: specs.DiscreteArray,\n",
        "               num_hiddens: Sequence[int] = [50],\n",
        "               epsilon: float = 0.01,\n",
        "               learning_rate: float = 0.005,\n",
        "               target_period = 10):\n",
        "    self._observation_spec = observation_spec\n",
        "    self._num_actions = action_spec.num_values\n",
        "    self._epsilon = epsilon\n",
        "    self._target_period = target_period\n",
        "    self._optimizer = optax.adam(learning_rate)\n",
        "\n",
        "    def network(obs):\n",
        "      \"\"\"Q network of the agent.\"\"\"\n",
        "      # Unlike the previous version of the agent, here the observation has a\n",
        "      # leading batch dimension.\n",
        "      mlp = hk.Sequential(\n",
        "          [hk.Flatten(),\n",
        "           hk.nets.MLP(num_hiddens + [self._num_actions])])\n",
        "      return mlp(obs)\n",
        "\n",
        "    self._network = hk.without_apply_rng(hk.transform(network, apply_rng=True))\n",
        "    # Jitting for speed.\n",
        "    self.actor_step = jax.jit(self.actor_step)\n",
        "    self.learner_step = jax.jit(self.learner_step)\n",
        "\n",
        "  def initial_params(self, rng_key):\n",
        "    \"\"\"Initialises the agent params given the RNG key.\"\"\"\n",
        "    sample_input = self._observation_spec.generate_value()\n",
        "    sample_input = jnp.expand_dims(sample_input, 0)\n",
        "    online_params = self._network.init(rng_key, sample_input)\n",
        "    return Params(online_params, online_params)\n",
        "\n",
        "  def initial_learner_state(self, params):\n",
        "    learner_count = jnp.zeros((), dtype=jnp.float32)\n",
        "    opt_state = self._optimizer.init(params.online)\n",
        "    return LearnerState(learner_count, opt_state)\n",
        "\n",
        "  def actor_step(self, params, timestep, rng_key, evaluation):\n",
        "    \"\"\"Given the observation, computes the action using epsilon-greedy algorithm.\"\"\"\n",
        "    # The actor step works with batch size 1 but our network expects\n",
        "    # the inputs to have a batch dimension.\n",
        "    obs = jnp.expand_dims(timestep.observation, 0)  # Add dummy batch.\n",
        "    qvalues = self._network.apply(params.online, obs)[0]  # Remove dummy batch.\n",
        "\n",
        "    if np.random.random() > self._epsilon:\n",
        "      train_a = jnp.argmax(qvalues)\n",
        "    else:\n",
        "      train_a = jax.random.choice(rng_key, self._num_actions)\n",
        "    # If evaluating, return the greedy action. Otherwise, return the\n",
        "    # epsilon-greedy action.\n",
        "    return jax.lax.select(evaluation, jnp.argmax(qvalues), train_a)\n",
        "\n",
        "  def learner_step(self, params: hk.Params, data, learner_state, rng_key):\n",
        "    \"\"\"Computes the loss and its gradient with respect to the parameters and\n",
        "    does a step of optimisation.\"\"\"\n",
        "    # Update the target network parameters periodically.\n",
        "    is_time = learner_state.count % self._target_period == 0\n",
        "    target_params = jax.tree_map(\n",
        "        lambda new, old: jax.lax.select(is_time, new, old),\n",
        "        params.online, params.target)\n",
        "\n",
        "    dloss_dtheta, loss = jax.grad(self._loss, has_aux=True)(\n",
        "        params.online, target_params, *data)\n",
        "\n",
        "    updates, opt_state = self._optimizer.update(\n",
        "        dloss_dtheta, learner_state.opt_state)\n",
        "    online_params = optax.apply_updates(params.online, updates)\n",
        "    return (\n",
        "        Params(online_params, target_params),\n",
        "        LearnerState(learner_state.count + 1, opt_state),\n",
        "        loss)\n",
        "\n",
        "  def _loss(self, online_params, target_params, obs_tm1, a_tm1, r_t,\n",
        "            discount_t, obs_t):\n",
        "    \"\"\"Computes the TD error loss.\"\"\"\n",
        "    q_tm1 = self._network.apply(online_params, obs_tm1)\n",
        "    q_t_val = self._network.apply(target_params, obs_t)\n",
        "    q_t_select = self._network.apply(online_params, obs_t)\n",
        "\n",
        "    def q_learning_loss(q_tm1, a_tm1,  r_t, discount_t, q_t_value,\n",
        "                        q_t_selector):\n",
        "      target_tm1 = r_t + discount_t * q_t_value[q_t_selector.argmax()]\n",
        "      target_tm1 = jax.lax.stop_gradient(target_tm1)\n",
        "      return target_tm1 - q_tm1[a_tm1]\n",
        "\n",
        "    batched_loss = jax.vmap(q_learning_loss)\n",
        "    td_error = batched_loss(q_tm1, a_tm1, r_t, discount_t, q_t_val, q_t_select)\n",
        "    loss = jnp.mean(0.5 * td_error ** 2)\n",
        "    return loss, loss"
      ],
      "metadata": {
        "id": "TpCGp4QfCNge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run Loop\n",
        "\n",
        "The training loop for the DQN agent is identical to the one we used above for the Q-learning agent. We only need to change the accumulator and the agent's class."
      ],
      "metadata": {
        "id": "1i-svxUuDEfc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 10  # @param\n",
        "discount_factor = 0.99  # @param\n",
        "replay_buffer_capacity = 100  # @param\n",
        "train_episodes = 300  # @param\n",
        "evaluate_every = 25  # @param\n",
        "eval_episodes = 20  # @param\n",
        "num_hiddens = [50]  # @param\n",
        "learning_rate = 0.005  # @param\n",
        "epsilon = 0.01  # @param\n",
        "seed = 1221  # @param\n",
        "\n",
        "rng = hk.PRNGSequence(jax.random.PRNGKey(seed))\n",
        "\n",
        "# Initialise the environment.\n",
        "env = Catch()\n",
        "timestep = env.reset()\n",
        "\n",
        "# Build and initialise the agent.\n",
        "agent = DQNAgent(env.action_spec(),\n",
        "                 env.observation_spec(),\n",
        "                 num_hiddens=num_hiddens,\n",
        "                 epsilon=epsilon,\n",
        "                 learning_rate=learning_rate)\n",
        "params = agent.initial_params(next(rng))\n",
        "learner_state = agent.initial_learner_state(params)\n",
        "\n",
        "# Initialise the accumulator.\n",
        "accumulator = ReplayBuffer(replay_buffer_capacity, discount_factor)\n",
        "\n",
        "# Run loop\n",
        "avg_returns = []\n",
        "losses = []\n",
        "\n",
        "for episode in range(train_episodes):\n",
        "  # Prepare agent, environment and accumulator for a new episode.\n",
        "  timestep = env.reset()\n",
        "  accumulator.push(timestep, None)\n",
        "  timesteps = []\n",
        "\n",
        "  while not timestep.last():\n",
        "    timesteps.append(timestep)\n",
        "    # Acting.\n",
        "    action = agent.actor_step(params, timestep, next(rng), False)\n",
        "    # Agent-environment interaction.\n",
        "    timestep = env.step(action)\n",
        "    # Accumulate experience.\n",
        "    accumulator.push(timestep, action)\n",
        "    # Learning.\n",
        "    if accumulator.is_ready(batch_size):\n",
        "      params, learner_state, loss = agent.learner_step(\n",
        "          params, accumulator.sample(batch_size), learner_state, next(rng))\n",
        "      losses.append(np.asarray(loss))\n",
        "\n",
        "  # Evaluation.\n",
        "  if not episode % evaluate_every:\n",
        "    returns = []\n",
        "    for _ in range(eval_episodes):\n",
        "      timestep = env.reset()\n",
        "      timesteps = [timestep]\n",
        "      while not timestep.last():\n",
        "        action = agent.actor_step(params, timestep, next(rng), True)\n",
        "        timestep = env.step(action)\n",
        "        timesteps.append(timestep)\n",
        "      returns.append(np.sum([item.reward for item in timesteps[1:]]))\n",
        "\n",
        "    avg_returns.append(np.mean(returns))\n",
        "    print(f\"Episode {episode:4d}: Average returns: {avg_returns[-1]:.2f}.\")\n"
      ],
      "metadata": {
        "id": "UB-iAWwwUGCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the last epsiode\n",
        "animate([item.observation for item in timesteps])"
      ],
      "metadata": {
        "id": "VFGkuRsxdzCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ⭐ Exercise\n",
        "\n",
        "Plot the loss and average return again. How does it look now? How fast is the learning? Experiemnt with:\n",
        "- batch_size\n",
        "- size of the network: num_hidden\n",
        "- number of episodes: num_train\n",
        "- size of the replay buffer: replay_buffer_capacity\n",
        "- size of the learning step: learning_rate\n",
        "- amount of exploration: epsilon\n",
        "- any other hyperparameters\n",
        "\n",
        "What effect do they have on learning? Why? Does is learn faster, slower, more or less stable?\n",
        "\n",
        "Next, experiment with the size of the board in the game."
      ],
      "metadata": {
        "id": "tdkQE2SyT05S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------#\n",
        "\n",
        "# -----------------------------------#"
      ],
      "metadata": {
        "id": "_W6gKq2IT0gX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ⭐ Exercise\n",
        "\n",
        "Now, make DQN agent work with Cartpole environment. Experiment with the paramters again."
      ],
      "metadata": {
        "id": "1fnRHWWlNMrj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ⭐ Bonus exercise\n",
        "\n",
        "DQN method on Atari works on the observations which are represented as images. You can try this by either using the image representation from Catch, or by using one of the Atari environment for training (Pong is the simplest, but you can also try Breakout). Note tha training on Atari games would be significantly longer. Good luck!"
      ],
      "metadata": {
        "id": "YP5ac_i1WMDW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusions\n",
        "\n",
        "🥳 Congratulations on completing the Reinforcement Learning tutorial!\n",
        "\n",
        "If you would like to learn more about Reinforcement Learning, consider checking:\n",
        "\n",
        "- [DeepMind x UCL RL Lecture Series](https://www.youtube.com/watch?v=TCCjZe0y4Qc)\n",
        "- [Introduction to Reinforcement Learning with David Silver](https://www.deepmind.com/learning-resources/introduction-to-reinforcement-learning-with-david-silver)\n",
        "- [Spinning Up in Deep RL](https://spinningup.openai.com/en/latest/)"
      ],
      "metadata": {
        "id": "jO-6gWVzgiwf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ek_hwUBcG_yT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}