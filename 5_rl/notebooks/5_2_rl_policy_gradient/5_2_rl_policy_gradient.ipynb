{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Part II ü¶Ä Intermediate: Deep RL with Policy Gradient methods.\n",
        "---"
      ],
      "metadata": {
        "id": "7xARRI95HDa9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "%%capture\n",
        "\n",
        "!pip install dm-haiku\n",
        "!pip install optax\n",
        "!pip install dm_env\n",
        "!pip install gym[accept-rom-license]\n",
        "!pip install dm-acme[envs]\n",
        "!pip install autorom[accept-rom-license]"
      ],
      "metadata": {
        "id": "7K96ZoNDCDyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "\n",
        "import collections\n",
        "import random\n",
        "from typing import Sequence\n",
        "\n",
        "import chex\n",
        "import dm_env\n",
        "from dm_env import specs\n",
        "import haiku as hk\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from matplotlib import animation\n",
        "from matplotlib import rc\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import optax\n",
        "import gym\n",
        "from acme.wrappers import atari_wrapper, gym_wrapper\n",
        "from acme import wrappers\n",
        "from matplotlib.patches import namedtuple\n",
        "\n",
        "rc('animation', html='jshtml')\n",
        "import warnings\n",
        "# warnings.filterwarnings(action='once')\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "gWisHY8ECIjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Some helper functions copied from 4.1\n",
        "\n",
        "# Function to animate the observations\n",
        "def animate(data, interval=200):\n",
        "  fig = plt.figure(1)\n",
        "  img = plt.imshow(data[0])\n",
        "  plt.axis('off')\n",
        "\n",
        "  def animate(i):\n",
        "    img.set_data(data[i])\n",
        "\n",
        "  anim = animation.FuncAnimation(fig, animate, frames=len(data), interval=interval)\n",
        "  plt.close(1)\n",
        "  return anim\n",
        "\n",
        "_ACTIONS = (-1, 0, 1)  # Move paddle left, no-op, move paddle right.\n",
        "\n",
        "class Catch(dm_env.Environment):\n",
        "  \"\"\"A Catch environment built on the dm_env.Environment class.\n",
        "\n",
        "  The agent must move a paddle to intercept falling balls. Falling balls only\n",
        "  move downwards on the column they are in.\n",
        "\n",
        "  The observation is an array with shape (rows, columns) containing binary\n",
        "  values: 0 if a space is empty; 1 if it contains the paddle and 2 for a ball.\n",
        "\n",
        "  The actions are discrete, and by default there are three available actions:\n",
        "  move left, stay, and move right.\n",
        "\n",
        "  The episode terminates when the ball reaches the bottom of the screen.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               rows: int = 10,\n",
        "               columns: int = 5,\n",
        "               discount: float = 1.0):\n",
        "    \"\"\"Initializes a new Catch environment.\n",
        "\n",
        "    Args:\n",
        "      rows: number of rows.\n",
        "      columns: number of columns.\n",
        "      discount: discount factor for calculating reward.\n",
        "    \"\"\"\n",
        "    self._rows = rows\n",
        "    self._columns = columns\n",
        "    self._discount = discount\n",
        "    self._board = np.zeros((rows, columns), dtype=np.float32)\n",
        "    self._ball_x = None\n",
        "    self._ball_y = None\n",
        "    self._paddle_x = None\n",
        "    self._reset_next_step = True\n",
        "\n",
        "  def reset(self) -> dm_env.TimeStep:\n",
        "    \"\"\"Returns the first `TimeStep` of a new episode.\"\"\"\n",
        "    self._reset_next_step = False\n",
        "    # Ball can drop from any column.\n",
        "    self._ball_x = np.random.randint(self._columns)\n",
        "    self._ball_y = 0  # Top of matrix.\n",
        "    self._paddle_x = self._columns // 2  # Centre.\n",
        "\n",
        "    return dm_env.restart(self._observation())\n",
        "\n",
        "  def step(self, action: int) -> dm_env.TimeStep:\n",
        "    \"\"\"Updates the environment according to the action.\"\"\"\n",
        "    if self._reset_next_step:\n",
        "      return self.reset()\n",
        "\n",
        "    # Move the paddle.\n",
        "    dx = _ACTIONS[action]  # Get action. dx = change in x position.\n",
        "    # Clip to keep paddle in bounds of the environment matrix.\n",
        "    self._paddle_x = np.clip(self._paddle_x + dx, 0, self._columns - 1)\n",
        "\n",
        "    # -----------------------------------#\n",
        "    # Drop the ball down one row: increase y coordinate of the ball by 1.\n",
        "    self._ball_y += 1\n",
        "    # -----------------------------------#\n",
        "\n",
        "    # Check for termination.\n",
        "    if self._ball_y == self._rows - 1:  # Ball has fallen below the rows.\n",
        "      # Reward depends on whether the paddle is on the ball (positions match).\n",
        "      reward = 1. if self._paddle_x == self._ball_x else -1.\n",
        "      self._reset_next_step = True\n",
        "      return dm_env.termination(reward=reward, observation=self._observation())\n",
        "\n",
        "    return dm_env.transition(reward=0., observation=self._observation(),\n",
        "                             discount=self._discount)\n",
        "\n",
        "  def observation_spec(self) -> specs.BoundedArray:\n",
        "    \"\"\"Returns the observation spec.\"\"\"\n",
        "    return specs.BoundedArray(\n",
        "        shape=self._board.shape,\n",
        "        dtype=self._board.dtype,\n",
        "        name='board',\n",
        "        minimum=0,\n",
        "        maximum=2)\n",
        "\n",
        "  def action_spec(self) -> specs.DiscreteArray:\n",
        "    \"\"\"Returns the action spec.\"\"\"\n",
        "    return specs.DiscreteArray(\n",
        "        dtype=int, num_values=len(_ACTIONS), name='action')\n",
        "\n",
        "  def _observation(self) -> np.ndarray:\n",
        "    self._board.fill(0.)\n",
        "    self._board[self._ball_y, self._ball_x] = 2.\n",
        "    self._board[self._rows - 1, self._paddle_x] = 1.\n",
        "\n",
        "    return self._board.copy()\n",
        "\n",
        "# Function to animate the observations\n",
        "def animate(data, interval=200):\n",
        "  fig = plt.figure(1)\n",
        "  img = plt.imshow(data[0])\n",
        "  plt.axis('off')\n",
        "\n",
        "  def animate(i):\n",
        "    img.set_data(data[i])\n",
        "\n",
        "  anim = animation.FuncAnimation(fig, animate, frames=len(data), interval=interval)\n",
        "  plt.close(1)\n",
        "  return anim"
      ],
      "metadata": {
        "cellView": "form",
        "id": "_ApIzbyx0gUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to policy gradient RL\n",
        "In this section, we are going to look at alternative to **value-based methods** approach: **policy gradient methods**.\n",
        "The name \"policy gradient\" comes from the fact that we are estimating the gradient of the policy (rather than Q-function). **Policy-based methods** use iterative update rules to calculate the expected return associated with a state and action.\n",
        "\n",
        "In order to learn, we need a loss function or *objective*. In RL, the general objective is to maximise the expected episode return by taking actions in the environment. Suppose that the agent's policy is parametrised by a function with paramethers $\\theta$: then the actions are determined by $\\pi_\\theta(a|s)$. A very general way to represent a policy is with a neural network with parameters $\\theta$. So, the task of RL is to find the neural network parameters $\\theta$ that maximise\n",
        "\n",
        "$$J(\\pi_\\theta)=\\mathrm{E}_{\\tau\\sim\\pi_\\theta}\\ [R(\\tau)],$$\n",
        "\n",
        "where $\\mathrm{E}$ means *expectation*, $\\tau$ is again shorthand for episode, and $R(\\tau)$ denotes the return of episode $\\tau$.\n",
        "\n",
        "Then, the goal in RL is to find the parameters $\\theta$ that maximise the function $J(\\pi_\\theta)$. One way to find the parameters $\\theta$ that maximise $J(\\pi_\\theta)$ is to perform gradient ascent on $J(\\pi_\\theta)$ with respect to the parameters $\\theta$.\n",
        "\n",
        "$$\\theta_{k+1}=\\theta_k + \\alpha \\nabla J(\\pi_\\theta)|_{\\theta_{k}},$$\n",
        "\n",
        "where $\\nabla J(\\pi_\\theta)|_{\\theta_{k}}$ is the gradient of the expected return with respect to the policy parameters $\\theta_k$ and $\\alpha$ is the step size. This quantity, $\\nabla J(\\pi_\\theta)$, is also called the **policy gradient**. If we can compute the policy gradient, then we will have a means by which to directly optimise our policy.\n",
        "\n",
        "As it turns out, there is a [way](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html) for us to compute the policy gradient:\n",
        "\n",
        "\n",
        "$$\\nabla_{\\theta} J(\\pi_{\\theta})=\\underset{\\tau \\sim \\pi_{\\theta}}{\\mathrm{E}}[\\sum_{t=0}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t} \\mid s_{t}) R(\\tau)].$$\n",
        "\n",
        "Informaly, the policy gradient is equal to the gradient of the log of the probability of the action chosen, multiplied by the return of the episode in which the action was taken.\n",
        "**REINFORCE** is a simple RL algorithm that uses the policy gradient to find the optimal policy by increasing the probability of choosing actions (*reinforcing* actions) that tend to lead to high return episodes.\n"
      ],
      "metadata": {
        "id": "LL7LBchRDi07"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this part, we will start by experimenting with the CartPole environment. As a reminder, this is how we load it:"
      ],
      "metadata": {
        "id": "-QrKDCjJ_gOK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym_wrapper.GymWrapper(gym.make('CartPole-v1'))\n",
        "env = wrappers.SinglePrecisionWrapper(env)"
      ],
      "metadata": {
        "id": "zpr0s-ULpEE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rewards-to-go\n",
        "Performing gradient ascent on the gradient of the log of the action probability, weighted by the return of the episode will tend to push up the probability of actions that were in episodes with high return, regardless of *where* in the episode the action was taken. This does not really make much sense because an action near the end of an episode may be reinforced because lots of reward was collected earlier on in the episode, *before* the action was taken. RL agents should really only reinforce actions on the basis of their *consequences*. Rewards obtained before taking an action have no bearing on how good that action was: only rewards that come after. The cummulative rewards received after an action was taken is called the **rewards-to-go** and can be computed as:\n",
        "\n",
        "$$\\hat{R}_i=\\sum_{t=i}^Tr_t.$$\n",
        "\n",
        "Compare this to the episode return:\n",
        "\n",
        "$$R(\\tau)=\\sum_{t=0}^Tr_t.$$\n",
        "\n",
        "We can improve the reliability of the policy gradient by substituting the episode return with the rewards-to-go. The policy gradient with rewards-to-go is given by:\n",
        "\n",
        "$$\\nabla_{\\theta} J(\\pi_{\\theta})=\\underset{\\tau \\sim \\pi_{\\theta}}{\\mathrm{E}}[\\sum_{t=0}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t} \\mid s_{t}) \\hat{R}_t].$$"
      ],
      "metadata": {
        "id": "GFbUkYBA4quP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚≠ê Exercise\n",
        "\n",
        "Implement a function that takes a list of all the rewards obtained in an episode and computes the rewards-to-go."
      ],
      "metadata": {
        "id": "pn0sfRQoCfk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_rewards_to_go(rewards):\n",
        "  rewards_to_go = []\n",
        "  # -----------------------------------#\n",
        "  # Given a list of rewards, return a list of rewards to go\n",
        "\n",
        "  # -----------------------------------#\n",
        "  return rewards_to_go"
      ],
      "metadata": {
        "id": "SZ5JFgfh4i8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example\n",
        "rewards_to_go = compute_rewards_to_go([1, 1, 1, 1, 0])\n",
        "print('rewards_to_go:', rewards_to_go)\n",
        "print('rewards_to_go should be [4, 3, 2, 1, 0]')"
      ],
      "metadata": {
        "id": "Tccp_YaZvVvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handling the data\n",
        "We will need to make a new agent memory to store the rewards-to-go $\\hat{R}_t$ along with the observation $o_t$ and action $a_t$ at every timestep. As before, the memory class implements the basic functions `__init__, push, sample, is_ready`. The memory now has a maximum size and the sampling function return a batch of samples."
      ],
      "metadata": {
        "id": "0cMm9VBX4zU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For the purpose of this section, we redefine transition to include if the observation is final\n",
        "Transition = collections.namedtuple(\n",
        "    'Transition', 'obs_tm1 a_tm1 r_t obs_t done')\n",
        "EpisodeRewardsToGoMemory = collections.namedtuple(\n",
        "    'EpisodeRewardsToGoMemory', 'obs, action, reward_to_go')\n",
        "\n",
        "\n",
        "class EpisodeRewardsToGoBuffer:\n",
        "  def __init__(self, num_transitions_to_store=512, batch_size=256):\n",
        "    self.batch_size = batch_size\n",
        "    self.memory_buffer = collections.deque(maxlen=num_transitions_to_store)\n",
        "    self.current_episode_transition_buffer = []\n",
        "\n",
        "  def push(self, transition):\n",
        "    self.current_episode_transition_buffer.append(transition)\n",
        "    if transition.done:\n",
        "      episode_rewards = []\n",
        "      for t in self.current_episode_transition_buffer:\n",
        "        episode_rewards.append(t.r_t)\n",
        "      r2g = compute_rewards_to_go(episode_rewards)\n",
        "      for i, t in enumerate(self.current_episode_transition_buffer):\n",
        "        memory = EpisodeRewardsToGoMemory(t.obs_tm1, t.a_tm1, r2g[i])\n",
        "        self.memory_buffer.append(memory)\n",
        "      # Reset episode buffer\n",
        "      self.current_episode_transition_buffer = []\n",
        "\n",
        "  def sample(self):\n",
        "    random_memory_sample = random.sample(self.memory_buffer, self.batch_size)\n",
        "    obs_batch, action_batch, reward_to_go_batch = zip(*random_memory_sample)\n",
        "    return EpisodeRewardsToGoMemory(\n",
        "        np.stack(obs_batch).astype(\"float32\"),\n",
        "        np.asarray(action_batch).astype(\"int32\"),\n",
        "        np.asarray(reward_to_go_batch).astype(\"int32\")\n",
        "    )\n",
        "\n",
        "  def is_ready(self):\n",
        "    return len(self.memory_buffer) >= self.batch_size\n",
        "\n",
        "# Instantiate Memory\n",
        "REINFORCE_memory = EpisodeRewardsToGoBuffer(num_transitions_to_store=512, batch_size=256)"
      ],
      "metadata": {
        "id": "xq-oybSq4v2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# REINFORCE Agent\n",
        "\n",
        "We will use the same structure for the agent class as before, it will include `__init__`, `actor_step` and `learner_step` functions. In addition, as the policy is now represented by a neural network, we will use helper functions `initial_params` and `initial_learner_state`."
      ],
      "metadata": {
        "id": "ItqHKv1w0zaO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚≠ê Exercise\n",
        "\n",
        "1) **Policy neural network.**\n",
        "Our policy neural network policy will take the observation as input and passes it through an MLP with `len(num_hiddens)` hidden layers and then outputs one scalar value for each of the possible actions (`2` in CartPole). The outputs of our policy network are [logits](https://qr.ae/pv4YTe). We will use Jax and Haiku for the neural network training. Please have a look [here](https://dm-haiku.readthedocs.io/en/latest/api.html) to understand Haiku transformations. Use [Haiku](https://github.com/deepmind/dm-haiku) (a library for implementing neural networks is JAX) to fill in the gaps in the policy `network` function. Hint: use functions `hk.Flatten` and `hk.nets.MLP`. Having done this, you can run the next cell \"Network test\" to check your implementation. You may need to comment out incomplete functions when running the tests.\n",
        "\n",
        "2) **Actor function.**\n",
        "Next we implement `actor_step` function, which takes network parameters, timestep and random key and returns an action of the policy. Fill in the gaps in the `actor_step` function. Use [`jax.random.categorical`](https://jax.readthedocs.io/en/latest/_autosummary/jax.random.categorical.html) function to sample an action given the logits. Having done this, you can run the cell after \"Actor test\" to check your implementation.\n",
        "\n",
        "3) **Learner function.**\n",
        "Finally, we implement the learner function that computes the gradient of the loss and makes a step. Remember, the loss function is\n",
        "\n",
        "$$- \\log \\pi_{\\theta}(a_{t} \\mid s_{t}) \\hat{R}_t.$$ Fill in the gaps in the `_loss` function. First, convert `logits` into probabilities (use [softmax](https://en.wikipedia.org/wiki/Softmax_function) function), and then, weight them by the `rewards_to_go`. Having done this, you can run the cell after \"Loss test\" to check your implementation."
      ],
      "metadata": {
        "id": "0JahF7esgbwM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class REINFORCEAgent(object):\n",
        "  \"\"\"Q-learning agent.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               action_spec: specs.DiscreteArray,\n",
        "               observation_spec: specs.DiscreteArray,\n",
        "               num_hiddens: Sequence[int] = [50],\n",
        "               learning_rate: float = 0.005):\n",
        "    self._observation_spec = observation_spec\n",
        "    self._num_actions = action_spec.num_values\n",
        "    self._optimizer = optax.adam(learning_rate)\n",
        "\n",
        "    def network(obs):\n",
        "      \"\"\"Q network of the agent.\"\"\"\n",
        "      mlp = hk.Sequential(\n",
        "          # -----------------------------------#\n",
        "          [\n",
        "          ]\n",
        "          )\n",
        "          # -----------------------------------#\n",
        "      return mlp(obs)\n",
        "\n",
        "    self._network = hk.without_apply_rng(hk.transform(network)) #, apply_rng=True))\n",
        "    # Jitting for speed. Can comment out for debugging\n",
        "    self.actor_step = jax.jit(self.actor_step)\n",
        "    self.learner_step = jax.jit(self.learner_step)\n",
        "\n",
        "  def initial_params(self, rng_key):\n",
        "    \"\"\"Initialises the agent params given the RNG key.\"\"\"\n",
        "    sample_input = self._observation_spec.generate_value()\n",
        "    sample_input = jnp.expand_dims(sample_input, 0)\n",
        "    # Haiku networks have network.init(<random_key>, <input>) function\n",
        "    # which returns a set of random initial parameters.\n",
        "    return self._network.init(rng_key, sample_input)\n",
        "\n",
        "  def initial_learner_state(self, params):\n",
        "    \"\"\"Initialises the state of the optimizer.\"\"\"\n",
        "    return self._optimizer.init(params)\n",
        "\n",
        "  def actor_step(self, params, timestep, rng_key):\n",
        "    \"\"\".\"\"\"\n",
        "    obs = jnp.expand_dims(timestep.observation, 0)  # Add dummy batch.\n",
        "    # Pass obs through policy network to compute logits\n",
        "    logits = self._network.apply(params, obs)\n",
        "    # Remove batch dim\n",
        "    logits = logits[0]\n",
        "    # -----------------------------------#\n",
        "    # Randomly sample action\n",
        "    action =\n",
        "    # -----------------------------------#\n",
        "    return action\n",
        "\n",
        "  def _loss(self, params, obs, action, reward_to_go):\n",
        "\n",
        "    def policy_gradient_loss(action, logits, reward_to_go):\n",
        "      # -----------------------------------#\n",
        "      # Convert logits into probs\n",
        "\n",
        "      # Get the probability of action corresponding to the input _action_\n",
        "\n",
        "      # Compute_weighted_log_prob: log of the prob, multiplied by _reward_to_go_\n",
        "\n",
        "      weighted_log_prob =\n",
        "      # -----------------------------------#\n",
        "      return -weighted_log_prob\n",
        "\n",
        "    logits_batch = self._network.apply(params, obs)\n",
        "    batched_loss = jax.vmap(policy_gradient_loss)\n",
        "    loss = jnp.mean(batched_loss(action, logits_batch, reward_to_go))\n",
        "    return loss, loss\n",
        "\n",
        "  def learner_step(self, params: hk.Params, data, learner_state, rng_key):\n",
        "    \"\"\"Computes loss, its gradient w.r.t. params, and runs an optimisation step.\"\"\"\n",
        "\n",
        "    # Get the policy gradient by using `jax.grad()` on `batched_policy_gradient_loss`\n",
        "    grad_loss, loss = jax.grad(self._loss, has_aux=True)(params, *data)\n",
        "    updates, learner_state = self._optimizer.update(\n",
        "        grad_loss, learner_state)\n",
        "    params = optax.apply_updates(params, updates)\n",
        "    return params, learner_state, loss"
      ],
      "metadata": {
        "id": "qw1qJQiofe_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Network test"
      ],
      "metadata": {
        "id": "0DgC9_MO7oYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of initialising the agent and making a dummy observation pass through the network\n",
        "agent = REINFORCEAgent(env.action_spec(),\n",
        "                       env.observation_spec())\n",
        "\n",
        "rng = jax.random.PRNGKey(42)\n",
        "params = agent.initial_params(rng)\n",
        "learner_state = agent.initial_learner_state(params)\n",
        "\n",
        "print('Policy network paremeter keys:', params.keys())\n",
        "\n",
        "dummy_obs = np.ones(env.observation_spec().shape, \"float32\")\n",
        "dummy_obs = jnp.expand_dims(dummy_obs, 0) # add batch obs\n",
        "\n",
        "# `network.apply(<params>, <input>)` passes an input through the network using the set of provided parameters\n",
        "logits = agent._network.apply(params, dummy_obs)\n",
        "print(\"Policy network logits:\", logits)\n",
        "print(\"The policy network logits should be [[ 0.748849   -0.04347724]]\")"
      ],
      "metadata": {
        "id": "2wutQ8FP7nca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Actor test"
      ],
      "metadata": {
        "id": "LTMadzen8NFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of using actor funtion\n",
        "agent = REINFORCEAgent(env.action_spec(),\n",
        "                       env.observation_spec())\n",
        "\n",
        "DummyTransition = collections.namedtuple(\n",
        "    'DummyTransition', 'observation')\n",
        "dummy_timestep = DummyTransition(np.ones(env.observation_spec().shape, \"float32\"))\n",
        "action = agent.actor_step(params, dummy_timestep, rng)\n",
        "print('Action:', action)\n",
        "print('Action should be 1')"
      ],
      "metadata": {
        "id": "p0jsEnGs8Pz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss test"
      ],
      "metadata": {
        "id": "aulpE_888zc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example\n",
        "agent = REINFORCEAgent(env.action_spec(),\n",
        "                       env.observation_spec())\n",
        "\n",
        "# Create a batch of dummy observations, actions and rewards_to_go\n",
        "obs_batch = np.ones((5,*env.observation_spec().shape), \"float32\")\n",
        "actions_batch = np.array([1, 0, 0, 1, 0])\n",
        "rew2go_batch = np.array([2.3, 4.3, 2.1, 10, 100])\n",
        "\n",
        "loss = agent._loss(params, obs_batch, actions_batch, rew2go_batch)\n",
        "print('Loss = ', loss[0])\n",
        "print('Loss should be 10.815681')"
      ],
      "metadata": {
        "id": "i51VXXpwhB2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning loop\n",
        "\n",
        "Finally, let's run the learning loop. As the structure of the agent is very similar to Q-learning agent, the learning loop is very similar as well. A few differences to note:\n",
        "\n",
        "- We are handling data slightly differently as we need to compute rewards to go\n",
        "- A learning step is performed after the whole epsidose is added to the `accumulator`\n",
        "- A learning step is performed on a batch of samples instead of a single datapoint."
      ],
      "metadata": {
        "id": "jqaHmVzu7Qu6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚≠ê Exercise\n",
        "\n",
        "- Choose the parameters (number of training episodes, numbers of hidden units in MLP, learning rate, batch size) so that the average return of the agent increases with training and ends up being greater than $200$.\n",
        "- Run the learning loop and visualise the epsiode returns. Look at the animation of the last episode. Optional: modify the code to keep an episode every x iterations and look at the progression of the agents.\n",
        "- Note that this time we do not have $\\epsilon$-greedy agent for the exploration. Why is it not necessary?"
      ],
      "metadata": {
        "id": "_kxlFSQxioHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_episodes = None  # @param\n",
        "evaluate_every = 10  # @param\n",
        "eval_episodes = 10  # @param\n",
        "num_hiddens = None  # @param\n",
        "learning_rate = None  # @param\n",
        "batch_size = None  # @param\n",
        "num_transitions_to_store = 512  # @param\n",
        "seed = 1221\n",
        "\n",
        "rng = hk.PRNGSequence(jax.random.PRNGKey(seed))\n",
        "\n",
        "# Initialise the environment.\n",
        "env = gym_wrapper.GymWrapper(gym.make('CartPole-v1'))\n",
        "env = wrappers.SinglePrecisionWrapper(env)\n",
        "\n",
        "timestep = env.reset()\n",
        "\n",
        "# Build and initialise the agent.\n",
        "agent = REINFORCEAgent(env.action_spec(),\n",
        "                       env.observation_spec(),\n",
        "                       num_hiddens=num_hiddens,\n",
        "                       learning_rate=learning_rate)\n",
        "\n",
        "params = agent.initial_params(next(rng))\n",
        "learner_state = agent.initial_learner_state(params)\n",
        "\n",
        "# Initialise the accumulator.\n",
        "accumulator = EpisodeRewardsToGoBuffer(num_transitions_to_store=num_transitions_to_store, batch_size=batch_size)\n",
        "\n",
        "# Run loop.\n",
        "avg_returns = []\n",
        "losses = []\n",
        "\n",
        "for episode in range(train_episodes):\n",
        "\n",
        "  # Prepare agent, environment and accumulator for a new episode.\n",
        "  timestep = env.reset()\n",
        "  obs = timestep.observation\n",
        "\n",
        "  frames = []\n",
        "  while not timestep.last():\n",
        "    frames.append(env.render(mode='rgb_array'))\n",
        "    # Acting.\n",
        "    action = agent.actor_step(params, timestep, next(rng))\n",
        "    # Agent-environment interaction.\n",
        "    timestep = env.step(int(action))\n",
        "    # Accumulate experience.\n",
        "    reward = timestep.reward\n",
        "    next_obs = timestep.observation\n",
        "    done = timestep.last()\n",
        "    # Pack into transition.\n",
        "    transition = Transition(obs, action, reward, next_obs, done)\n",
        "    accumulator.push(transition)\n",
        "    obs = next_obs\n",
        "\n",
        "  # Note that here we are training only at the end of the episode\n",
        "  # Learning.\n",
        "  if accumulator.is_ready():\n",
        "    batch = accumulator.sample()\n",
        "    params, learner_state, loss = agent.learner_step(\n",
        "        params, accumulator.sample(), learner_state, next(rng))\n",
        "    losses.append(np.asarray(loss))\n",
        "  # Evaluation.\n",
        "  if not episode % evaluate_every:\n",
        "    returns = []\n",
        "    for _ in range(eval_episodes):\n",
        "      timestep = env.reset()\n",
        "      timesteps = [timestep]\n",
        "      while not timestep.last():\n",
        "        action = agent.actor_step(params, timestep, next(rng))\n",
        "        timestep = env.step(int(action))\n",
        "        timesteps.append(timestep)\n",
        "      returns.append(np.sum([item.reward for item in timesteps[1:]]))\n",
        "\n",
        "    avg_returns.append(np.mean(returns))\n",
        "    print(f\"Episode {episode:4d}: Average returns: {avg_returns[-1]:.2f}\")"
      ],
      "metadata": {
        "id": "mc6haXIS2hbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Animate the last epsiode\n",
        "animate(frames, interval=50)"
      ],
      "metadata": {
        "id": "co4h-1VeaUyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------#\n",
        "# Plot the return\n",
        "\n",
        "# -----------------------------------#"
      ],
      "metadata": {
        "id": "mFcqLZM2_jPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚≠ê Bonus exercise\n",
        "Next, try to use Catch environment as a training environment. You can try computing the observation representation as we did in Part I of this tutorial, or you can try learning directly from the state (\"from pixels\"), which would similar to Atari and would require more training episodes."
      ],
      "metadata": {
        "id": "_ptvhzoh3U7o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------#\n",
        "\n",
        "# -----------------------------------#"
      ],
      "metadata": {
        "id": "dnFc9huWjMmy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}