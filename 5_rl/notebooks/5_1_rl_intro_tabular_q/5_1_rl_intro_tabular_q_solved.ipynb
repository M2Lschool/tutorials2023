{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yy9UF1Vhg8rv"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "# Part I ‚õ≥  Basics: environment, agent and tabular value-based RL\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75ij5E4VHHE8"
   },
   "source": [
    "This tutorial is based on\n",
    "- [Deep Learning Indaba](https://github.com/deep-learning-indaba/indaba-pracs-2022)\n",
    "- [DeepMind Educational Resources](https://github.com/deepmind/educational)\n",
    "\n",
    "Adapted by Ksenia Konyushkova with feedback from Lucas Maystre, Andrey Konyushkov, Nemanja Rakiƒáeviƒá, and Pavlos Tosidis.\n",
    "\n",
    "Note: in this tutorial we don't need to run the code on any accelerator, so you can select CPU kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SF6XjKzGKcMQ"
   },
   "source": [
    "##üéâüéâüéâ Welcome to the Reinforcement Learning (RL) tutorial! üéâüéâüéâ\n",
    "\n",
    "In this tutorial, we will be learning about Reinforcement Learning, a type of Machine Learning where an **agent** learns to choose **actions** in an **environment** that lead to maximal **reward** in the long run. RL has seen tremendous success on a wide range of challenging problems such as learning to play complex games like [Atari](https://www.deepmind.com/blog/agent57-outperforming-the-human-atari-benchmark), [StarCraft II](https://www.deepmind.com/blog/alphastar-mastering-the-real-time-strategy-game-starcraft-ii) and [Dota II](https://openai.com/five/) and [Go](https://www.nature.com/articles/nature16961).\n",
    "\n",
    "Unlike fields like supervised learning, where we give examples of expected behaviour to our models, RL focuses on *goal-orientated* learning from interactions, through trial-and-error. RL algorithms learn what to do (i.e., which actions to take) in an environment to maximise some reward signal. In settings like a video game, the reward signal could be the score of the game, i.e., RL algorithms will try to maximise the score in the game by choosing the best actions.  \n",
    "\n",
    "<center>\n",
    "<img src=\"https://miro.medium.com/max/1400/1*Ews7HaMiSn2l8r70eeIszQ.png\" width=\"40%\" />\n",
    "</center>\n",
    "\n",
    "[*Image Source*](https://towardsdatascience.com/multi-agent-deep-reinforcement-learning-in-15-lines-of-code-using-pettingzoo-e0b963c0820b)\n",
    "\n",
    "More precisely, in RL we have an **agent** which perceives an **observation** $o_t$ of the current state $s_t$ of the **environment** and must choose an **action** $a_t$ to take. The environment then transitions to a new state $s_{t+1}$ in response to the agent's action and also gives the agent a scalar reward $r_t$ to indicate how good or bad the chosen action was given the environment's state. The goal in RL is for the agent to maximise the amount of reward it receives from the environment over time. The subscript $t$ is used to indicate the timestep number. In this tutorial we will only consider fully observable games, which means thet state of the environment and observation of the agent are the same.\n",
    "\n",
    "We will cover the following sections in this tutorial:\n",
    "\n",
    "* **Environments** where we will either implement a simple environment or use a classical gym environment.\n",
    "* **Agent-Environment loop** where we will see how agent and environment interact. We will consider the simplest possible agent for this: the agent that just takes a random action at each step.\n",
    "* **Value-based reinforcement learning** where we will implement an agent that learns from its interactions with the environment which actions to take to achieve the highest reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "833wUQyy5DsU"
   },
   "source": [
    "In this colab you will find some\n",
    "\n",
    "#### ‚≠ê Exercises\n",
    "where you need to implement missing parts in the code, or answer the quesions that test your understanding of code and algothims.\n",
    "When you need to complete some code, the section is marked as:\n",
    "\n",
    "```\n",
    "# -----------------------------------#\n",
    "# You code goes here\n",
    "# -----------------------------------#\n",
    "```\n",
    "with any comments to help you to complete the task. Sometimes it is useful to have a look at further code in the cell to understand which variables you need to assign in your implementation.\n",
    "Some exercises only require you to answer the questions or experiment with the code.\n",
    "You can skip bonus exercises if you are short on time as the rest of the content does not depend on them. You can use the Table of content on the left to vavigate the tutorial. Let's begin!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "bz9ZYSVK5D_e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%%capture` not found.\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "%%capture\n",
    "\n",
    "!pip install dm_env\n",
    "!pip install gym[accept-rom-license]\n",
    "!pip install dm-acme[envs]\n",
    "!pip install autorom[accept-rom-license]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "JAlbDPgu5I1m"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dm_env'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequence\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdm_env\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdm_env\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m specs\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m animation\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dm_env'"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "import collections\n",
    "import random\n",
    "from typing import Sequence\n",
    "\n",
    "import dm_env\n",
    "from dm_env import specs\n",
    "from matplotlib import animation\n",
    "from matplotlib import rc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gym\n",
    "from acme.wrappers import atari_wrapper, gym_wrapper\n",
    "from acme import wrappers\n",
    "from matplotlib.patches import namedtuple\n",
    "\n",
    "rc('animation', html='jshtml')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "811cGXaC-AN7"
   },
   "source": [
    "# Environments\n",
    "\n",
    "In reinforcement learning, we do not start with a dataset, but with an environment. Environments in RL represent the task or problem that we are trying to solve. There are many types of environments, such as board or computer games, simulated robotics settings, etc.\n",
    "\n",
    "In this tutorial, we will look in details at two simple environments: Catch and Cartpole."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0r3SlrpIPrRH"
   },
   "source": [
    "## Create your own environment\n",
    "\n",
    "We will use `dm_env` library to define the environment. We will rely on the example [Catch](https://github.com/deepmind/dm_env/blob/master/examples/catch.py). from the library. Catch is a simple game, which is often used as a test bed for RL algorithms.\n",
    "\n",
    "In this environment, a ball drops from the top and the agent controls the paddle at the bottom via three possible  **actions**: `left`, `stay`, and `right`. The **reward** ü•ï is given at the end of the episode, and is either $+1$ for catching the ball or $-1$ dropping the ball (and $0$ in all intermediate steps). The **episode** ends when the ball reaches the bottom of the screen. The **return** is the discounted sum of rewards in an episode:\n",
    "\n",
    "$${r}(\\tau_i) = \\sum_{t=1}^{T_i} \\gamma^t {r_{i,t}},$$\n",
    "\n",
    "where $T_i$ is the episode length.\n",
    "The discount factor allows us to increase the importance of rewards received quickly and decrease the importance of rewards that take long to receive. In this environment the discount does not have a large role to play, so we  set it to $\\gamma^t=1$ for now and for the rest of the tutorial consider return to be just the sum of rewards in an episode:\n",
    "$${r}(\\tau_i) = \\sum_{t=1}^{T_i} {r_{i,t}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I4YN8W-mRIaX"
   },
   "source": [
    "### ‚≠ê Exercise\n",
    "Read through the environment specification code. Fill in the missing code to complete the `Catch` class. In particular, you need to make sure the ball is moving down by one row at every timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d5uibC6q8kvd"
   },
   "outputs": [],
   "source": [
    "_ACTIONS = (-1, 0, 1)  # Move paddle left, no-op, move paddle right.\n",
    "\n",
    "class Catch(dm_env.Environment):\n",
    "  \"\"\"A Catch environment built on the dm_env.Environment class.\n",
    "\n",
    "  The agent must move a paddle to intercept falling balls. Falling balls only\n",
    "  move downwards on the column they are in.\n",
    "\n",
    "  The observation is an array with shape (rows, columns) containing binary\n",
    "  values: 0 if a space is empty; 1 if it contains the paddle and 2 for a ball.\n",
    "\n",
    "  The actions are discrete, and by default there are three available actions:\n",
    "  move left, stay, and move right.\n",
    "\n",
    "  The episode terminates when the ball reaches the bottom of the screen.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               rows: int = 10,\n",
    "               columns: int = 5,\n",
    "               discount: float = 1.0):\n",
    "    \"\"\"Initializes a new Catch environment.\n",
    "\n",
    "    Args:\n",
    "      rows: number of rows.\n",
    "      columns: number of columns.\n",
    "      discount: discount factor for calculating reward.\n",
    "    \"\"\"\n",
    "    self._rows = rows\n",
    "    self._columns = columns\n",
    "    self._discount = discount\n",
    "    self._board = np.zeros((rows, columns), dtype=np.float32)\n",
    "    self._ball_x = None\n",
    "    self._ball_y = None\n",
    "    self._paddle_x = None\n",
    "    self._reset_next_step = True\n",
    "\n",
    "  def reset(self) -> dm_env.TimeStep:\n",
    "    \"\"\"Returns the first `TimeStep` of a new episode.\"\"\"\n",
    "    self._reset_next_step = False\n",
    "    # Ball can drop from any column.\n",
    "    self._ball_x = np.random.randint(self._columns)\n",
    "    self._ball_y = 0  # Top of matrix.\n",
    "    self._paddle_x = self._columns // 2  # Centre.\n",
    "\n",
    "    return dm_env.restart(self._observation())\n",
    "\n",
    "  def step(self, action: int) -> dm_env.TimeStep:\n",
    "    \"\"\"Updates the environment according to the action.\"\"\"\n",
    "    if self._reset_next_step:\n",
    "      return self.reset()\n",
    "\n",
    "    # Move the paddle.\n",
    "    dx = _ACTIONS[action]  # Get action. dx = change in x position.\n",
    "    # Clip to keep paddle in bounds of the environment matrix.\n",
    "    self._paddle_x = np.clip(self._paddle_x + dx, 0, self._columns - 1)\n",
    "\n",
    "    # -----------------------------------#\n",
    "    # Drop the ball down one row: increase y coordinate of the ball by 1.\n",
    "    self._ball_y += 1\n",
    "    # -----------------------------------#\n",
    "\n",
    "    # Check for termination.\n",
    "    if self._ball_y == self._rows - 1:  # Ball has fallen below the rows.\n",
    "      # Reward depends on whether the paddle is on the ball (positions match).\n",
    "      reward = 1. if self._paddle_x == self._ball_x else -1.\n",
    "      self._reset_next_step = True\n",
    "      return dm_env.termination(reward=reward, observation=self._observation())\n",
    "\n",
    "    return dm_env.transition(reward=0., observation=self._observation(),\n",
    "                             discount=self._discount)\n",
    "\n",
    "  def observation_spec(self) -> specs.BoundedArray:\n",
    "    \"\"\"Returns the observation spec.\"\"\"\n",
    "    return specs.BoundedArray(\n",
    "        shape=self._board.shape,\n",
    "        dtype=self._board.dtype,\n",
    "        name='board',\n",
    "        minimum=0,\n",
    "        maximum=2)\n",
    "\n",
    "  def action_spec(self) -> specs.DiscreteArray:\n",
    "    \"\"\"Returns the action spec.\"\"\"\n",
    "    return specs.DiscreteArray(\n",
    "        dtype=int, num_values=len(_ACTIONS), name='action')\n",
    "\n",
    "  def _observation(self) -> np.ndarray:\n",
    "    self._board.fill(0.)\n",
    "    self._board[self._ball_y, self._ball_x] = 2.\n",
    "    self._board[self._rows - 1, self._paddle_x] = 1.\n",
    "\n",
    "    return self._board.copy()\n",
    "\n",
    "# Function to animate the observations\n",
    "def animate(data, interval=200):\n",
    "  fig = plt.figure(1)\n",
    "  img = plt.imshow(data[0])\n",
    "  plt.axis('off')\n",
    "\n",
    "  def animate(i):\n",
    "    img.set_data(data[i])\n",
    "\n",
    "  anim = animation.FuncAnimation(fig, animate, frames=len(data), interval=interval)\n",
    "  plt.close(1)\n",
    "  return anim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SB-cx8PD-Fgg"
   },
   "source": [
    "\n",
    "Let's look at the types of objects the environment returns (observations) and consumes (actions). The `environment_spec` will show you the form of the **observations**, **rewards** and discounts that the environment exposes and the form of the **actions** that can be taken:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fp4ufjyj98LK",
    "outputId": "ca26e095-89a7-4c66-9671-87ac1f6ccca8"
   },
   "outputs": [],
   "source": [
    "env = Catch(rows=10, columns=5)\n",
    "print(env.observation_spec())\n",
    "print(env.reward_spec())\n",
    "print(env.action_spec())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u6Y81Jo3jUdC"
   },
   "source": [
    "### ‚≠ê Exercise\n",
    "\n",
    "What does observation\n",
    "```\n",
    "[[0., 0., 0., 0., 0.],\n",
    " [0., 0., 0., 0., 2.],\n",
    " [0., 0., 0., 0., 0.],\n",
    " [0., 0., 0., 0., 0.],\n",
    " [0., 0., 0., 0., 0.],\n",
    " [0., 0., 0., 0., 0.],\n",
    " [0., 0., 0., 0., 0.],\n",
    " [0., 0., 0., 0., 0.],\n",
    " [0., 0., 0., 0., 0.],\n",
    " [0., 1., 0., 0., 0.]]\n",
    "```\n",
    "represent? Does is correspond to `env.observation_spec()`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IgPb8rRL-pCT"
   },
   "source": [
    "We can see that by default the **observations** consist of a matrix of shape $(10, 5)$. You can change the size of the game by setting the `rows` and `columns` parameters to different values when creating an instance of a game. The **actions** is a $1$-D integer array with possible values $[0, 1, 2]$. Finally, the **reward** is a scalar.\n",
    "\n",
    "Now we want to take an action using the `step` method to interact with the environment, which will return a `TimeStep` namedtuple with fields:\n",
    "\n",
    "```\n",
    "timestep = (step_type, reward, discount, observation)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G9K1ODXU_C8o"
   },
   "source": [
    "**step_type**: indicates whether we're at the beginning, middle, or end of the episode. For more details, look [here](https://github.com/deepmind/dm_env/blob/master/dm_env/_environment.py#L32). For example, if you want to check if you are at the end of the episode, you can check if `step_type.last()` is `True`.\n",
    "\n",
    "**reward**: is the reward returned by the environment at this step. Note that before we take an action the reward is `None`.\n",
    "\n",
    "**discount**: is the discount factor $\\gamma$ (will be set to $1$ for simplicity).\n",
    "\n",
    "**observations**: our observations are zero everywhere except where the paddle is indicated by $1$ and ball is indicated by $2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T5qOi851Rw-n"
   },
   "source": [
    "We can visualise the observations after taking some actions as images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 982
    },
    "id": "AiF4LbQ0-JYa",
    "outputId": "cdeb5da7-070d-49ec-a294-73a84c99b24f"
   },
   "outputs": [],
   "source": [
    "# Reset and initialise the environment.\n",
    "step_type, reward, discount, observation = env.reset()\n",
    "plt.imshow(observation)\n",
    "plt.show()\n",
    "print('\\nstep_type:', step_type)\n",
    "print('reward:', reward)\n",
    "print('discount:', discount)\n",
    "\n",
    "# Let's take a single action.\n",
    "step_type, reward, discount, observation = env.step(0)\n",
    "plt.imshow(observation)\n",
    "plt.show()\n",
    "\n",
    "print('\\nstep_type:', step_type)\n",
    "print('reward:', reward)\n",
    "print('discount:', discount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H8wxwt_cSTUE"
   },
   "source": [
    "### ‚≠ê Exercise\n",
    "\n",
    "Make a game board with $3$ rows and $2$ colums. Take $2$ actions to go to the right (until the game is finished). Questions:\n",
    "- what is the reward at the very first timestep?\n",
    "- compute the return as sum of rewards in the episode.\n",
    "- given that the ball's position is random at initialisation, what is the probability to catch the ball?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "DlcTt1qkTJfa",
    "outputId": "261afb1b-d6c9-49b1-9e86-e90e75859e88"
   },
   "outputs": [],
   "source": [
    "# Make an environment\n",
    "# -----------------------------------#\n",
    "# Create an environment\n",
    "env = Catch(rows=3, columns=2)\n",
    "# -----------------------------------#\n",
    "\n",
    "a_return = 0\n",
    "# -----------------------------------#\n",
    "# Reset and initialise the environment.\n",
    "# Remember that reward is None and it does not need to be used for return computation.\n",
    "step_type, reward, discount, observation = env.reset()\n",
    "# -----------------------------------#\n",
    "# show an observation\n",
    "plt.imshow(observation)\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------------#\n",
    "# Go to the right\n",
    "# Remember to add the reward to a_return\n",
    "step_type, reward, discount, observation = env.step(1)\n",
    "plt.imshow(observation)\n",
    "# -----------------------------------#\n",
    "plt.show()\n",
    "# update a_return\n",
    "a_return += reward\n",
    "\n",
    "# -----------------------------------#\n",
    "# Go to the right\n",
    "# Remember to add the reward to a_return\n",
    "step_type, reward, discount, observation = env.step(1)\n",
    "plt.imshow(observation)\n",
    "plt.show()\n",
    "# update a_return\n",
    "a_return += reward\n",
    "# -----------------------------------#\n",
    "\n",
    "print('return=', a_return)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZnIsEzdUoGq"
   },
   "source": [
    "## Use one of the gym environments\n",
    "\n",
    "We will use a famous [CartPole](https://gymnasium.farama.org/environments/classic_control/cart_pole/) or inverted pendulum environment.\n",
    "\n",
    "<center><img src=\"https://user-images.githubusercontent.com/10624937/42135683-dde5c6f0-7d13-11e8-90b1-8770df3e40cf.gif\" height=\"250\" /></center>\n",
    "\n",
    "From the documentation:\n",
    "\n",
    "```A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum is placed upright on the cart and the goal is to balance the pole by applying forces in the left and right direction on the cart. The action ... can take values $\\{0, 1\\}$ indicating the direction of the fixed force the cart is pushed with. The observation ... includes the values corresponding to the following positions and velocities: Cart Position, Cart Velocity, Pole Angle, Pole Angular Velocity. Since the goal is to keep the pole upright for as long as possible, a reward of $+1$ for every step taken, including the termination step, is allotted. At the starting state all observations are assigned a uniformly random value in $(-0.05, 0.05)$.```\n",
    "\n",
    "Please heck the [documentation](https://gymnasium.farama.org/environments/classic_control/cart_pole/) to see more details about the environment.\n",
    "\n",
    "Note that in this tutorial we will use *environement wrapper* in order to make the interface of the environment the same as `dm_env` so that we can swap them easily. This means that you can use the same functions as we used for Catch environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T0T82Q-7rs8B"
   },
   "outputs": [],
   "source": [
    "# Make an environment\n",
    "env = gym_wrapper.GymWrapper(gym.make('CartPole-v1'))\n",
    "env = wrappers.SinglePrecisionWrapper(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sisnidxvzarO"
   },
   "source": [
    "### ‚≠ê Exercise\n",
    "\n",
    "What are the observation, action and reward spec? You the same functions for checking this as we used above for Catch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QB9bejahwGTK",
    "outputId": "cc73436e-6098-4a23-8210-b6e6fbe3ac9f"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# -----------------------------------#\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# print observation spec\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(env\u001b[38;5;241m.\u001b[39mobservation_spec())\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# print action spec\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(env\u001b[38;5;241m.\u001b[39maction_spec())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "# -----------------------------------#\n",
    "# print observation spec\n",
    "print(env.observation_spec())\n",
    "# print action spec\n",
    "print(env.action_spec())\n",
    "# print reward spec\n",
    "print(env.reward_spec())\n",
    "# -----------------------------------#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DkaQ6p9ycW5h"
   },
   "source": [
    "### ‚≠ê Exercise\n",
    "\n",
    "Let's reset the environment and then apply the force in one direction until the episode is terminated. Fill in the gaps and compute the retrun. Run the code several times. Does the sum of rewards change? Why? What is the average (not discounted) return?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BE-8YiJyxpC4",
    "outputId": "250266ab-268b-4fa6-ef10-e7ba097e27e6"
   },
   "outputs": [],
   "source": [
    "# -----------------------------------#\n",
    "# Reset the environment.\n",
    "step_type, reward, discount, observation = env.reset()\n",
    "# -----------------------------------#\n",
    "print('reward = ', reward)\n",
    "\n",
    "a_return = 0\n",
    "# we can use step_type.last() to determine if an episode terminated\n",
    "while not step_type.last():\n",
    "  # -----------------------------------#\n",
    "  # Go to the right\n",
    "  step_type, reward, discount, observation = env.step(1)\n",
    "  print('reward = ', reward)\n",
    "  # update a_return\n",
    "  a_return += reward\n",
    "  # -----------------------------------#\n",
    "\n",
    "print('Return=', a_return)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YFr5A2Jtc1u-"
   },
   "source": [
    "Let's visualise the environment frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 413
    },
    "id": "RMBmvyTrDhuw",
    "outputId": "49136d4e-df81-4e31-cf51-3f09b02eb83e"
   },
   "outputs": [],
   "source": [
    "plt.imshow(env.render(mode='rgb_array'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H51yO57XEy00"
   },
   "source": [
    "### ‚≠ê Bonus exercise\n",
    "\n",
    "Load one of the other gym environments. Check what its specs are. Manually step through the environment. In atari, you can plot the observation which contains the game image.\n",
    "\n",
    "This is how you can load the Mountain Car environment:\n",
    "```\n",
    "env = gym_wrapper.GymWrapper(gym.make('MountainCar-v0'))\n",
    "env = wrappers.SinglePrecisionWrapper(env)\n",
    "```\n",
    "\n",
    "And this is how you can load some of the atari environments:\n",
    "```\n",
    "env = gym_wrapper.GymAtariAdapter(gym.make('Pong-v4'))\n",
    "env = atari_wrapper.AtariWrapper(env)\n",
    "env = wrappers.SinglePrecisionWrapper(env)\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 450
    },
    "id": "sq77d2C9FDt_",
    "outputId": "d57c52a2-ede8-4677-9e1b-70de5a4587b2"
   },
   "outputs": [],
   "source": [
    "# -----------------------------------#\n",
    "# env = gym_wrapper.GymWrapper(gym.make('MountainCar-v0'))\n",
    "# env = wrappers.SinglePrecisionWrapper(env)\n",
    "\n",
    "env = gym_wrapper.GymAtariAdapter(gym.make('Pong-v4'))\n",
    "env = atari_wrapper.AtariWrapper(env)\n",
    "env = wrappers.SinglePrecisionWrapper(env)\n",
    "\n",
    "step_type, reward0, discount, observation = env.reset()\n",
    "plt.imshow(observation[:,:,3])\n",
    "# -----------------------------------#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vRTe5Tc7_HGQ"
   },
   "source": [
    "# Agent-Environment loop\n",
    "We now turn to the agent. An agent receives the current **state** from the environment, and uses an internal **policy** to determine an **action** to take. We implement the agent as a Python [class](https://en.wikibooks.org/wiki/A_Beginner%27s_Python_Tutorial/Classes), which is just a logical wrapper of variables and methods (functions) that operate on those variables. The methods our first agent will have are the following:\n",
    "\n",
    "\n",
    "* ```__init__```:  Initialises the agent the first time it's created.\n",
    "* `actor_step`: Receives the timestep information from the environment and returns an action.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "634NRlgl_JkG"
   },
   "source": [
    "## Random Agent\n",
    "\n",
    "To get a feel for an agent and the methods it has, let's first implement an agent that ignores the observations and just takes a *random* action at every step: for example, for Catch it decides to go righ, left or stay in place with equal probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dvOZbBV0_M6k"
   },
   "source": [
    "### ‚≠ê Exercise\n",
    "\n",
    "Fill in the gap in the random agent to return a random action. The main piece of information we need in order to implement this agent is the number of available actions in this environment (e.g., $3$ in Catch for going right, left or staying in place). We can get this information from the `num_values` attribute of an action spec `env.action_spec()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lc_pNAB1_L4H"
   },
   "outputs": [],
   "source": [
    "class RandomAgent(object):\n",
    "  \"\"\"An agent which simply takes random actions ignoring observations.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               action_spec: specs.DiscreteArray):\n",
    "    # -----------------------------------#\n",
    "    # _num_actions is the number of possible actions in the environment\n",
    "    self._num_actions = action_spec.num_values\n",
    "    # -----------------------------------#\n",
    "\n",
    "  def actor_step(self, timestep: dm_env.TimeStep):\n",
    "    # This agent is ignoring the observations, so we delete timestep\n",
    "    del timestep\n",
    "    # -----------------------------------#\n",
    "    # Return a random integer between 0 and (self._num_actions - 1)\n",
    "    # You can use np.random module here\n",
    "    return np.random.randint(self._num_actions)\n",
    "    # -----------------------------------#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ssUdHg7q_ZHl"
   },
   "source": [
    "## Run Loop\n",
    "\n",
    "Now we can loop through the environment using our random agent until the environment is terminated. We call each sequence of interactions with the environment until the termination of the episode. From now on we will refer to the NamedTuple `timestep = (step_type, reward, discount, observation)`. Note how we use `step_type` to determin the end of the episode. Here we repeat the loop $10$ times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2FOgLAtb8V3"
   },
   "source": [
    "### ‚≠ê Exercise\n",
    "\n",
    "Fill in the gaps in the agent - environment loop with Catch environment. Compute the agent return at the end of each epsiode. What is the average return of the random agent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-wS5Tmjf-s3t",
    "outputId": "fc1b2ce7-b1a4-4d54-b0cd-c96053fac87a"
   },
   "outputs": [],
   "source": [
    "num_episodes = 10  # @param\n",
    "\n",
    "# Initialise Catch environment.\n",
    "# -----------------------------------#\n",
    "env = Catch()\n",
    "# -----------------------------------#\n",
    "# Initialise the agent.\n",
    "agent = RandomAgent(env.action_spec())\n",
    "\n",
    "all_returns = []\n",
    "# Run loop.\n",
    "for episode in range(num_episodes):\n",
    "  timesteps = []  # Accumulate data for the episode.\n",
    "\n",
    "  # Prepare agent, environment and accumulator for a new episode.\n",
    "  # -----------------------------------#\n",
    "  # reset the environment\n",
    "  timestep = env.reset()\n",
    "  # -----------------------------------#\n",
    "\n",
    "  while not timestep.last():\n",
    "    # Save the timestep\n",
    "    timesteps.append(timestep)\n",
    "    # -----------------------------------#\n",
    "    # use agents actor_step method to select the action\n",
    "    action = agent.actor_step(timestep)\n",
    "    # -----------------------------------#\n",
    "    # make a step in the environment and get the next timestep\n",
    "    timestep = env.step(action)\n",
    "\n",
    "  # Save the last timestep too.\n",
    "  timesteps.append(timestep)\n",
    "\n",
    "  # -----------------------------------#\n",
    "  # Compute the return from a list of timesteps\n",
    "  # Use can use timesteps.reward to access the reward\n",
    "  # Remember: The first timestep is ignored as reward is None.\n",
    "  returns = sum([x.reward for x in timesteps[1:]])\n",
    "  # -----------------------------------#\n",
    "  all_returns.append(returns)\n",
    "  print(f'Episode {episode:}: Returns: {returns:.2f}.')\n",
    "average_return = np.mean(all_returns)\n",
    "print(f'Average return = {average_return:.2f}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AynOodKQAK-X"
   },
   "source": [
    "We can now look at the game from the last episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 579
    },
    "id": "Jiy-4Qql_BqB",
    "outputId": "7a342bfb-1087-422f-9b58-0237a507b7a9"
   },
   "outputs": [],
   "source": [
    "animate([item.observation for item in timesteps])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UevfPALZbyrt"
   },
   "source": [
    "### ‚≠ê Bonus exercise\n",
    "\n",
    "- Experiment with the size of the game field (number of rows and number of columns). How does the average return vary with it?\n",
    "- Consider, for example, the number of columns and rows between $2$ and $7$ and make a plot of the average return. To make the results less noisy, you might need to increase the number of episodes for each size.\n",
    "- Which version of the game is the easiest? Which one is the hardest? What does it depend on?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 883
    },
    "id": "YOT88s5ZYBzw",
    "outputId": "6dd4699c-2430-4099-df48-218e4d73d458"
   },
   "outputs": [],
   "source": [
    "# -----------------------------------#\n",
    "def compute_return(num_episodes, n_rows, n_columns):\n",
    "\n",
    "  # Initialise the environment.\n",
    "  env = Catch(rows=n_rows, columns=n_columns)\n",
    "  timestep = env.reset()\n",
    "\n",
    "  # Initialise the agent.\n",
    "  agent = RandomAgent(env.action_spec())\n",
    "\n",
    "  all_returns = []\n",
    "  # Run loop.\n",
    "  for episode in range(num_episodes):\n",
    "    timesteps = []  # Accumulate data for the episode.\n",
    "\n",
    "    # Prepare agent, environment and accumulator for a new episode.\n",
    "    # reset the environment\n",
    "    timestep = env.reset()\n",
    "\n",
    "    while not timestep.last():\n",
    "      timesteps.append(timestep)\n",
    "      # use agents actor_step to select the action\n",
    "      action = agent.actor_step(timestep)\n",
    "      # make a step in the environment and get the next timestep\n",
    "      timestep = env.step(action)\n",
    "\n",
    "    # Save the last timestep too.\n",
    "    timesteps.append(timestep)\n",
    "\n",
    "    # Compute the return from a list of timesteps\n",
    "    # Remember: The first timestep is ignored as reward is NaN.\n",
    "    returns = sum([x.reward for x in timesteps[1:]])\n",
    "    all_returns.append(returns)\n",
    "  average_return = np.mean(all_returns)\n",
    "  print(f'Average return = {average_return:.2f}.')\n",
    "  return average_return\n",
    "\n",
    "all_average_retuns = []\n",
    "for n_rows in range(2,7):\n",
    "  for n_columns in range(2,7):\n",
    "    average_return = compute_return(1000, n_rows, n_columns)\n",
    "    all_average_retuns.append(average_return)\n",
    "plt.imshow(np.array(all_average_retuns).reshape(5,5))\n",
    "plt.colorbar()\n",
    "plt.xlabel('n rows')\n",
    "plt.ylabel('n columns')\n",
    "plt.xticks(range(0,5), [str(x) for x in range(2,7)]);\n",
    "plt.yticks(range(0,5), [str(x) for x in range(2,7)]);\n",
    "# -----------------------------------#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s06XD55yFbPq"
   },
   "source": [
    "As the interface of CartPole environment is the same as for Catch, we can just replace the environment and run the same agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XrxbrZ0jEIJN",
    "outputId": "4dff7d7f-03f4-4309-a181-20e4cfc7ff63"
   },
   "outputs": [],
   "source": [
    "num_episodes = 10  # @param\n",
    "\n",
    "# Initialise the environment.\n",
    "env = gym_wrapper.GymWrapper(gym.make('CartPole-v1'))\n",
    "env = wrappers.SinglePrecisionWrapper(env)\n",
    "\n",
    "# Initialise the agent.\n",
    "agent = RandomAgent(env.action_spec())\n",
    "\n",
    "all_returns = []\n",
    "# Run loop.\n",
    "for episode in range(num_episodes):\n",
    "  timesteps = []  # Accumulate data for the episode.\n",
    "\n",
    "  # Prepare agent, environment and accumulator for a new episode.\n",
    "  # reset the environment\n",
    "  timestep = env.reset()\n",
    "\n",
    "  frames = []\n",
    "  while not timestep.last():\n",
    "    frames.append(env.render(mode='rgb_array'))\n",
    "    timesteps.append(timestep)\n",
    "    # use agents actor_step to select the action\n",
    "    action = agent.actor_step(timestep)\n",
    "    # make a step in the environment and get the next timestep\n",
    "    timestep = env.step(action)\n",
    "\n",
    "  # Save the last timestep too.\n",
    "  timesteps.append(timestep)\n",
    "\n",
    "  # Compute the return from a list of timesteps\n",
    "  # Remember: The first timestep is ignored as reward is NaN.\n",
    "  returns = sum([x.reward for x in timesteps[1:]])\n",
    "  all_returns.append(returns)\n",
    "  print(f'Episode {episode:}: Returns: {returns:.2f}.')\n",
    "average_return = np.mean(all_returns)\n",
    "print(f'Average return = {average_return:.2f}.')\n",
    "print('You are likely to get the average return between 15 and 30.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 579
    },
    "id": "WCwRYkXGFk1O",
    "outputId": "95b51ece-817f-4813-e294-6265e39cfb7a"
   },
   "outputs": [],
   "source": [
    "animate(frames, interval=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ksA42QGY2JaI"
   },
   "source": [
    "# Value-Based Reinforcement Learning\n",
    "\n",
    "Not surprisingly, our random agent is not really good at this game and we need to use some learning.\n",
    "\n",
    "In **value-based** reinforcement learning methods, agents maintain a **value** for all **state-action** pairs. A **value** of a **state-action** pair is telling us what reward to expect if we start at this **state**, take this action and afterwards follow a policy. Then, we use these **value** estimates to choose actions that maximise the value.\n",
    "\n",
    "\n",
    "## Q-learning\n",
    "\n",
    "One efficient algorithm for value-based learning is [Q-learning](https://en.wikipedia.org/wiki/Q-learning).\n",
    "The function that mapping state-action pairs to values for a specific policy $\\pi$ is called **Q-function**.\n",
    "Formally, **Q-function** is defined as:\n",
    "\n",
    "$$ Q^{\\pi}(s,a) = \\mathbb{E}_{\\tau \\sim P^{\\pi}} \\left[ \\sum_t \\gamma^t R_t| s_0=s,a=a_0 \\right]$$\n",
    "\n",
    "where $\\tau = \\{s_0, a_0, r_0, s_1, a_1, r_1, \\cdots \\}$. In other words, $Q^{\\pi}(s,a)$ is the expected **value** (sum of discounted rewards) of being in a given **state** $s$ and taking the **action** $a$ and then following policy ${\\pi}$ thereafter.\n",
    "\n",
    "Given a Q-function, it is easy to construct a good policy. For example, a greedy policy selects the action that maximises the Q-function estimate:\n",
    "$$\\pi_{greedy} (a|s) = \\arg\\max_a Q^{\\pi}(s,a). $$\n",
    "\n",
    "The value $V^\\pi$ of a state is the expected $Q^\\pi$ over possible actions:\n",
    "\n",
    "$$V^\\pi(s) = \\sum_{a \\in A} \\pi(a |s) Q^\\pi(s,a)$$\n",
    "\n",
    "We will reply on famous Bellman Optimality Equation to estimate the Q-function:\n",
    "\n",
    "$$ Q^\\pi(s,a) =  r(s,a) + \\gamma  \\sum_{s'\\in S} P(s' |s,a) V^\\pi(s'). $$\n",
    "\n",
    "It breaks down $Q^{\\pi}(s,a)$ into 2 parts: the immediate reward associated with being in state $s$ and taking action $a$, and the discounted sum of all future rewards. Then, to learn the Q-function we can use [temporal difference (TD) learning](https://en.wikipedia.org/wiki/Temporal_difference_learning).\n",
    "If we are given a sample of state $s$, action $a$, reward $r(s,a)$ and next state $s'$, we compute the TD-error $\\delta$ of a policy $\\pi_e$ as:\n",
    "\n",
    "$$\\delta = r(s,a) + \\gamma Q(s', \\underbrace{\\pi_e(s'}_{a'})) ‚àí Q(s, a).$$\n",
    "The first two terms is the Q-function estimate of $Q(s,a)$ through the immediate reward and Q-function of the next state, and the third term is the direct estimate of the Q-function.\n",
    "\n",
    "Then, we will update the $Q$ value estimates at each step with the following update rule:\n",
    "\n",
    "$$Q(s, a) \\gets Q(s, a) + \\alpha \\delta, $$\n",
    "\n",
    "where $\\delta$ is a TD-error and $\\alpha$ is a small learning step size will influence how quickly our $Q$ values will be updated given new observations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0vis_uAl-uJ"
   },
   "source": [
    "## Tabular Q-learning\n",
    "\n",
    "We will start with tabular Q-learning where Q-function is represented by a table with a value for every state and action. For this, we will need to enumerate all possible states and actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0qFHG3Z2Pqf"
   },
   "source": [
    "### Steps to implement Q-learning agent\n",
    "\n",
    "We will modify the Random Agent in the following way:\n",
    "\n",
    "1. **Represent Q values.** Our state space is the position of the ball and paddle in the grid so its size is $c*r*c$, where $r, c$ are the numbers of rows and columns, respectively. Our number of actions is $3$ (move left, stay, move right). So a tabular representation of $Q$ will be a matrix of size `(number of states, number of actions)=(c*r*c, 3)`.\n",
    "\n",
    "2. **Implement a policy.** We will use a greedy policy that returns the action with the highest $Q$ value.\n",
    "\n",
    "3. **Implement a learning step.** We need to add a new method to our agent class to do the learning step which updates the $Q$ values based on the data. We will call this new method  `learner_step`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IrhQ_CRF2cTm"
   },
   "source": [
    "### Handling the data\n",
    "\n",
    "To compute the loss, we need timestep information from both the current and last timesteps. Let's define a new class to handle the data. We will call this new class `TransitionAccumulator`. This class is not absolutely necessary in this simple tabular learning, but it will set the interface that we will reuse later.\n",
    "\n",
    "At each timestep, we will save the data using the `push` method and retrieve data using the `sample` method. The `sample` method returns the previous observation in addition to the data for the current timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YLX6q29y1mE3"
   },
   "outputs": [],
   "source": [
    "Transition = collections.namedtuple(\n",
    "    'Transition', 'obs_tm1 a_tm1 r_t discount_t obs_t')\n",
    "\n",
    "class TransitionAccumulator:\n",
    "  \"\"\"Simple Python accumulator for transitions.\"\"\"\n",
    "\n",
    "  def __init__(self):\n",
    "    self._prev = None\n",
    "    self._action = None\n",
    "    self._latest = None\n",
    "\n",
    "  def push(self, env_output, action):\n",
    "    self._prev = self._latest\n",
    "    self._action = action\n",
    "    self._latest = env_output\n",
    "\n",
    "  def sample(self):\n",
    "    return Transition(self._prev.observation, self._action, self._latest.reward,\n",
    "                      self._latest.discount, self._latest.observation)\n",
    "\n",
    "  def is_ready(self):\n",
    "    \"\"\"Checks if there is previous data stored.\"\"\"\n",
    "    return self._prev is not None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Der43tCVknnB"
   },
   "source": [
    "## Greedy agent\n",
    "\n",
    "Now, let's follow similar interface to the Random agent and implement Q-learning agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qcxk7URam8vY"
   },
   "source": [
    "### ‚≠ê Exercise\n",
    "\n",
    "1) We implemented the `__init__` function that initialised the Q-function with $-1$ (attribute `_q`), have a look at this function and understand it.\n",
    "\n",
    "2) We also implemented a helper `_obs_to_index` function that helps us to convert the observation represented as an image into the index of the Q-function table. Take an example of a state from before, and think about how it will be converted into an index.\n",
    "\n",
    "3) Fill in the gaps in `actor_step`. First, use the helper funtion `_obs_to_index` in order to find the index that corresponds to the observation in the current timestep. Then, make sure we select the action that maximized the Q-function value.\n",
    "\n",
    "4) Fill in the gaps in the `learner_step` function to compute the TD error as $$\\delta = r(s,a) + \\gamma Q(s', a')) ‚àí Q(s, a).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QqOKX8ow2VcP"
   },
   "outputs": [],
   "source": [
    "class QlearningAgent(object):\n",
    "  \"\"\"Q-learning agent.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               action_spec: specs.DiscreteArray,\n",
    "               observation_spec: specs.DiscreteArray,\n",
    "               step_size: float = 0.1):\n",
    "    self._num_actions = action_spec.num_values\n",
    "    self._step_size = step_size\n",
    "    r, c = observation_spec.shape\n",
    "    # variable for representing the Q-function\n",
    "    # before we know anything, assume that all actions are bad\n",
    "    self._q = -np.ones((c * r * c, self._num_actions))\n",
    "\n",
    "  def _obs_to_index(self, obs):\n",
    "    \"\"\"Convert the observation into an index for accessing q values.\"\"\"\n",
    "    # The paddle location is always at the bottom.\n",
    "    obs_shape = obs.shape\n",
    "    paddle = np.where(obs[-1, :].flatten() == 1)[0][0]\n",
    "    obs = obs.flatten().astype(int)\n",
    "    # Case where the ball and paddle overlap.\n",
    "    if obs.sum() == 1:\n",
    "      ball = (obs_shape[0] - 1) * obs_shape[1]  + paddle\n",
    "    else:\n",
    "      ball = np.where(obs == 2)[0][0]\n",
    "    return paddle * np.prod(obs_shape) + ball\n",
    "\n",
    "  def actor_step(self, timestep):\n",
    "    # -----------------------------------#\n",
    "    # Find the index corresponding to the current observation\n",
    "    observation_index = self._obs_to_index(timestep.observation)\n",
    "    # Index into the Q value matrix.\n",
    "    qvalue = self._q[observation_index]\n",
    "    # Greedy policy: select the action that corresponds to the largest Q-value.\n",
    "    selected_action = np.argmax(qvalue)\n",
    "    # -----------------------------------#\n",
    "    return selected_action\n",
    "\n",
    "  def learner_step(self, obs_tm1, a_tm1, r_t, discount_t, obs_t):\n",
    "    # obs_tm1 corresponds to s\n",
    "    # a_tm1 corresponds to a\n",
    "    # obs_t corresponds to s'\n",
    "    # Offline Q-value update.\n",
    "    # Find the indices corresponding to both observations\n",
    "    obs_t = self._obs_to_index(obs_t)\n",
    "    obs_tm1 = self._obs_to_index(obs_tm1)\n",
    "    # Greedy policy selects the action that maximises the Q-function\n",
    "    a_t = np.argmax(self._q[obs_t])\n",
    "    # -----------------------------------#\n",
    "    # Compute TD error as indicated above\n",
    "    td_error = r_t + discount_t * self._q[obs_t, a_t] - self._q[obs_tm1, a_tm1]\n",
    "    # -----------------------------------#\n",
    "    self._q[obs_tm1, a_tm1] += self._step_size * td_error\n",
    "    return td_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IryvzNGdBDMb"
   },
   "source": [
    "Let's test `actor_step`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4JXhZZ6kBPD5",
    "outputId": "e1c1057e-8df1-481c-bda1-c76d135ad0a5"
   },
   "outputs": [],
   "source": [
    "# Example of initialising the agent and making a dummy observation and selecting an action\n",
    "env = Catch()\n",
    "# Create an agent\n",
    "agent = QlearningAgent(action_spec=env.action_spec(),\n",
    "                       observation_spec=env.observation_spec())\n",
    "\n",
    "# Make a fake observation\n",
    "DummyTransition = collections.namedtuple(\n",
    "    'DummyTransition', 'observation')\n",
    "obs = np.zeros(env.observation_spec().shape, \"float32\")\n",
    "obs[0,0]=2\n",
    "obs[-1,-1]=1\n",
    "dummy_timestep = DummyTransition(obs)\n",
    "\n",
    "# Select an action\n",
    "action_to_take = agent.actor_step(dummy_timestep)\n",
    "print('Action = ', action_to_take)\n",
    "print('Action should be 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v6Gn3_1eBGru"
   },
   "source": [
    "Let's test `learner_step`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fuuss0Rl-O2n",
    "outputId": "869df090-475e-4443-f89d-d2e712b4e6db"
   },
   "outputs": [],
   "source": [
    "# Example of initialising the agent and making a dummy observation and selecting an action\n",
    "env = Catch()\n",
    "# Create an agent\n",
    "agent = QlearningAgent(action_spec=env.action_spec(),\n",
    "                       observation_spec=env.observation_spec())\n",
    "\n",
    "# Make a fake observation\n",
    "obs = np.zeros(env.observation_spec().shape, \"float32\")\n",
    "obs[0,0]=2\n",
    "obs[-1,-1]=1\n",
    "\n",
    "# Select an action\n",
    "td_error = agent.learner_step(obs, 0, 2, 0.9, obs)\n",
    "print('td-error = ', td_error)\n",
    "print('td-error should be 2.1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WW3V85E52ocu"
   },
   "source": [
    "### Run Loop\n",
    "\n",
    "Have a look again at the running loop of the random agent. The part that is missing now to make the greedy Q-learning is the `learner_step`. Note that we will need to run the environment for many episodes in order to gather sufficient data for learning. We will only evaluate our policy occasionally, every `evaluate_every` steps, to reduce the amount of logging info and computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KtpHKBt2t48n"
   },
   "source": [
    "### ‚≠ê Exercise\n",
    "\n",
    "Add the learner step to complete the training loop. For this, `sample` the datapoint from `accumulator` and run the agent's `learning_step` on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-1fFRpo52kiR",
    "outputId": "c07988ca-93d4-4d3d-fd33-93e8e24f6c40"
   },
   "outputs": [],
   "source": [
    "train_episodes = 1_000  # @param\n",
    "evaluate_every = 100  # @param\n",
    "eval_episodes = 10  # @param\n",
    "\n",
    "# Initialise the environment.\n",
    "env = Catch()\n",
    "timestep = env.reset()\n",
    "\n",
    "# Build and initialise the agent.\n",
    "agent = QlearningAgent(env.action_spec(),\n",
    "                       env.observation_spec())\n",
    "\n",
    "# Initialise the accumulator.\n",
    "accumulator = TransitionAccumulator()\n",
    "\n",
    "# Run loop\n",
    "avg_returns = []\n",
    "\n",
    "for episode in range(train_episodes):\n",
    "  # Prepare agent, environment and accumulator for a new episode.\n",
    "  timestep = env.reset()\n",
    "  accumulator.push(timestep, None)\n",
    "  while not timestep.last():\n",
    "    # Acting.\n",
    "    action = agent.actor_step(timestep)\n",
    "    # Agent-environment interaction.\n",
    "    timestep = env.step(action)\n",
    "    # Accumulate experience.\n",
    "    accumulator.push(timestep, action)\n",
    "    if accumulator.is_ready():\n",
    "      # -----------------------------------#\n",
    "      # Learner step on sample from accumulator\n",
    "      accumulator_sample = accumulator.sample()\n",
    "      agent.learner_step(*accumulator_sample)\n",
    "      # -----------------------------------#\n",
    "   # Evaluation.\n",
    "  if not episode % evaluate_every:\n",
    "    returns = []\n",
    "    for _ in range(eval_episodes):\n",
    "      timestep = env.reset()\n",
    "      timesteps = [timestep]\n",
    "      while not timestep.last():\n",
    "        action = agent.actor_step(timestep)\n",
    "        timestep = env.step(action)\n",
    "        timesteps.append(timestep)\n",
    "      returns.append(np.sum([item.reward for item in timesteps[1:]]))\n",
    "\n",
    "    avg_returns.append(np.mean(returns))\n",
    "    print(f'Episode {episode:4d}: Average returns: {avg_returns[-1]:.2f}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2U4s5nHJuLKH"
   },
   "source": [
    "### ‚≠ê Exercise\n",
    "\n",
    "- Animate the last episode and plot the retun of the agent as a function of number of trainin episodes.\n",
    "\n",
    "- Does the agent manage to solve the task? Experiment with the number of training episodes, does it help? **Answer:** in most cases it won't help, the agent does not learn a good policy.\n",
    "\n",
    "- Look at the initialisation of the Q-function table, all the values are set to $-1$. What do you think would happen after updating the Q-function with an episode where an agent a) succeeds, b) fails in an episode? Do you see any problem here? For the ways to fix it, look at the next section. **Answer:** we initialises Q-funtion with -1 which is the worst possible value in this environment. It means that any random action would be better than that and after initial random actions, the greedy agent will just select the same action all the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "bM937s7HwXnc",
    "outputId": "43195bcb-7407-4a39-dce8-bf520eda633e"
   },
   "outputs": [],
   "source": [
    "# -----------------------------------#\n",
    "# Plot the average return as a function of number of episodes\n",
    "plt.plot(avg_returns)\n",
    "plt.xlabel('Number of episodes')\n",
    "plt.ylabel('Return')\n",
    "plt.title('Return of the greedy agent');\n",
    "# -----------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 579
    },
    "id": "KtKyWSwP2tw5",
    "outputId": "1c241027-c50b-48a0-c8ed-e94c6397a49f"
   },
   "outputs": [],
   "source": [
    "# -----------------------------------#\n",
    "# Animate the last episode\n",
    "animate([item.observation for item in timesteps])\n",
    "# -----------------------------------#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pumORMIq3AFD"
   },
   "source": [
    "## Epsilon-greedy agent\n",
    "\n",
    "So, most of the time, a Q-learning agent fails to solve the task. One of the reasons for it is that the greedy policy with respect to a given estimate of $Q^\\pi$ fails to *explore* the environment as needed. The problem is that our initialisation of the Q-function was \"pessimistic\" that means that once any of the actions is tried and is successful (by chance), the agent will keep selecting this action as it is \"greedily\" exploiting the Q-function estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqPXfIte8qUs"
   },
   "source": [
    "### ‚≠ê Exercise\n",
    "\n",
    "One way to encourage exploration is to initialise the Q-funtion \"optimistically\", for example, with zeros or ones. Try modifying the agent above like this, what happens in this case? Hint: look at the line `self._q = -np.ones((c * r * c, self._num_actions))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZCMvHFNvRkNp"
   },
   "outputs": [],
   "source": [
    "# Modify in the `__init__` function:\n",
    "# self._q = np.ones((c * r * c, self._num_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UVQqn2OxDOIL"
   },
   "source": [
    "Optimistic initialisation might help us in this case, but a more general solution is to use **epsilon-greedy agent**. An $\\epsilon$-greedy policy is a simple policy that at each time-step with probability $\\epsilon$ will choose a random action instead of the greedy action. This ensures constant exploration  during learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edVk9U6i-Q4Y"
   },
   "source": [
    "### ‚≠ê Exercise\n",
    "\n",
    "Update the QlearningAgent's policy to an $\\epsilon$-greedy policy. Most things stays the same, but you need to modify `actor_step` function to select the best action with probability $1-\\epsilon$ and a random action with probability $\\epsilon$. Note that agent takes parameters `epsilon` at the initialisation now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3UiaeE9K2rG4"
   },
   "outputs": [],
   "source": [
    "class EGQlearningAgent(object):\n",
    "  \"\"\"Epsilon-greedy Q learning agent.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               action_spec: specs.DiscreteArray,\n",
    "               observation_spec: specs.DiscreteArray,\n",
    "               epsilon: float = 0.1,\n",
    "               step_size: float = 0.1):\n",
    "    self._num_actions = action_spec.num_values\n",
    "    self._epsilon = epsilon\n",
    "    self._step_size = step_size\n",
    "    r, c = observation_spec.shape\n",
    "    # variable for representing the Q-function\n",
    "    # before we know anything, assume that all actions are bad\n",
    "    self._q = -np.ones((c * r * c, self._num_actions))\n",
    "\n",
    "  def _obs_to_index(self, obs):\n",
    "    \"\"\"Convert the observation into an index for accessing q values.\"\"\"\n",
    "    # The paddle location is always at the bottom.\n",
    "    obs_shape = obs.shape\n",
    "    paddle = np.where(obs[-1, :].flatten() == 1)[0][0]\n",
    "    obs = obs.flatten().astype(int)\n",
    "    # Case where the ball and paddle overlap.\n",
    "    if obs.sum() == 1:\n",
    "      ball = (obs_shape[0] - 1) * obs_shape[1]  + paddle\n",
    "    else:\n",
    "      ball = np.where(obs == 2)[0][0]\n",
    "    return paddle * np.prod(obs_shape) + ball\n",
    "\n",
    "  def actor_step(self, timestep, evaluation):\n",
    "    # Index into the Q value matrix.\n",
    "    qvalue = self._q[self._obs_to_index(timestep.observation)]\n",
    "    # Epsilon-greedy policy.\n",
    "    # -----------------------------------#\n",
    "    # With probability 1-self._epsilon, select the best action according to\n",
    "    # qvalue as before\n",
    "    if np.random.random() > self._epsilon:\n",
    "      train_a = np.argmax(qvalue)\n",
    "    # With probability self._epsilon, select a random action\n",
    "    else:\n",
    "      train_a = np.random.choice(self._num_actions)\n",
    "    # -----------------------------------#\n",
    "    if evaluation:\n",
    "      # During evaluation, always select the best action according to qvalue\n",
    "      return np.argmax(qvalue)\n",
    "    else:\n",
    "      return train_a\n",
    "\n",
    "  def learner_step(self, obs_tm1, a_tm1, r_t, discount_t, obs_t):\n",
    "    # Offline Q-value update.\n",
    "    obs_t = self._obs_to_index(obs_t)\n",
    "    obs_tm1 = self._obs_to_index(obs_tm1)\n",
    "    td_error = r_t + discount_t * np.max(self._q[obs_t]) - self._q[obs_tm1, a_tm1]\n",
    "    self._q[obs_tm1, a_tm1] += self._step_size * td_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RFsb924tEkfm"
   },
   "source": [
    "Let's test `actor_step`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4c91_TiGD__y",
    "outputId": "4c271c00-d53c-417b-d15c-9e76886e55c3"
   },
   "outputs": [],
   "source": [
    "# Example of initialising the agent and making a dummy observation and selecting an action\n",
    "env = Catch()\n",
    "# Create an agent\n",
    "agent = EGQlearningAgent(action_spec=env.action_spec(),\n",
    "                         observation_spec=env.observation_spec(),\n",
    "                         epsilon=0.3)\n",
    "\n",
    "# Make a fake observation\n",
    "DummyTransition = collections.namedtuple(\n",
    "    'DummyTransition', 'observation')\n",
    "obs = np.zeros(env.observation_spec().shape, \"float32\")\n",
    "obs[0,0]=2\n",
    "obs[-1,-1]=1\n",
    "dummy_timestep = DummyTransition(obs)\n",
    "\n",
    "# Select an action\n",
    "action_to_take = agent.actor_step(dummy_timestep, evaluation=False)\n",
    "print('Action = ', action_to_take)\n",
    "print('If you run this code several times, most of the time action would be 0, but sometimes 1 or 2.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X7fA7ox1ADYK"
   },
   "source": [
    "### Run loop\n",
    "\n",
    "Now let's run the training loop for an epsilon greedy agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "flIsxAsIeutr"
   },
   "source": [
    "### ‚≠ê Exercise\n",
    "\n",
    "Fill in the missing parameter values `train_episodes`, `epsilon`, and `step_size` to run the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I50JzJUt3NqL",
    "outputId": "63420caa-11a4-4b1c-a17d-c6a0d978d56b"
   },
   "outputs": [],
   "source": [
    "train_episodes = 1000  # @param\n",
    "evaluate_every = 100  # @param\n",
    "eval_episodes = 10  # @param\n",
    "epsilon = 0.5  # @param\n",
    "step_size = 0.2  # @param\n",
    "\n",
    "# Initialise the environment.\n",
    "env = Catch()\n",
    "timestep = env.reset()\n",
    "\n",
    "# Build and initialise the agent.\n",
    "agent = EGQlearningAgent(env.action_spec(),\n",
    "                         env.observation_spec(),\n",
    "                         epsilon=epsilon,\n",
    "                         step_size=step_size)\n",
    "\n",
    "# Initialise the accumulator.\n",
    "accumulator = TransitionAccumulator()\n",
    "\n",
    "# Run loop.\n",
    "avg_returns = []\n",
    "\n",
    "for episode in range(train_episodes):\n",
    "\n",
    "  # Prepare agent, environment and accumulator for a new episode.\n",
    "  timestep = env.reset()\n",
    "  accumulator.push(timestep, None)\n",
    "\n",
    "  while not timestep.last():\n",
    "    # Acting.\n",
    "    action = agent.actor_step(timestep, False)\n",
    "    # Agent-environment interaction.\n",
    "    timestep = env.step(action)\n",
    "    # Accumulate experience.\n",
    "    accumulator.push(timestep, action)\n",
    "\n",
    "    # Learning.\n",
    "    if accumulator.is_ready():\n",
    "      agent.learner_step(*accumulator.sample())\n",
    "\n",
    "  # Evaluation.\n",
    "  if not episode % evaluate_every:\n",
    "    returns = []\n",
    "    for _ in range(eval_episodes):\n",
    "      timestep = env.reset()\n",
    "      timesteps = [timestep]\n",
    "      while not timestep.last():\n",
    "        action = agent.actor_step(timestep, True)\n",
    "        timestep = env.step(action)\n",
    "        timesteps.append(timestep)\n",
    "      returns.append(np.sum([item.reward for item in timesteps[1:]]))\n",
    "\n",
    "    avg_returns.append(np.mean(returns))\n",
    "    print(f\"Episode {episode:4d}: Average returns: {avg_returns[-1]:.2f}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "DDGgxatuBNcN",
    "outputId": "f7d20e91-c73c-4c87-e3bd-fe342cdccf70"
   },
   "outputs": [],
   "source": [
    "# Plot the average return as a function of number of epsiodes\n",
    "plt.plot(avg_returns)\n",
    "plt.xlabel('Number of episodes')\n",
    "plt.ylabel('Return')\n",
    "plt.title('Return of the epsilon greedy agent');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 579
    },
    "id": "KsUn9CIRA0GO",
    "outputId": "de83c2c1-5738-4764-ac43-6f77976bb7da"
   },
   "outputs": [],
   "source": [
    "animate([item.observation for item in timesteps])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0LGZBrKEAsv2"
   },
   "source": [
    "### ‚≠ê Exercise\n",
    "\n",
    "- Does the agent manage to solve the task? Experiment with the number of training episodes, the exploration rate $\\epsilon$ and learning step. Can an agent achieve the perferc score? **Answer:** the hyperparameters above should be good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nlapdV4HFa38"
   },
   "source": [
    "### ‚≠ê Bonus exercise\n",
    "\n",
    "- Try to make the task harder: increase the size of the game board. Can the agent still solve this task?\n",
    "\n",
    "- Sometimes you may notice that the agent's policy is not very natural (even if its return is 1): instead of going to the location where the ball is going to fall and waiting, it takes some other random actions before that. Why do you think this might happen? How could it be fixed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nz6jNmWn-Odb"
   },
   "source": [
    "ü•≥ Congratulations on completing the first part of this tutorial, great job!!!\n",
    "\n",
    "It is the most important part as it sets the foundations which would allow you to understand more advanced RL algorithms. Next, we will look into Deep RL: when RL algorithms are combined with neural network training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J5Xa71zdFpRh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
