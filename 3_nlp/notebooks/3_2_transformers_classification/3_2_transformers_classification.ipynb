{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d996dfcd",
   "metadata": {
    "id": "d996dfcd",
    "tags": []
   },
   "source": [
    "Natural Language Processing Tutorial\n",
    "======\n",
    "\n",
    "This is the tutorial of the 2023 [Mediterranean Machine Learning Summer School](https://www.m2lschool.org/) on Natural Language Processing!\n",
    "\n",
    "This tutorial will explore the fundamental aspects of Natural Language Processing (NLP). Whether you are new to NLP or just beginning your journey, there's no need to worry, as the tutorial assumes minimal prior knowledge. Our focus will be on implementing everything from scratch to ensure clarity and understanding. To facilitate this, we will be using [JAX](https://jax.readthedocs.io/en/latest/), a library that offers an API similar to Numpy (and often identical) with the added benefit of automatic differentiation.\n",
    "\n",
    "## Outline\n",
    "\n",
    "- 0 Refresher on JAX, Haiku and Optax\n",
    "- 1 Introduction to NLP\n",
    "   - 1.1 The NLP pipeline\n",
    "   - 1.2 Classification pipeline: Multi-hot encoding + MLP model\n",
    "   - 1.3 Classification pipeline: Embeddings + Sequential Model\n",
    "- <span style=\"color:blue\">2 Introduction to the Transformers architecture </span>\n",
    "   - <span style=\"color:blue\">2.1 Transformer architecture </span>\n",
    "   - <span style=\"color:blue\">2.2 Implementing the core components </span>\n",
    "   - <span style=\"color:blue\">2.3 Transformer for classification pipeline </span>\n",
    "- 3 Advanced: Transformers for language translation\n",
    " - 3.1 The Transformer Decoder\n",
    " - 3.2 Transformer Decoder for character-based Language Modelling\n",
    " - 3.3 The full Transformer\n",
    " - 3.4 Transformer for Neural Machine Translation\n",
    "\n",
    "## Emojis\n",
    "\n",
    "Sections marked as [ 📝 ] contain cells with missing code that you should complete [ &#x1F4C4; ] is used for links to interesting external resources. When we use the words of an external resource we will cite it with &#x1F449; resource &#x1F448;.\n",
    "\n",
    "## Libraries\n",
    "\n",
    "We will keep our promise that (almost &#x1F60B; ) everything will be built from scratch. Indeed, all the vital and challenging components will be developed from zero. In fact the whole tutorial could be done only based on JAX. However, we recognize that certain minor technical details can become distracting if much time spent for them. For these tiny bits, we will ask help from [haiku](https://dm-haiku.readthedocs.io/en/latest/) to code neural network architectures, [optax](https://optax.readthedocs.io/en/latest/) to bring us the optimal parameters and [tokenizers](https://huggingface.co/docs/tokenizers/index) to quickly learning the vocabulary in each dataset. And the ubiquitous [numpy](https://numpy.org/) and [pandas](https://pandas.pydata.org/) for tensor handling. That's all!\n",
    "\n",
    "It would be also nice, if you have access to GPU! And hopefully due to google colab you can immediately access a cudas-enabled environment pressing the the button below.\n",
    "\n",
    "## Credits\n",
    "\n",
    "The tutorial is created by [Vasilis Gkolemis](https://givasile.github.io/) and [Matko Bošnjak](https://matko.info). It is highly inspired by [Deep Learning with Python (DLP)](https://www.manning.com/books/deep-learning-with-python) the famous book of Francois Chollet, last year's [M2Lschool](https://github.com/M2Lschool/tutorials2022) NLP tutorial especially for the transformers part, the [annotated transformer](http://nlp.seas.harvard.edu/annotated-transformer/) presentation\n",
    "\n",
    "## Note for Colab users\n",
    "\n",
    "To grab a GPU (if available), make sure you go to `Edit -> Notebook settings` and choose a GPU under `Hardware accelerator`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oVoarJlsMGdj",
   "metadata": {
    "id": "oVoarJlsMGdj"
   },
   "source": [
    "## Practical 2: Introduction to the Transformers architecture\n",
    "\n",
    "Welcome to Practical 2, where we will delve into the world of the Transformers architecture. Transformers were introduced in 2017 through the seminal work of Vaswani et. al, titled [\"Attention is All You Need\"](https://arxiv.org/abs/1706.03762). Since their inception, Transformers have garnered immense recognition, dominating the realm of Natural Language Processing (NLP) and serving as a wellspring of inspiration across various domains of Machine Learning, including computer vision and more.\n",
    "\n",
    "This Practical will focus on the Transformers' encoder component (Transformers consist of two parts: the encoder and the decoder). In the end, we will use the Transformers' encoder to tackle the IMDB sentiment analysis task, which was introduced in Practical 1. The key aim of this session is to attain a comprehensive understanding of the core elements that constitute Transformers, with a significant emphasis on the foundational building-block: the self-attention layer. For this reason, we will code every from scratch.\n",
    "\n",
    "In Practical 3, we will also code the decoder part (they share common building blocks) and use the complete Transformers architecture to solve natural language generation tasks; these are the tasks where the transformers really shine! In particular, we will attempt to train a English-Greek translation machine!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9445f099",
   "metadata": {},
   "source": [
    "#### The following cells are identical to the ones implemented in Practical 1. Let's just run them to ensure that all crucial functions are in RAM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ORFKb828ybV6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ORFKb828ybV6",
    "outputId": "3f55017a-0791-49e8-858f-3738b11bf671"
   },
   "outputs": [],
   "source": [
    "# assert that all important packages are installed\n",
    "!pip install dm-haiku optax tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2ef14f",
   "metadata": {
    "id": "4d2ef14f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import haiku as hk\n",
    "import optax\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tokenizers\n",
    "import os\n",
    "import typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e0917a-60de-4f35-a3b8-154418bb8c45",
   "metadata": {
    "id": "46e0917a-60de-4f35-a3b8-154418bb8c45",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# forces JAX to allocate memory as needed\n",
    "# see https://jax.readthedocs.io/en/latest/gpu_memory_allocation.html\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a173516-4bd2-41d5-9802-bcaa252b580c",
   "metadata": {
    "id": "1a173516-4bd2-41d5-9802-bcaa252b580c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def jax_has_gpu():\n",
    "    try:\n",
    "        _ = jax.device_put(jax.numpy.ones(1), device=jax.devices('gpu')[0])\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "gpu = jax_has_gpu()   # automatically checks for gpus, override if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05571cc6-e345-4d23-aa63-4a89c96e300c",
   "metadata": {
    "id": "05571cc6-e345-4d23-aa63-4a89c96e300c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize a key and a key iteration\n",
    "init_seed = 21\n",
    "key_iter = hk.PRNGSequence(jax.random.PRNGKey(init_seed))\n",
    "key = jax.random.PRNGKey(init_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5056aa5",
   "metadata": {
    "id": "c5056aa5"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31fd91a-49bd-4cd0-8c82-5c954e968b6c",
   "metadata": {
    "id": "b31fd91a-49bd-4cd0-8c82-5c954e968b6c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def funcs_from_stateful(stateful_forward, jit=True):\n",
    "    \"\"\"\n",
    "    Create and return stateless (pure JAX) functions from the stateful_forward function.\n",
    "\n",
    "    Args:\n",
    "        stateful_forward (Callable): The stateful forward function to be transformed.\n",
    "        jit (bool, optional): Whether to jit-compile the functions. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: A tuple containing the transformed model with full PRNG sequence support (model),\n",
    "        the transformed model with PRNG sequence support removed (model_fw), the predict function\n",
    "        with or without jit compilation (predict), the predict function with PRNG sequence support\n",
    "        removed and with or without jit compilation (predict_fw), and the init function for initializing\n",
    "        the model parameters (init_params).\n",
    "    \"\"\"\n",
    "    model = hk.transform(stateful_forward)\n",
    "    model_fw = hk.without_apply_rng(model)\n",
    "\n",
    "    if jit:\n",
    "        predict = jax.jit(model.apply)\n",
    "        predict_fw = jax.jit(model_fw.apply)\n",
    "        init_params = jax.jit(model.init)\n",
    "    else:\n",
    "        predict = model.apply\n",
    "        predict_fw = model_fw.apply\n",
    "        init_params = model.init\n",
    "    return model, model_fw, predict, predict_fw, init_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ae2996-9e57-420d-83fa-c83809d32a9a",
   "metadata": {
    "id": "86ae2996-9e57-420d-83fa-c83809d32a9a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_and_pred(key, x, mask):\n",
    "    \"\"\"\n",
    "    Perform initialization and inference using the provided parameters.\n",
    "\n",
    "    Args:\n",
    "        key (jax.random.PRNGKey): Random number generator key for initialization.\n",
    "        x (jnp.ndarray): Input data with shape (batch_size, num_features).\n",
    "        mask (jnp.ndarray): Mask with shape (batch_size,) representing valid elements in xx.\n",
    "\n",
    "    Returns:\n",
    "        None: This function does not return any value. It prints the results of the inference.\n",
    "    \"\"\"\n",
    "    key, *skey = jax.random.split(key, 3)\n",
    "    params = init_params(skey[0], x, mask, False)\n",
    "\n",
    "    print(\"Inference with predict, is_train=False\")\n",
    "    print(predict(params, skey[1], x, mask, False))\n",
    "\n",
    "    print(\"Inference with predict_fw, is_train=False\")\n",
    "    print(predict_fw(params, x, mask, False))\n",
    "\n",
    "    print(\"Inference with predict, is_train=True\")\n",
    "    print(predict(params, skey[1], x, mask, True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762d8051-a16a-4750-9432-b6404689653b",
   "metadata": {
    "id": "762d8051-a16a-4750-9432-b6404689653b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def get_loss(params, skey, x: jnp.ndarray, mask, y_gt: jnp.ndarray):\n",
    "    \"\"\"\n",
    "    Compute the binary cross-entropy loss for the given input and ground truth.\n",
    "\n",
    "    Args:\n",
    "        params (List): A list containing the weight vector (W) and bias (b).\n",
    "        skey (jax.random.PRNGKey): Random number generator key for prediction.\n",
    "        x (jnp.ndarray): Input data with shape (batch_size, num_features).\n",
    "        mask (jnp.ndarray): Mask with shape (batch_size,) representing valid elements in x.\n",
    "        y_gt (jnp.ndarray): Ground truth labels with shape (batch_size,).\n",
    "\n",
    "    Returns:\n",
    "        jnp.ndarray: The computed loss value.\n",
    "    \"\"\"\n",
    "\n",
    "    # Predict using skey state and is_train\n",
    "    y = predict(params, skey, x, mask, is_train=True)\n",
    "\n",
    "    # Compute the loss value\n",
    "    loss_value = optax.sigmoid_binary_cross_entropy(y, y_gt).mean(axis=-1)\n",
    "\n",
    "    return loss_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff7bf77",
   "metadata": {
    "id": "dff7bf77",
    "tags": []
   },
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(params, key, opt_state, x, mask, y_gt):\n",
    "    \"\"\"\n",
    "    Perform a single training step for the given input batch.\n",
    "\n",
    "    Args:\n",
    "        params (List): A list containing the weight vector (W) and bias (b).\n",
    "        key (jax.random.PRNGKey): Random number generator key for randomness.\n",
    "        opt_state (OptState): The state of the optimizer.\n",
    "        x (jnp.ndarray): Input data with shape (batch_size, num_features).\n",
    "        mask (jnp.ndarray): Mask with shape (batch_size,) representing valid elements in x.\n",
    "        y_gt (jnp.ndarray): Ground truth labels with shape (batch_size,).\n",
    "\n",
    "    Returns:\n",
    "        Tuple: A tuple containing the updated parameters (params), the new optimizer state (opt_state),\n",
    "        the computed loss value (loss), and the updated random number generator key (key).\n",
    "    \"\"\"\n",
    "\n",
    "    # Move the random generator\n",
    "    key, skey = jax.random.split(key)\n",
    "\n",
    "    # Define gradients with respect to the loss function\n",
    "    val_grad = jax.value_and_grad(get_loss)\n",
    "\n",
    "    # Get loss and gradients with respect to the input batch\n",
    "    loss, grads = val_grad(params, skey, x, mask, y_gt)\n",
    "\n",
    "    # Get updates and new optimizer state, based on gradients and previous state\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "\n",
    "    # Get new params based on previous params and updates\n",
    "    params = optax.apply_updates(params, updates)\n",
    "\n",
    "    return params, opt_state, loss, key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c5f2c1",
   "metadata": {
    "id": "08c5f2c1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def evaluate_on_batch(params, x: np.ndarray, mask, y_gt: np.ndarray):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a single input batch.\n",
    "\n",
    "    Args:\n",
    "        params (List): A list containing the weight vector (W) and bias (b).\n",
    "        x (jnp.ndarray): Input data with shape (batch_size, num_features).\n",
    "        mask (jnp.ndarray): Mask with shape (batch_size,) representing valid elements in x.\n",
    "        y_gt (jnp.ndarray): Ground truth labels with shape (batch_size,).\n",
    "\n",
    "    Returns:\n",
    "        Tuple: A tuple containing the accuracy and confusion matrix.\n",
    "            - accuracy (float): The accuracy of the model's predictions.\n",
    "            - confusion_matrix (jnp.ndarray): The confusion matrix with shape (2, 2).\n",
    "    \"\"\"\n",
    "    y_pred = predict_fw(params, x, mask, is_train=False)\n",
    "    y_pred_binary = (y_pred > 0).astype(int)\n",
    "    accuracy = jnp.mean(y_pred_binary == y_gt)\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    true_positive = jnp.sum(jnp.logical_and(y_pred_binary == 1, y_gt == 1))\n",
    "    false_positive = jnp.sum(jnp.logical_and(y_pred_binary == 1, y_gt == 0))\n",
    "    true_negative = jnp.sum(jnp.logical_and(y_pred_binary == 0, y_gt == 0))\n",
    "    false_negative = jnp.sum(jnp.logical_and(y_pred_binary == 0, y_gt == 1))\n",
    "\n",
    "    confusion_matrix = jnp.array([[true_negative, false_positive],\n",
    "                                  [false_negative, true_positive]])\n",
    "    return accuracy, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7865714f",
   "metadata": {
    "id": "7865714f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(params, x: jnp.ndarray, mask, y_gt: jnp.ndarray, batch_encode, batch_size=32):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the given input data using batching.\n",
    "\n",
    "    Args:\n",
    "        params (List): A list containing the weight vector (W) and bias (b).\n",
    "        x (jnp.ndarray): Input data with shape (num_samples, num_features).\n",
    "        mask (jnp.ndarray): Mask with shape (num_samples,) representing valid elements in x.\n",
    "        y_gt (jnp.ndarray): Ground truth labels with shape (num_samples,).\n",
    "        batch_encode (Callable): Function to encode the input data into batches.\n",
    "        batch_size (int, optional): Batch size. Defaults to 32.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: A tuple containing the test accuracy and confusion matrix.\n",
    "            - test_accuracy (float): The accuracy of the model's predictions on the test data.\n",
    "            - cm (jnp.ndarray): The confusion matrix with shape (2, 2).\n",
    "    \"\"\"\n",
    "    cm = jnp.zeros([2, 2])\n",
    "\n",
    "    # Evaluate on the test set with batching\n",
    "    test_accuracy = 0.0\n",
    "    num_batches = int(len(x) / batch_size)\n",
    "    for j in range(num_batches):\n",
    "        start_idx = j * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        x_batch = x[start_idx:end_idx]\n",
    "        y_batch = y_gt[start_idx:end_idx]\n",
    "\n",
    "        if mask is not None:\n",
    "            mask_batch = mask[start_idx:end_idx]\n",
    "        else:\n",
    "            mask_batch = None\n",
    "\n",
    "        if batch_encode is not None:\n",
    "            x_batch = batch_encode(x_batch)\n",
    "\n",
    "        # Move to GPU\n",
    "        if gpu:\n",
    "            x_batch = jnp.array(x_batch)\n",
    "            y_batch = jnp.array(y_batch)\n",
    "            x_batch = jax.device_put(x_batch, jax.devices(\"gpu\")[0])  # Assuming you have only one GPU\n",
    "            y_batch = jax.device_put(y_batch, jax.devices(\"gpu\")[0])  # Assuming you have only one GPU\n",
    "\n",
    "        batch_accuracy, batch_cm = evaluate_on_batch(params, x_batch, mask_batch, y_batch)\n",
    "        test_accuracy += batch_accuracy\n",
    "        cm += batch_cm\n",
    "\n",
    "    test_accuracy /= num_batches\n",
    "    return test_accuracy, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968573cc",
   "metadata": {
    "id": "968573cc",
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(params: list,\n",
    "          key,\n",
    "          x_tr: jnp.ndarray,\n",
    "          mask_tr: jnp.ndarray,\n",
    "          y_tr: jnp.ndarray,\n",
    "          epochs: int,\n",
    "          batch_size: int,\n",
    "          x_te: jnp.ndarray,\n",
    "          mask_te: jnp.ndarray,\n",
    "          y_te: jnp.ndarray,\n",
    "          batch_encode: typing.Union[None, typing.Callable] = None,\n",
    "          eval_every: int = 1,\n",
    "          loss_every_batch: int = 32,\n",
    "          gpu=False):\n",
    "    \"\"\"\n",
    "    Train the model using mini-batch stochastic gradient descent.\n",
    "\n",
    "    Args:\n",
    "        params (List): A list containing the weight vector (W) and bias (b).\n",
    "        key (jax.random.PRNGKey): Random number generator key for randomness.\n",
    "        x_tr (jnp.ndarray): Training input data with shape (num_train_samples, num_features).\n",
    "        mask_tr (jnp.ndarray): Mask with shape (num_train_samples,) representing valid elements in x_tr.\n",
    "        y_tr (jnp.ndarray): Training ground truth labels with shape (num_train_samples,).\n",
    "        epochs (int): Number of training epochs.\n",
    "        batch_size (int): Batch size.\n",
    "        x_te (jnp.ndarray): Test input data with shape (num_test_samples, num_features).\n",
    "        mask_te (jnp.ndarray): Mask with shape (num_test_samples,) representing valid elements in x_te.\n",
    "        y_te (jnp.ndarray): Test ground truth labels with shape (num_test_samples,).\n",
    "        batch_encode (Union[None, Callable], optional): Function to encode the input data into batches. Defaults to None.\n",
    "        eval_every (int, optional): Number of epochs between evaluations on train and test sets. Defaults to 1.\n",
    "        loss_every_batch (int, optional): Number of batches between printing the loss value. Set to False to disable printing. Defaults to 32.\n",
    "        gpu (bool, optional): Whether to use GPU acceleration. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        List: The updated model parameters after training.\n",
    "    \"\"\"\n",
    "    opt_state = optimizer.init(params)\n",
    "    nof_instances = x_tr.shape[0]\n",
    "    for e in range(epochs):\n",
    "        nof_full_batches = nof_instances // batch_size\n",
    "        for i in range(nof_full_batches):\n",
    "\n",
    "            # Get batch\n",
    "            batch_start = i * batch_size\n",
    "            batch_end = (i + 1) * batch_size\n",
    "            x_batch = x_tr[batch_start:batch_end]\n",
    "            y_batch = y_tr[batch_start:batch_end]\n",
    "\n",
    "            if mask_tr is not None:\n",
    "                mask_batch = mask_tr[batch_start:batch_end]\n",
    "            else:\n",
    "                mask_batch = None\n",
    "\n",
    "            # Vectorize if needed\n",
    "            if batch_encode is not None:\n",
    "                x_batch = batch_encode(x_batch)\n",
    "\n",
    "            # Move to GPU\n",
    "            if gpu:\n",
    "                x_batch = jnp.array(x_batch)\n",
    "                y_batch = jnp.array(y_batch)\n",
    "                x_batch = jax.device_put(x_batch, jax.devices(\"gpu\")[0])  # Assuming you have only one GPU\n",
    "                y_batch = jax.device_put(y_batch, jax.devices(\"gpu\")[0])  # Assuming you have only one GPU\n",
    "\n",
    "            params, opt_state, loss, key = train_step(params, key, opt_state, x_batch, mask_batch, y_batch)\n",
    "\n",
    "            if loss_every_batch is not False:\n",
    "                if i % loss_every_batch == 0:\n",
    "                    print(\"Epoch: %d, Step %d/%d, Loss: %.3f\" % (e, i, nof_full_batches, loss))\n",
    "\n",
    "        if eval_every is not False:\n",
    "            if e % eval_every == 0:\n",
    "\n",
    "                # Evaluate on the whole training set\n",
    "                train_accuracy, train_cm = evaluate(params, x_tr, mask_tr, y_tr, batch_encode, batch_size)\n",
    "                print(\"Epoch: %d, Train Accuracy: %.4f\" % (e, train_accuracy))\n",
    "                print(\"Confusion Matrix:\\n\", train_cm)\n",
    "\n",
    "                # Evaluate on the test set\n",
    "                test_accuracy, test_cm = evaluate(params, x_te, mask_te, y_te, batch_encode, batch_size)\n",
    "                print(\"Epoch: %d, Test Accuracy: %.4f\" % (e, test_accuracy))\n",
    "                print(\"Confusion Matrix:\\n\", test_cm)\n",
    "\n",
    "                print(\"\\n\")\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf7305c-80d6-444a-b914-62becf3eb817",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8cf7305c-80d6-444a-b914-62becf3eb817",
    "outputId": "d09779a7-4832-4781-84b3-b04aaf3e0d56",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load dataset if needed\n",
    "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -xf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39131fe-2a3c-4c98-89f6-3e572a6ed051",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a39131fe-2a3c-4c98-89f6-3e572a6ed051",
    "outputId": "b2dc8ff5-0655-4e7e-8fbb-aa304959b692",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_all_txts(dir):\n",
    "    # all files in dir\n",
    "    files = os.listdir(dir)\n",
    "\n",
    "    sentences = []\n",
    "    for file in files:\n",
    "        filepath = dir + '/' + file\n",
    "        with open(filepath, 'r') as ff:\n",
    "            for line in ff:\n",
    "                line = line.strip()\n",
    "                sentences.append(line)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def load_dataset(dir):\n",
    "    pos_dir = dir + '/pos'\n",
    "    neg_dir = dir + '/neg'\n",
    "\n",
    "    # read all txts in dir\n",
    "    pos_sentences = read_all_txts(pos_dir)\n",
    "    neg_sentences = read_all_txts(neg_dir)\n",
    "\n",
    "    # to a dataframe\n",
    "    df_pos = pd.DataFrame({'text': pos_sentences, 'label': 1})\n",
    "    df_neg = pd.DataFrame({'text': neg_sentences, 'label': 0})\n",
    "\n",
    "    df = pd.concat([df_pos, df_neg])\n",
    "\n",
    "    # shuffle\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "dir = 'aclImdb/train/'\n",
    "df_tr = load_dataset(dir)\n",
    "dir = 'aclImdb/test/'\n",
    "df_te = load_dataset(dir)\n",
    "\n",
    "df_tr = df_tr.dropna()\n",
    "df_te = df_te.dropna()\n",
    "\n",
    "X_tr = df_tr.iloc[:, 0].to_numpy()\n",
    "Y_tr = df_tr.iloc[:, 1].to_numpy()\n",
    "\n",
    "X_te = df_te.iloc[:, 0].to_numpy()\n",
    "Y_te = df_te.iloc[:, 1].to_numpy()\n",
    "\n",
    "print(\"The are %d training examples, where %d are positive and %d are negative reviews\" % (df_tr.shape[0], df_tr.loc[df_tr[\"label\"] == 1, :].shape[0], df_tr.loc[df_tr[\"label\"] == 1, :].shape[0]))\n",
    "print(\"The are %d testing examples, where %d are positive and %d are negative reviews\" % (df_te.shape[0], df_te.loc[df_te[\"label\"] == 1, :].shape[0], df_te.loc[df_te[\"label\"] == 1, :].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918ce8f3-ff58-408b-be8f-c7b1497034f8",
   "metadata": {
    "id": "918ce8f3-ff58-408b-be8f-c7b1497034f8",
    "outputId": "ce6a555a-4d58-4688-cc71-37a35da857f5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define the tokenizer\n",
    "tokenizer = tokenizers.Tokenizer(tokenizers.models.Unigram())\n",
    "tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.Whitespace()\n",
    "tokenizer.normalizer = tokenizers.normalizers.Lowercase()\n",
    "\n",
    "# say how it will be trained, to learn the vocabulary\n",
    "vocab_size = 20000\n",
    "special_tokens = [\"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\",]\n",
    "unk_token = \"[UNK]\"\n",
    "max_piece_length = 16\n",
    "trainer = tokenizers.trainers.UnigramTrainer(\n",
    "    special_tokens=special_tokens, # special tokens\n",
    "    vocab_size=vocab_size, # vocabulary size\n",
    "    unk_token=unk_token, # set the unknown token\n",
    "    show_progress=True, # show progress\n",
    ")\n",
    "\n",
    "# train the tokenizer\n",
    "tokenizer.train_from_iterator(X_tr, trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0853305a-0d5f-47e7-a066-be09624485b7",
   "metadata": {
    "id": "0853305a-0d5f-47e7-a066-be09624485b7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# now let's set it to a normal value\n",
    "sequence_length = 600\n",
    "truncation_length = 600\n",
    "tokenizer.enable_padding(length=sequence_length)\n",
    "tokenizer.enable_truncation(max_length=truncation_length)\n",
    "def batch_encode(X):\n",
    "    enc_list = tokenizer.encode_batch(X)\n",
    "    enc_list_2 = []\n",
    "    mask_list = []\n",
    "    for i, enc in enumerate(enc_list):\n",
    "        enc_list_2.append(enc.ids)\n",
    "        mask_list.append(enc.attention_mask)\n",
    "    return np.array(enc_list_2), np.array(mask_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69579c47-46cd-4c8c-97fa-ef557b480735",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "69579c47-46cd-4c8c-97fa-ef557b480735",
    "outputId": "78d818c1-c6a6-4523-fb0d-b284cb5541d9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# but since it is not memory-consuming, we can use the whole dataset\n",
    "X_tr_enc, X_tr_mask = batch_encode(X_tr)\n",
    "X_te_enc, X_te_mask = batch_encode(X_te)\n",
    "\n",
    "print(X_tr_enc.shape)\n",
    "print(X_te_enc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bhszV0MGdo",
   "metadata": {
    "id": "22bhszV0MGdo"
   },
   "outputs": [],
   "source": [
    "class MaskedEmbedding(hk.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(MaskedEmbedding, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "    def __call__(self, inputs, mask):\n",
    "        \"\"\"\n",
    "        Apply masked embedding to the input tokens.\n",
    "\n",
    "        Args:\n",
    "            inputs (jnp.ndarray): Input data with shape (batch_size, sequence_length).\n",
    "            mask (jnp.ndarray): Mask with shape (batch_size, sequence_length) representing valid elements in inputs.\n",
    "\n",
    "        Returns:\n",
    "            jnp.ndarray: Masked embeddings with shape (batch_size, sequence_length, embed_size).\n",
    "        \"\"\"\n",
    "        # Define an Embedding layer\n",
    "        embeddings = hk.Embed(self.vocab_size, self.embed_size)(inputs)\n",
    "\n",
    "        # Expand mask to match the embedding shape\n",
    "        mask_expanded = jnp.expand_dims(mask, axis=-1)  # (BS, S, 1)\n",
    "\n",
    "        # Apply mask to zero out embeddings for masked tokens\n",
    "        masked_embeddings = embeddings * mask_expanded\n",
    "\n",
    "        return masked_embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96WKo4zPMGdo",
   "metadata": {
    "id": "96WKo4zPMGdo"
   },
   "source": [
    "# 2 Introduction to the Transformers architecture\n",
    "\n",
    "The content covered in this section is a summary of the material presented in last year's [M2Lschool](https://github.com/M2Lschool/tutorials2022). For those seeking a deeper dive into Transformers, the original source provides further in-depth information.\n",
    "\n",
    "## 2.1 The Transformers Architecture\n",
    "\n",
    "Transformers consist of two parts; the encoder and the decoder. The **encoder** is responsible for transforming raw input data, such as a sequence of words, into meaningful hidden representations and the **decoder** is used for predicting sequences of outputs, such as sequence of words. Therefore, the complete Transformer architecture (encoder and decoder) is appropriate for solving sequence-to-sequence tasks. For simpler classification tasks, such as the one that we will use in this Practical, we can only use the encoder part of the architecture.\n",
    "\n",
    "Let's briefly introduce the encoder and decoder, along with their core logic:\n",
    "\n",
    "#### Encoder\n",
    "\n",
    "The encoder's role is to convert a sequence of words into dense, meaningful hidden representations, making them usable by other components like the decoder or other networks.\n",
    "\n",
    "The Transformer Encoder (depicted in the left figure below) takes a source sequence, processes it using **Attention**, and passes the results through a **fully-connected feed-forward** block with pointwise non-linear activation. Residual connections and layer normalization are applied to both operations. This process is repeated $N$ times using stacked replicas to compute the final word representations.\n",
    "\n",
    "#### Decoder\n",
    "\n",
    "The decoder's task is to learn the alignment between the source and target sequences. For instance, in machine translation, the decoder learns which words to generate in the target language based on the words in the source language.\n",
    "\n",
    "The Transformer Decoder (shown in the right figure below) can receive word representations as inputs and is given the **target sentence** during training to establish an association with the source.\n",
    "\n",
    "Notably, the decoder incorporates two attention operations: the first involves masked self-attention, and the second attends to the encoder output. We will delve into more details on these operations later in the tutorial.\n",
    "\n",
    "\n",
    "![alt text for screen readers](./../../images/transformers.png)\n",
    "---\n",
    "\n",
    "\n",
    "- Original paper: [Attention is All you Need](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)\n",
    "- In-depth guide on Transformer components: [Formal Definitions in Transformers](https://arxiv.org/pdf/2207.09238.pdf)\n",
    "- Practical PyTorch Transformer walkthrough: [The Annotated Transformer](http://nlp.seas.harvard.edu/annotated-transformer/http://nlp.seas.harvard.edu/annotated-transformer/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eec7fdb-95dd-4cfc-9f74-f63b5da681f1",
   "metadata": {
    "id": "7eec7fdb-95dd-4cfc-9f74-f63b5da681f1"
   },
   "source": [
    "## 2.2 Implementing the Encoder from scratch\n",
    "\n",
    "In this section, we will focus on implementing the essential building blocks that are utilized by both the encoder and the decoder. The core components of an encoder are:\n",
    "\n",
    "- Self-Attention (Scaled Dot Product Attention)\n",
    "- Multi-Headed Attention\n",
    "- Feed-Forward Networks\n",
    "- Positional Encoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "B38LT4QdMGdp",
   "metadata": {
    "id": "B38LT4QdMGdp"
   },
   "source": [
    "#### [ 📝 ] Self-attention\n",
    "\n",
    "The intuition behind self-attention is that individual entities (tokens), have different meanings (representations) depending on their context. The self-attention layer, the fundamental building-block of transformers, enriches individual token embeddings to context-aware embeddings &#x1F449; [DLP](https://www.manning.com/books/deep-learning-with-python) &#x1F448;\n",
    "\n",
    "<img src=\"./../../images/attention_mechanism_chollet.png\">\n",
    "\n",
    "The scaled dot product attention is defined as:\n",
    "\n",
    "$$\n",
    "attention(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\n",
    "$$\n",
    "\n",
    "Where $d_k$ is a constant scalar. In the original paper, $d_k$ corresponds to the dimension of the query/key/value (they all share the same dimension).\n",
    "\n",
    "For your first exercise, you will implement the scaled dot product attention (Equation above). You will notice that the function accepts a `mask` parameter. The mask allows us to *ignore* some portion of the sequence (typically, if any padding is present)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b82000-2b71-4034-8cce-a228b91e40f3",
   "metadata": {
    "id": "49b82000-2b71-4034-8cce-a228b91e40f3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    \"\"\"\n",
    "    Scaled Dot Product Attention.\n",
    "\n",
    "    Args:\n",
    "        q (jnp.ndarray): query, shape (batch_size, ..., hidden_dim)\n",
    "        k (jnp.ndarray): key, shape (batch_size, ..., hidden_dim)\n",
    "        v (jnp.ndarray): values, shape (batch_size, ..., hidden_dim)\n",
    "        mask (jnp.ndarray): values, shape (broadcastable to: B, ..., S, S)\n",
    "\n",
    "    Returns:\n",
    "        List(jnp.ndarray, jnp.ndarray):\n",
    "            - attention output (batch_size, ..., hidden_dim)\n",
    "            - attention_weights (batch_size, ...)\n",
    "    \"\"\"\n",
    "    ###################\n",
    "    # YOUR CODE HERE #\n",
    "    # Steps:\n",
    "    # (i) tensor multiplication between q, k to get the scores\n",
    "    # (ii) if mask is not None, set the scores of masked out values a very small value\n",
    "    # (iii) softmax the scores to get attention weights\n",
    "    # (iv) tensor multiplication between attention_weights and values to get values\n",
    "    # (v) return values and attention_weights\n",
    "    ###################\n",
    "    return values, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2bd430",
   "metadata": {},
   "source": [
    "Check that your implementation is correct by running the following cell. Check that the output shape is `(BS, S, DIM)` and `(BS, S, S)` for values and attention, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89be39af-7a8b-4248-84da-c8bfa6d0b507",
   "metadata": {
    "id": "89be39af-7a8b-4248-84da-c8bfa6d0b507",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# generate a random input for testing\n",
    "BS = 10\n",
    "S = 100\n",
    "DIM = 128\n",
    "xx_emb = np.random.randn(BS, S, DIM)\n",
    "mask = np.random.randint(0, 2, size=(BS, S))\n",
    "\n",
    "# Testing Scaled Dot Product\n",
    "values, attention = scaled_dot_product(xx_emb, xx_emb, xx_emb, jnp.expand_dims(mask, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kZ3s_IyUMGdp",
   "metadata": {
    "id": "kZ3s_IyUMGdp",
    "outputId": "cd29df12-a97e-416a-aceb-404779d077e1"
   },
   "outputs": [],
   "source": [
    "print(\"Values shape:\", values.shape)\n",
    "print(\"Attention shape:\", attention.shape)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a982036",
   "metadata": {},
   "source": [
    "Check that self-attention can generalize to any number of dimensions.\n",
    "But also check that the self-attention happens only on the `[q|v|k].shape[-2]` dimension (not all the dimensions)\n",
    "For example, see the ouput of the next cell. The attention shape is `(BS, H, W, W)` which means that the self-attention happens only along the `H` dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oaQ9GRiQMGdp",
   "metadata": {
    "id": "oaQ9GRiQMGdp"
   },
   "outputs": [],
   "source": [
    "# Check that self-attention works for any number of dimensions\n",
    "# But also check that the self-attention happens only on the [q|v|k].shape[-2] dimension (not all except the first)\n",
    "# You can check that looking at the attention shape (next cell)\n",
    "BS = 10\n",
    "H = 32\n",
    "W = 64\n",
    "DIM = 8\n",
    "xx_image = np.random.randn(BS, H, W, DIM)\n",
    "mask_image = np.random.randint(0, 2, size=(BS, H, W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DLIjjUYiMGdq",
   "metadata": {
    "id": "DLIjjUYiMGdq",
    "outputId": "26c014dc-d23c-45f9-a805-d6bfdd6ec070"
   },
   "outputs": [],
   "source": [
    "# Testing Scaled Dot Product\n",
    "values, attention = scaled_dot_product(xx_image, xx_image, xx_image, jnp.expand_dims(mask_image, -1))\n",
    "\n",
    "print(\"Values shape:\", values.shape)\n",
    "print(\"Attention shape:\", attention.shape)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77394d15-b565-4486-abaf-b575c49d9e51",
   "metadata": {
    "id": "77394d15-b565-4486-abaf-b575c49d9e51"
   },
   "source": [
    "#### [ 📝 ] Multi-Headed Attention\n",
    "\n",
    "In the scaled dot product attention, an element of the sequence can attend to any other element, but it cannot focus on multiple aspects of the sequence simultaneously. To address this limitation, we introduce the concept of multi-headed attention.\n",
    "\n",
    "In the encoder unit of the Transformer, the first layer applies a *multi-headed self-attention*, which means that words within the sequence interact and align with each other (self-attention), and multiple different alignments are learned simultaneously (multi-headed) through separate *attention heads*. This learning paradigm, combined with a linguistically founded training objective, has contributed to the success of modern language models.\n",
    "\n",
    "With multi-headed attention, we have $h$ attention heads, where each attention head is a linear projection of the sequence $Q$, $K$, and $V$:\n",
    "\n",
    "$$\n",
    "\\text{attention}(Q, K, V) = \\text{concat}(head_1, ..., head_h)W^O\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{head}_i = \\text{attention}(QW^Q_i, KW^K_i, VW^V_i)\n",
    "$$\n",
    "\n",
    "Here, $W^Q_i \\in \\mathbb{R}^{d_q \\times d_k/h}$, $W^K_i \\in \\mathbb{R}^{d_k \\times d_k/h}$, $W^V_i \\in \\mathbb{R}^{d_v \\times d_v/h}$, and $W^O \\in \\mathbb{R}^{hd_v \\times d_v}$. It's important to note that $d_k$ and $d_v$ have the same dimension, so $d_v/h$ is equal to $d_k/h$.\n",
    "\n",
    "When implementing multi-headed attention, we first apply linear projections for $Q$, $K$, and $V$ using matrix multiplication, and then split the results into $h$ heads. Next, we apply the scaled dot product attention independently for each attention head and concatenate the results.\n",
    "To facilitate your implementation, we provide a Haiku Module for Multi-Headed Attention. Take your time to review the class and understand its main components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b385d5e-3914-41d4-b48d-8af1af512ffa",
   "metadata": {
    "id": "5b385d5e-3914-41d4-b48d-8af1af512ffa",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiheadAttention(hk.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, name=None):\n",
    "        \"\"\"\n",
    "        Multi-Headed Attention Module.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): The dimension of the model, i.e. last dimension of q, k, v matrices (should be divisible by num_heads)\n",
    "            num_heads (int): The number of attention heads.\n",
    "            name (str): Optional name of the module.\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = self.d_model // self.num_heads\n",
    "        self.proj_q = hk.Linear(self.d_model)\n",
    "        self.proj_k = hk.Linear(self.d_model)\n",
    "        self.proj_v = hk.Linear(self.d_model)\n",
    "        self.proj_o = hk.Linear(self.d_model)\n",
    "\n",
    "    def __call__(self, q, k, v, mask=None):\n",
    "        \"\"\"\n",
    "        Apply Multi-Headed Attention.\n",
    "\n",
    "        Args:\n",
    "            q (jnp.ndarray): Query tensor with shape (batch_size, sequence_length, d_model).\n",
    "            k (jnp.ndarray): Key tensor with shape (batch_size, sequence_length, d_model).\n",
    "            v (jnp.ndarray): Value tensor with shape (batch_size, sequence_length, d_model).\n",
    "            mask (jnp.ndarray): Mask tensor with shape (batch_size, sequence_length) representing valid elements in the input sequences.\n",
    "\n",
    "        Returns:\n",
    "            jnp.ndarray: Output tensor after multi-headed attention with shape (batch_size, sequence_length, d_model).\n",
    "            jnp.ndarray: Attention weights tensor with shape (batch_size, num_heads, sequence_length, sequence_length).\n",
    "        \"\"\"\n",
    "        ###################\n",
    "        # YOUR CODE HERE #\n",
    "        # Steps:\n",
    "        # (i) for each of q, k, v\n",
    "        # (a) linear projection from (B, D, d_model) to (B, D, d_model)\n",
    "        # (b) reshape from (B, D, d_model) to (B, h, S, d_k)\n",
    "        # (ii) add a dimension to the mask (from (B, ...) to (B, 1, ...)\n",
    "        # (iii) apply scaled dot product to (h, q, v)\n",
    "        # note that input of (B,h,S,d_k) will give us output of (B,h,S,d_k) and attention matrix of (B, h, S, S)\n",
    "        # which is exactly what we want\n",
    "        # (iv) reshape output from (B,h,S,d_k) to (B, S, h*d_k)\n",
    "        # (v) last linear projection from (B, S, h*d_k) to (B, S, h*d_k)\n",
    "        # (vi) return output, attention\n",
    "        ###################\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c219572-c2e8-4cbf-ae01-556b52eeeffb",
   "metadata": {
    "id": "8c219572-c2e8-4cbf-ae01-556b52eeeffb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test MultiheadAttention implementation\n",
    "bs = xx_emb.shape[0]\n",
    "seq_len = xx_emb.shape[1]\n",
    "d_model = xx_emb.shape[-1]\n",
    "num_heads = 16\n",
    "def stateful_forward(q, k, v, mask=None):\n",
    "    mha = MultiheadAttention(d_model, num_heads, name=\"mha\")\n",
    "    return mha(q, k, v, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e44566-cf4e-430b-94fd-87ef7ce4fd58",
   "metadata": {
    "id": "f9e44566-cf4e-430b-94fd-87ef7ce4fd58",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model, model_fw, predict, predict_fw, init_params = funcs_from_stateful(stateful_forward, jit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfceb492-de1f-4b98-931e-72bac866bb06",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cfceb492-de1f-4b98-931e-72bac866bb06",
    "outputId": "3045c342-536a-4091-b7b1-34d63bf58313",
    "tags": []
   },
   "outputs": [],
   "source": [
    "key, skey = jax.random.split(key)\n",
    "\n",
    "params = init_params(key, xx_emb, xx_emb, xx_emb, jnp.expand_dims(mask, -1))\n",
    "out = predict_fw(params, xx_emb, xx_emb, xx_emb, jnp.expand_dims(mask, -1))\n",
    "output_matrix = out[0]\n",
    "attention_matrix = out[1]\n",
    "print(output_matrix.shape)\n",
    "print(attention_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83de7946-154c-422c-bed7-e64523bc7fd5",
   "metadata": {
    "id": "83de7946-154c-422c-bed7-e64523bc7fd5"
   },
   "source": [
    "#### Feed Forward Sublayer\n",
    "\n",
    "This sublayer is composed of a fully-connected feed-forward network. The main idea is to learn a linear transformation of the hidden representation of the previous layer. This layer has an inner hidden layer of size `d_ff`, and an inner activation function (e.g., ReLU). The `PositionwiseFeedForward` class below implements this sub-layer. It is initialized using the parameters:\n",
    "\n",
    "- `d_model`: size of the hidden representation of the input.\n",
    "- `d_ff`: inner size of the hidden layer.\n",
    "- `p_dropout`: dropout probability (dropout will be applied during training).\n",
    "\n",
    "The `PositionwiseFeedForward` class implements the `__call__` method. It takes as input the previous layer's hidden representation and returns the current layer's hidden representation by applying the fully-connected network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a62ad48-d53a-4b7b-a2f4-b325b137ecc5",
   "metadata": {
    "id": "3a62ad48-d53a-4b7b-a2f4-b325b137ecc5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(hk.Module):\n",
    "    \"\"\"\n",
    "    This class is used to create a position-wise feed-forward network.\n",
    "\n",
    "    Args:\n",
    "        d_model (int): The size of the embedding vector.\n",
    "        d_ff (int): The size of the hidden layer.\n",
    "        p_dropout (float, optional): The dropout probability. Default is 0.1.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, d_ff: int, p_dropout: float = 0.1, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.p_dropout = p_dropout\n",
    "\n",
    "        self.w_1 = hk.Linear(self.d_ff)\n",
    "        self.w_2 = hk.Linear(self.d_model)\n",
    "\n",
    "    def __call__(self, x, is_train=True):\n",
    "        \"\"\"\n",
    "        Apply the position-wise feed-forward network.\n",
    "\n",
    "        Args:\n",
    "            x (jnp.ndarray): The input sequence with shape (batch_size, sequence_length, d_model).\n",
    "            is_train (bool, optional): Whether the model is in training mode. Default is True.\n",
    "\n",
    "        Returns:\n",
    "            jnp.ndarray: The output of the position-wise feed-forward network with shape (batch_size, sequence_length, d_model).\n",
    "        \"\"\"\n",
    "        x = jax.nn.relu(self.w_1(x))\n",
    "        if is_train:\n",
    "            x = hk.dropout(hk.next_rng_key(), self.p_dropout, x)\n",
    "\n",
    "        y = self.w_2(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f96da7-6982-4b72-9e19-4328add57941",
   "metadata": {
    "id": "88f96da7-6982-4b72-9e19-4328add57941"
   },
   "source": [
    "#### Encoder Block\n",
    "\n",
    "And let's pack the Multihead Attention and the PositionwiseFeedForward in a single block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0c39a0-f4b1-4782-9615-b49ac49de8ea",
   "metadata": {
    "id": "fa0c39a0-f4b1-4782-9615-b49ac49de8ea",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EncoderBlock(hk.Module):\n",
    "    \"\"\"\n",
    "    This class is used to create an encoder block.\n",
    "\n",
    "    Args:\n",
    "        d_model (int): The size of the embedding vector.\n",
    "        num_heads (int): The number of attention heads.\n",
    "        d_ff (int): The size of the hidden layer.\n",
    "        p_dropout (float): The dropout probability.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, p_dropout, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_ff = d_ff\n",
    "        self.p_dropout = p_dropout\n",
    "\n",
    "        # self-attention sub-layer\n",
    "        self.self_attn = MultiheadAttention(\n",
    "            d_model=self.d_model, num_heads=self.num_heads\n",
    "        )\n",
    "        # positionwise feedforward sub-layer\n",
    "        self.ff = PositionwiseFeedForward(\n",
    "            d_model=self.d_model, d_ff=self.d_ff, p_dropout=self.p_dropout\n",
    "        )\n",
    "\n",
    "        self.norm1 = hk.LayerNorm(\n",
    "            axis=-1, param_axis=-1, create_scale=True, create_offset=True\n",
    "        )\n",
    "        self.norm2 = hk.LayerNorm(\n",
    "            axis=-1, param_axis=-1, create_scale=True, create_offset=True\n",
    "        )\n",
    "\n",
    "    def __call__(self, x, mask=None, is_train=True):\n",
    "        \"\"\"\n",
    "        Apply the encoder block to the input sequence.\n",
    "\n",
    "        Args:\n",
    "            x (jnp.ndarray): The input sequence with shape (batch_size, sequence_length, d_model).\n",
    "            mask (jnp.ndarray, optional): The mask to be applied to the self-attention layer with shape (batch_size, sequence_length). Default is None.\n",
    "            is_train (bool, optional): Whether the model is in training mode. Default is True.\n",
    "\n",
    "        Returns:\n",
    "            jnp.ndarray: The output of the encoder block, which is the updated input sequence with shape (batch_size, sequence_length, d_model).\n",
    "        \"\"\"\n",
    "        d_rate = self.p_dropout if is_train else 0.0\n",
    "\n",
    "        # attention sub-layer\n",
    "        sub_x, _ = self.self_attn(x, x, x, mask=mask)\n",
    "        if is_train:\n",
    "            sub_x = hk.dropout(hk.next_rng_key(), self.p_dropout, sub_x)\n",
    "        x = self.norm1(x + sub_x)  # residual conn\n",
    "\n",
    "        # feedforward sub-layer\n",
    "        sub_x = self.ff(x, is_train=is_train)\n",
    "        if is_train:\n",
    "            sub_x = hk.dropout(hk.next_rng_key(), self.p_dropout, sub_x)\n",
    "        x = self.norm2(x + sub_x)  # sub_x\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ed6f71-582a-4089-8574-89400c02378d",
   "metadata": {
    "id": "58ed6f71-582a-4089-8574-89400c02378d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Testing the Encoder block\"\"\"\n",
    "d_ff = 128\n",
    "\n",
    "def stateful_forward(x, mask, is_train):\n",
    "    bl = EncoderBlock(d_model=d_model, num_heads=num_heads, d_ff=d_ff, p_dropout=0.1)\n",
    "    return bl(x, mask, is_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a482ca-87b5-4005-bdaa-55cf6d36b591",
   "metadata": {
    "id": "29a482ca-87b5-4005-bdaa-55cf6d36b591",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model, model_fw, predict, predict_fw, init_params = funcs_from_stateful(stateful_forward, jit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c22b0b-d490-4a67-8396-0cd3849fb31a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "11c22b0b-d490-4a67-8396-0cd3849fb31a",
    "outputId": "e5c11a1e-f19c-49b6-fcc8-e99655c6e05f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "key, skey = jax.random.split(key)\n",
    "is_train = True\n",
    "params = init_params(key, xx_emb,  jnp.expand_dims(mask, -1), is_train)\n",
    "out = predict(params, key, xx_emb,  jnp.expand_dims(mask, -1), is_train)\n",
    "print(out[0].shape)\n",
    "print(out[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3910aa33-22d6-4cc0-bdd2-03ddd9891d86",
   "metadata": {
    "id": "3910aa33-22d6-4cc0-bdd2-03ddd9891d86"
   },
   "source": [
    "#### Transformer Encoder - multiple encoder blocks\n",
    "\n",
    "As introduced in the previous sections, the Transformer encoder is composed of multiple *encoder blocks*. The `TransformerEncoder` class below implements it by stacking $N$ `EncoderBlock`s, where $N$ is the number of stacked encoder blocks.\n",
    "\n",
    "This class inputs the same set of parameters as the `EncoderBlock` class and adds the parameter `num_layers` to specify the number of stacked encoder blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f774df-043f-40f0-9120-ec1d234b4c02",
   "metadata": {
    "id": "38f774df-043f-40f0-9120-ec1d234b4c02",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(hk.Module):\n",
    "    \"\"\"\n",
    "    This class is used to create a transformer encoder.\n",
    "    :param num_layers: The number of encoder blocks.\n",
    "    :param num_heads: The number of attention heads.\n",
    "    :param d_model: The size of the embedding vector.\n",
    "    :param d_ff: The size of the hidden layer.\n",
    "    :param p_dropout: The dropout probability.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_layers, num_heads, d_model, d_ff, p_dropout, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.p_dropout = p_dropout\n",
    "\n",
    "        self.layers = [\n",
    "            EncoderBlock(self.d_model, self.num_heads, self.d_ff, self.p_dropout)\n",
    "            for _ in range(self.num_layers)\n",
    "        ]\n",
    "\n",
    "    def __call__(self, x: typing.List[int], mask=None, is_train=True):\n",
    "        \"\"\"\n",
    "        It applies the transformer encoder to the input sequence.\n",
    "        :param x: The input sequence.\n",
    "        :param mask: The mask to be applied to the self-attention layer.\n",
    "        :param is_train: Whether the model is in training mode.\n",
    "        :return: The final output of the encoder that contains the last encoder block output.\n",
    "        \"\"\"\n",
    "        ###################\n",
    "        # YOUR CODE HERE #\n",
    "        # Steps:\n",
    "        # (i) apply num_layers encoder blocks one after the other, hint: use a for loop over self.layers\n",
    "        ###################\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efebd365-13b2-4567-be37-46703e752353",
   "metadata": {
    "id": "efebd365-13b2-4567-be37-46703e752353",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test the encoder\n",
    "num_layers = 4\n",
    "p_dropout = 0.1\n",
    "def stateful_forward(x, mask, is_train):\n",
    "    enc = TransformerEncoder(num_layers, num_heads, d_model, d_ff, p_dropout, \"t_enc\")\n",
    "    return enc(x, mask, is_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78661d7b-72a3-4c55-83a9-f4492968f964",
   "metadata": {
    "id": "78661d7b-72a3-4c55-83a9-f4492968f964",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model, model_fw, predict, predict_fw, init_params = funcs_from_stateful(stateful_forward, jit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d38971-69aa-4288-82fd-848ebd3a3678",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "02d38971-69aa-4288-82fd-848ebd3a3678",
    "outputId": "c393e6d8-78de-43cf-a01e-898da39697b9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "key, skey = jax.random.split(key)\n",
    "is_train = True\n",
    "params = init_params(key, xx_emb,  jnp.expand_dims(mask, -1), is_train)\n",
    "out = predict(params, key, xx_emb,  jnp.expand_dims(mask, -1), is_train)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bad86f-b78d-42dd-947e-150f7f40da4f",
   "metadata": {
    "id": "03bad86f-b78d-42dd-947e-150f7f40da4f"
   },
   "source": [
    "#### Positional Encoding\n",
    "\n",
    "The Transformer model does not use recurrent or convolutional layers in the encoder/decoder of the model (only attention mechanisms). However, this also has a drawback: since the model has no memory (no recurrent/convolutional layers), it can not take into account the *order* of the sequence elements. The position of words in the sequence is thus not encoded explicitly by the model.\n",
    "\n",
    "As a solution to this issue, the original Transformer model uses a *positional encoding* scheme to represent the position of each element in the sequence. The positional encoding is added to the token embeddings of each element. Following the original paper, positional encodings are generated with multiple sinusoidal functions with varying frequencies.\n",
    "\n",
    "Positional encoding is defined as:\n",
    "\n",
    "$$\\text{PE}(pos, 2i) = \\sin \\left( \\frac{pos}{1000^{2i/d_{\\text{model}}}} \\right)$$\n",
    "$$\\text{PE}(pos, 2i+1) = \\cos \\left( \\frac{pos}{1000^{2i/d_{\\text{model}}}} \\right)$$\n",
    "\n",
    "where $pos$ is the position of the element in the sequence, $d_{\\text{model}}$ is the model's embedding dimension, and $i$ is the index of the position vector. Note that this is not a learned parameter; the values are pre-computed and added to the token embeddings at the beginning of the forward pass.\n",
    "\n",
    "Note that, we can optionally apply dropout to the positional encodings during training, thus providing additional regularization for the model.\n",
    "\n",
    "📚 **Resources**\n",
    "\n",
    "- Detailed explanation with visual aids: [Understanding Positional Encoding in Transformers](https://erdem.pl/2021/05/understanding-positional-encoding-in-transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837a026e-8580-41df-9bd8-c700bf03765a",
   "metadata": {
    "id": "837a026e-8580-41df-9bd8-c700bf03765a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(hk.Module):\n",
    "    \"\"\"\n",
    "    This class is used to add positional encoding to the input sequence.\n",
    "\n",
    "    Args:\n",
    "        d_model (int): The size of the embedding vector.\n",
    "        max_len (int): The maximum length of the input sequence.\n",
    "        p_dropout (float, optional): The dropout probability. Default is 0.1.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, max_len: int, p_dropout: float = 0.1, name=None):\n",
    "        \"\"\"\n",
    "        Initialize PositionalEncoding module.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): The size of the embedding vector.\n",
    "            max_len (int): The maximum length of the input sequence.\n",
    "            p_dropout (float, optional): The dropout probability. Default is 0.1.\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "        self.p_dropout = p_dropout\n",
    "\n",
    "        pe = jnp.zeros((self.max_len, self.d_model))\n",
    "        position = jnp.arange(0, self.max_len, dtype=jnp.float32)[:, None]\n",
    "        div_term = jnp.exp(jnp.arange(0, self.d_model, 2) * (-jnp.log(10000.0) / self.d_model))\n",
    "        pe.at[:, 0::2].set(jnp.sin(position * div_term))\n",
    "        pe.at[:, 1::2].set(jnp.cos(position * div_term))\n",
    "        pe = pe[None]\n",
    "        self.pe = jax.device_put(pe)\n",
    "\n",
    "    def __call__(self, x, is_train=True):\n",
    "        \"\"\"\n",
    "        Apply positional encoding to the input sequence.\n",
    "\n",
    "        Args:\n",
    "            x (jnp.ndarray): The input sequence with shape (batch_size, sequence_length, d_model).\n",
    "            is_train (bool, optional): Whether the model is in training mode. Default is True.\n",
    "\n",
    "        Returns:\n",
    "            jnp.ndarray: The input sequence with positional encoding.\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:, : x.shape[1]]\n",
    "        if is_train:\n",
    "            return hk.dropout(hk.next_rng_key(), self.p_dropout, x)\n",
    "        else:\n",
    "            return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd5e428-eb3a-4414-ba17-cbef2e8c75f3",
   "metadata": {
    "id": "edd5e428-eb3a-4414-ba17-cbef2e8c75f3"
   },
   "source": [
    "## 2.3 Lets Build a transformer model\n",
    "\n",
    "In this section, we will implement the Transformer encoder and apply it to the task of word-level language modeling. We have implemented each base operation in the previous sections, so we will combine all these to train a language model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b4bc96-fa2f-409b-b143-66b01ff405c8",
   "metadata": {
    "id": "53b4bc96-fa2f-409b-b143-66b01ff405c8"
   },
   "source": [
    "###  Combining all together: the Transformer Encoder\n",
    "\n",
    "The Transformer encoder is composed of multiple *encoder blocks*. Each of these blocks comprises two sub-layers: a *multi-head self-attention layer*, and a *feed-forward network*. There is also a residual connection around each sub-layer, followed by *layer normalization*. See the Figure above for a detailed diagram of a single encoder block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8bea29-c277-4c7d-96af-5198f3c68b9e",
   "metadata": {
    "id": "dd8bea29-c277-4c7d-96af-5198f3c68b9e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# some global variables\n",
    "MASK_PROBABILITY = 0.15\n",
    "NUM_LAYERS = 2\n",
    "NUM_HEADS = 2\n",
    "D_MODEL = xx_emb.shape[2]\n",
    "D_FF = 124\n",
    "P_DROPOUT = 0.1\n",
    "MAX_SEQ_LEN = xx_emb.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccf24bf-e3a9-4f1e-bae1-e3eb78db312c",
   "metadata": {
    "id": "6ccf24bf-e3a9-4f1e-bae1-e3eb78db312c"
   },
   "source": [
    "### Let's build the transformer classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b54f38-0a71-4812-999b-f7de1d0d6219",
   "metadata": {
    "id": "57b54f38-0a71-4812-999b-f7de1d0d6219",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def stateful_forward(input_ids, mask, is_train=True):\n",
    "\n",
    "    embed_dim = 32\n",
    "    num_layers = 2\n",
    "    num_heads = 8\n",
    "    d_ff = 32\n",
    "    p_dropout = .1\n",
    "    max_seq_length = 600\n",
    "    vocab_size = 20000\n",
    "    pe = PositionalEncoding(embed_dim, max_seq_length, p_dropout)\n",
    "    embeddings = MaskedEmbedding(vocab_size, embed_dim)\n",
    "    encoder = TransformerEncoder(num_layers, num_heads, embed_dim, d_ff, p_dropout)\n",
    "\n",
    "    out = embeddings(input_ids, mask)\n",
    "    if len(out.shape) == 2:\n",
    "        out = out[None, :, :]\n",
    "\n",
    "\n",
    "    out = pe(out, is_train=is_train)  # (B,S,d_model)\n",
    "    out = encoder(out, mask=jnp.expand_dims(mask, 1), is_train=is_train)\n",
    "\n",
    "    # Apply the first linear layer to the masked sequence\n",
    "    out = hk.Linear(1)(out).squeeze()  # You can adjust the output size (128) to your preference\n",
    "\n",
    "    # Apply global max pooling along the sequence dimension\n",
    "    max_pooled_x = jnp.max(out, axis=-1)  # (BS, H)\n",
    "\n",
    "    # Apply the second linear layer to get the final classification logits\n",
    "    out = hk.Linear(1)(out).squeeze()\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51cf512-55a3-4545-9841-086f75055816",
   "metadata": {
    "id": "d51cf512-55a3-4545-9841-086f75055816",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model, model_fw, predict, predict_fw, init_params = funcs_from_stateful(stateful_forward, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6461044-e16e-4eb4-8d9f-7d89bc251e75",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f6461044-e16e-4eb4-8d9f-7d89bc251e75",
    "outputId": "e40733d4-7aa2-429e-d8c9-8145f714668c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check inference\n",
    "xx = X_tr_enc[:4]\n",
    "mask = X_tr_mask[:4]\n",
    "key, skey = jax.random.split(key)\n",
    "\n",
    "init_and_pred(key, xx, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9d1e27-7c10-431c-bd44-e915e8b2b179",
   "metadata": {
    "id": "8e9d1e27-7c10-431c-bd44-e915e8b2b179",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# init params\n",
    "key, skey = jax.random.split(key)\n",
    "is_train = True\n",
    "batch_size = 8\n",
    "params = init_params(skey, xx, mask, is_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc741f43-7c98-45d0-ab3f-b1fe1496106d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "bc741f43-7c98-45d0-ab3f-b1fe1496106d",
    "outputId": "724c9eb4-e26b-4385-b072-58ee9e9506ad",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train loop\n",
    "# initialize optimizer\n",
    "lr = .001\n",
    "optimizer = optax.adam(learning_rate=lr)\n",
    "opt_state = optimizer.init(params)\n",
    "epochs = 2\n",
    "params = train(params,\n",
    "               skey,\n",
    "               X_tr_enc,\n",
    "               X_tr_mask,\n",
    "               Y_tr,\n",
    "               epochs=epochs,\n",
    "               batch_size=batch_size,\n",
    "               x_te=X_te_enc,\n",
    "               mask_te=X_te_mask,\n",
    "               y_te=Y_te,\n",
    "               batch_encode=None,\n",
    "               eval_every = 1,\n",
    "               loss_every_batch=128)\n",
    "\n",
    "lr = .0001\n",
    "optimizer = optax.adam(learning_rate=lr)\n",
    "opt_state = optimizer.init(params)\n",
    "epochs = 3\n",
    "params = train(params,\n",
    "               skey,\n",
    "               X_tr_enc,\n",
    "               X_tr_mask,\n",
    "               Y_tr,\n",
    "               epochs=epochs,\n",
    "               batch_size=batch_size,\n",
    "               x_te=X_te_enc,\n",
    "               mask_te=X_te_mask,\n",
    "               y_te=Y_te,\n",
    "               batch_encode=None,\n",
    "               eval_every = 1,\n",
    "               loss_every_batch=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f72039",
   "metadata": {},
   "source": [
    "### [ 📝 ] Inspect and Discuss\n",
    "\n",
    "(You may use the same inputs as in the previous Ispection and Discussion section)\n",
    "\n",
    "Play around with the model.Try to find some weaknesses using some inputs either from the training set or using your own inputs.Keep in mind that the model does not treat the input as sequence but as a set of words.Therefore, it does not take into account the order of the words.Can you find a characteristic input that reveals this weakness?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2f8b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_custom_input(test_input):\n",
    "    test_input = np.array([test_input])\n",
    "    test_enc, mask = batch_encode(test_input)\n",
    "    print(model_fw.apply(params, test_enc, None, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664b68a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "# YOUR CODE HERE #\n",
    "# real positive impact\n",
    "test_input = \"I have seen a lot of excellent movies and this one is one of them\"\n",
    "predict_on_custom_input(test_input)\n",
    "\n",
    "test_input = ....\n",
    "predict_on_custom_input(test_input)\n",
    "##################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6cfa42-9cff-4753-84d2-80de04e0ddff",
   "metadata": {
    "id": "ca6cfa42-9cff-4753-84d2-80de04e0ddff"
   },
   "source": [
    "## 2.4 Discussion\n",
    "\n",
    "So are transformers all we need?\n",
    "\n",
    "- How does the accuracy of the transformers model compare to the ones from the previous chapter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b91ee2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
