{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d996dfcd",
   "metadata": {
    "id": "d996dfcd",
    "tags": []
   },
   "source": [
    "Practical 1: Natural Language Processing Tutorial\n",
    "======\n",
    "\n",
    "This is the tutorial of the 2023 [Mediterranean Machine Learning Summer School](https://www.m2lschool.org/) on Natural Language Processing!\n",
    "\n",
    "This tutorial will explore the fundamental aspects of Natural Language Processing (NLP). Whether you are new to NLP or just beginning your journey, there's no need to worry, as the tutorial assumes minimal prior knowledge. Our focus will be on implementing everything from scratch to ensure clarity and understanding. To facilitate this, we will be using [JAX](https://jax.readthedocs.io/en/latest/), a library that offers an API similar to Numpy (and often identical) with the added benefit of automatic differentiation.\n",
    "\n",
    "## Outline\n",
    "\n",
    "- <span style=\"color:blue\">0 Refresher on JAX, Haiku and Optax </span>\n",
    "- <span style=\"color:blue\">1 Introduction to NLP </span>\n",
    "  - <span style=\"color:blue\">1.1 The NLP pipeline </span>\n",
    "  - <span style=\"color:blue\">1.2 Classification pipeline: Multi-hot encoding + MLP model </span>\n",
    "  - <span style=\"color:blue\">1.3 Classification pipeline: Embeddings + Sequential Model </span>\n",
    "- 2 Introduction to the Transformers architecture\n",
    "  - 2.1 Transformer architecture\n",
    "  - 2.2 Implementing the core components\n",
    "  - 2.3 Transformer for classification pipeline\n",
    "- 3 Advanced: Transformers for language translation\n",
    "  - 3.1 The Transformer Decoder\n",
    "  - 3.2 Transformer Decoder for character-based Language Modelling\n",
    "  - 3.3 The full Transformer\n",
    "  - 3.4 Transformer for Neural Machine Translation\n",
    "\n",
    "## Emojis\n",
    "\n",
    "Sections marked as [ üìù ] contain cells with missing code that you should complete,[ &#x1F4C4; ] is used for links to interesting external resources. When we use the words of an external resource we will cite it with &#x1F449; resource &#x1F448;.\n",
    "\n",
    "## Libraries\n",
    "\n",
    "We will keep our promise that (almost &#x1F60B; ) everything will be built from scratch. Indeed, all the vital and challenging components will be developed from zero. In fact the whole tutorial could be done only based on JAX. However, we recognize that certain minor technical details can become distracting if much time spent for them. For these tiny bits, we will ask help from: [haiku](https://dm-haiku.readthedocs.io/en/latest/) to code neural network architectures, [optax](https://optax.readthedocs.io/en/latest/) to find the optimal parameters and [tokenizers](https://huggingface.co/docs/tokenizers/index) to learn the vocabulary in each dataset. And the ubiquitous [numpy](https://numpy.org/) and [pandas](https://pandas.pydata.org/) for tensor handling. That's all!\n",
    "\n",
    "It would be also nice, if you have access to GPU! And hopefully due to google colab you can immediately access a cuda(s)-enabled environment pressing the button below.\n",
    "\n",
    "## Credits\n",
    "\n",
    "The tutorial is created by [Vasilis Gkolemis](https://givasile.github.io/) and [Matko Bo≈°njak](https://matko.info). It is mostly inspired by [Deep Learning with Python (DLP)](https://www.manning.com/books/deep-learning-with-python) the famous book of Francois Chollet, last year's [M2Lschool](https://github.com/M2Lschool/tutorials2022) NLP tutorial (especially for the transformers part) and the [annotated transformer](http://nlp.seas.harvard.edu/annotated-transformer/) presentation.\n",
    "\n",
    "## Note for Colab users\n",
    "\n",
    "To grab a GPU (if available), make sure you go to `Edit -> Notebook settings` and choose a GPU under `Hardware accelerator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ORFKb828ybV6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ORFKb828ybV6",
    "outputId": "3f55017a-0791-49e8-858f-3738b11bf671"
   },
   "outputs": [],
   "source": [
    "!pip install dm-haiku optax tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2ef14f",
   "metadata": {
    "id": "4d2ef14f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import haiku as hk\n",
    "import optax\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tokenizers\n",
    "import os\n",
    "import typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e0917a-60de-4f35-a3b8-154418bb8c45",
   "metadata": {
    "id": "46e0917a-60de-4f35-a3b8-154418bb8c45",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# forces JAX to allocate memory as needed\n",
    "# see https://jax.readthedocs.io/en/latest/gpu_memory_allocation.html\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vwShBP5hL-9l",
   "metadata": {
    "id": "vwShBP5hL-9l"
   },
   "outputs": [],
   "source": [
    "# haiku provides a nice helper for returning a seed generator\n",
    "init_seed = 21\n",
    "key_iter = hk.PRNGSequence(jax.random.PRNGKey(init_seed))\n",
    "key = jax.random.PRNGKey(init_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a173516-4bd2-41d5-9802-bcaa252b580c",
   "metadata": {
    "id": "1a173516-4bd2-41d5-9802-bcaa252b580c",
    "outputId": "4fe8da49-ef81-44ec-ca6a-5ed372058756",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def jax_has_gpu():\n",
    "    try:\n",
    "        _ = jax.device_put(jax.numpy.ones(1), device=jax.devices('gpu')[0])\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "gpu = jax_has_gpu()   # automatically checks for gpus, override if needed.\n",
    "print(\"Is my instance gpu-ready?\", gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NTY20AO8L-9l",
   "metadata": {
    "id": "NTY20AO8L-9l"
   },
   "source": [
    "# Practical 0: Refresher on JAX, Haiku and Optax\n",
    "\n",
    "In Part 0, we will (a) briefly introduce the libraries of the tutorial (JAX, Haiku, Optax) and (b) implement a linear classifier. Apart from getting familiarized with the libraries, we will also implement some crucial accessories (training, evaluation loops) that we will reused throughout the tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41aa70e",
   "metadata": {
    "id": "c41aa70e"
   },
   "source": [
    "## 0.1 Create a linear dataset\n",
    "\n",
    "To remember the key concepts of JAX, Haiku and Optax, we will solve a very simple linear classification problem:\n",
    "\n",
    "$$\n",
    "y = \\begin{cases}\n",
    "1, & \\text{if } wx^T + b > 0 \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where $ w \\in \\mathbb{R}^D $ and $ b \\in \\mathbb{R} $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e484cf36",
   "metadata": {
    "id": "e484cf36",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_dataset(key, n_samples: int, dim: int):\n",
    "    key, skey = jax.random.split(key)\n",
    "    x = jax.random.normal(skey, (n_samples, dim))\n",
    "\n",
    "    # generate ground truth\n",
    "    key, skey1, skey2 = jax.random.split(key, num=3)\n",
    "    w_gt = jax.random.normal(skey1, (dim, 1))\n",
    "    b_gt = jax.random.normal(skey2, (1, 1))\n",
    "    y_gt = (jnp.dot(x, w_gt) + b_gt).squeeze()\n",
    "\n",
    "    # convert to 0, 1\n",
    "    y_gt = (y_gt > 0).astype(jnp.int32)\n",
    "\n",
    "    return x, y_gt, [w_gt, b_gt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f5c47b",
   "metadata": {
    "id": "46f5c47b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# generate dataset\n",
    "n_samples = 10_000  # number of training instances\n",
    "dim = 20  # feature vector dimension\n",
    "x, y_gt, params_gt = generate_dataset(key, n_samples, dim)\n",
    "\n",
    "# split into train and test\n",
    "n_tr = int(n_samples * 0.8)\n",
    "x_tr, y_tr = x[:n_tr, :], y_gt[:n_tr]\n",
    "x_te, y_te = x[n_tr:, :], y_gt[n_tr:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304c7cd2-588a-4fdd-9595-d3e767a94e1b",
   "metadata": {
    "id": "304c7cd2-588a-4fdd-9595-d3e767a94e1b",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 0.2 [ üìù ] Create a linear classifier with Haiku"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6024ba",
   "metadata": {},
   "source": [
    "[ üìù ]Fill in the MyLinear module to create a custom linear layer. The module should define two parameters, `w` and `b`, and perform the forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hxh0xqu2L-9n",
   "metadata": {
    "id": "Hxh0xqu2L-9n"
   },
   "outputs": [],
   "source": [
    "# create a (custom) linear model\n",
    "class MyLinear(hk.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes a custom linear layer module.\n",
    "        \"\"\"\n",
    "        # there is no need to add something here\n",
    "        # except if you want to add some parameters\n",
    "        # in the initialization of the module\n",
    "        super().__init__()\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Performs the forward pass for the custom linear layer.\n",
    "\n",
    "        Args:\n",
    "            x (jnp.ndarray): Input data with shape (batch_size, num_features).\n",
    "\n",
    "        Returns:\n",
    "            jnp.ndarray: The output of the linear layer with shape (batch_size,).\n",
    "        \"\"\"\n",
    "        ##################\n",
    "        # YOUR CODE HERE #\n",
    "        # Hint: https://dm-haiku.readthedocs.io/en/latest/notebooks/basics.html#A-first-example-with-hk.transform\n",
    "        # (i) define the initializers of w and b\n",
    "        # (ii) define w and b as parameters of the module\n",
    "        # (iii) perform the linear projection: y = w*x + b\n",
    "        # (iv) make sure the output y has shape (batch_size,)\n",
    "        # return y\n",
    "        ##################\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ac4a3f",
   "metadata": {},
   "source": [
    "[ üìù ] Define the stateful forward function. The arguments `mask` and `is_train` are not needed for this part, but will be used later in the tutorial. For now, just ignore them. The function should return the output of the custom linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0ef328-0cc2-4af5-a713-adaff1e29bea",
   "metadata": {
    "id": "1a0ef328-0cc2-4af5-a713-adaff1e29bea",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def stateful_forward(x, mask, is_train):\n",
    "    \"\"\"\n",
    "    Perform the forward pass.\n",
    "\n",
    "    Args:\n",
    "        x (jnp.ndarray): Input data with shape (batch_size, num_features).\n",
    "        mask (jnp.ndarray): Mask with shape (batch_size,) representing valid elements in x.\n",
    "        is_train (bool): Flag indicating whether the model is in training mode.\n",
    "\n",
    "    Returns:\n",
    "        jnp.ndarray: The output of the custom linear layer with shape (batch_size,).\n",
    "    \"\"\"\n",
    "    linear = MyLinear()\n",
    "    y = linear(x)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b88c14b",
   "metadata": {},
   "source": [
    "In the following cells, we (a) transform the stateful forward function to a stateless one (as JAX demands), (b) initialize the parameters of the model and (c) perform a forward pass to make sure everything works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31fd91a-49bd-4cd0-8c82-5c954e968b6c",
   "metadata": {
    "id": "b31fd91a-49bd-4cd0-8c82-5c954e968b6c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def funcs_from_stateful(stateful_forward, jit=True):\n",
    "    \"\"\"\n",
    "    Helping function for getting stateless (pure JAX) functions from the stateful_forward function.\n",
    "\n",
    "    Args:\n",
    "        stateful_forward (Callable): The stateful forward function to be transformed.\n",
    "        jit (bool, optional): Whether to jit-compile the functions. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: A tuple containing:\n",
    "        - model: the transformed model with rng\n",
    "        - model_fw: the transformed model without rng\n",
    "        - predict: the predict function with rng\n",
    "        - predict_fw: the predict function without rng\n",
    "        - init_params: the init function for initializing the model parameters\n",
    "    \"\"\"\n",
    "    model = hk.transform(stateful_forward)\n",
    "    model_fw = hk.without_apply_rng(model)\n",
    "\n",
    "    if jit:\n",
    "        predict = jax.jit(model.apply)\n",
    "        predict_fw = jax.jit(model_fw.apply)\n",
    "        init_params = jax.jit(model.init)\n",
    "    else:\n",
    "        predict = model.apply\n",
    "        predict_fw = model_fw.apply\n",
    "        init_params = model.init\n",
    "    return model, model_fw, predict, predict_fw, init_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc12808",
   "metadata": {
    "id": "1fc12808",
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model, model_fw, predict, predict_fw, init_params = funcs_from_stateful(stateful_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ae2996-9e57-420d-83fa-c83809d32a9a",
   "metadata": {
    "id": "86ae2996-9e57-420d-83fa-c83809d32a9a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_and_pred(key, x, mask):\n",
    "    \"\"\"\n",
    "    Tests the underlying model. Initializes and performs inference with 3 ways:\n",
    "    - using predict (with rng) and `is_train=False`\n",
    "    - using predict_fw (without rng), `is_train=False`\n",
    "    - using predict (with rng), `is_train=True`\n",
    "\n",
    "    Args:\n",
    "        key (jax.random.PRNGKey): Random number generator key for initialization.\n",
    "        x (jnp.ndarray): Input data with shape (batch_size, num_features).\n",
    "        mask (jnp.ndarray): Mask with shape (batch_size,) representing valid elements in xx.\n",
    "\n",
    "    Returns:\n",
    "        None: This function does not return any value. It prints the results of the inference.\n",
    "    \"\"\"\n",
    "    key, *skey = jax.random.split(key, 3)\n",
    "    params = init_params(skey[0], x, mask, False)\n",
    "\n",
    "    print(\"Inference with predict (with rng), is_train=False\")\n",
    "    print(predict(params, skey[1], x, mask, False))\n",
    "\n",
    "    print(\"Inference with predict_fw (without rng), is_train=False\")\n",
    "    print(predict_fw(params, x, mask, False))\n",
    "\n",
    "    print(\"Inference with predict (with rng), is_train=True\")\n",
    "    print(predict(params, skey[1], x, mask, True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18eb272c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "18eb272c",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "fe9a7890-5f0b-4897-f4a3-9bb9cdec6f00",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check the model\n",
    "xx = x[0:5, :]\n",
    "mask = None\n",
    "key, skey = jax.random.split(key)\n",
    "init_and_pred(key, xx, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30354b27",
   "metadata": {
    "id": "30354b27"
   },
   "source": [
    "###  [ üìù ] 0.3 Implement training and evaluation Loops\n",
    "\n",
    "In the following cells we will implement five functions for general use:\n",
    "\n",
    "- `get_loss`: computes the loss on a specific batch\n",
    "- `train_step`: implements one training step [ üìù ]\n",
    "- `evaluate_on_batch`: evaluates the model on a specific batch\n",
    "- `evaluate`: evaluates the model on the whole dataset (test set)\n",
    "- `train`: implements the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762d8051-a16a-4750-9432-b6404689653b",
   "metadata": {
    "id": "762d8051-a16a-4750-9432-b6404689653b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# defing the function that computes the loss\n",
    "@jax.jit\n",
    "def get_loss(params, skey, x: jnp.ndarray, mask, y_gt: jnp.ndarray):\n",
    "    \"\"\"\n",
    "    Compute the binary cross-entropy loss for the given input and ground truth.\n",
    "\n",
    "    Args:\n",
    "        params (List): the parameters of the model.\n",
    "        skey (jax.random.PRNGKey): the random key that is used for the forward pass\n",
    "        x (jnp.ndarray): the input\n",
    "        mask (jnp.ndarray): the mask representing valid elements in x\n",
    "        y_gt (jnp.ndarray): the labels\n",
    "\n",
    "    Returns:\n",
    "        jnp.ndarray: The computed loss value.\n",
    "    \"\"\"\n",
    "    # Predict using skey state and is_train\n",
    "    y = predict(params, skey, x, mask, is_train=True)\n",
    "\n",
    "    # Compute the loss value\n",
    "    loss_value = optax.sigmoid_binary_cross_entropy(y, y_gt).mean(axis=-1)\n",
    "    return loss_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c41af62",
   "metadata": {},
   "source": [
    "[ üìù ] Implement the `train_step` function. This function should perform a single training step for the given input batch. It should return the updated parameters, the updated optimizer state, the computed loss value and the updated random number generator key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff7bf77",
   "metadata": {
    "id": "dff7bf77",
    "tags": []
   },
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(params, key, opt_state, x, mask, y_gt):\n",
    "    \"\"\"\n",
    "    Perform a single training step for the given input batch.\n",
    "\n",
    "    Args:\n",
    "        params (List): the parameters of the model.\n",
    "        skey (jax.random.PRNGKey): the random key that is used for the forward pass\n",
    "        opt_state (OptState): The state of the optimizer.\n",
    "        x (jnp.ndarray): the input\n",
    "        mask (jnp.ndarray): the mask representing valid elements in x\n",
    "        y_gt (jnp.ndarray): the labels\n",
    "\n",
    "    Returns:\n",
    "        Tuple: A tuple containing:\n",
    "         - params: the updated parameters\n",
    "         - opt_state: the updated optimizer state\n",
    "         - loss: the computed loss value\n",
    "         - key: the updated random number generator key\n",
    "    \"\"\"\n",
    "    ##################\n",
    "    # YOUR CODE HERE #\n",
    "    # (i) Move the random generator, hint: use jax.random.split\n",
    "    # (ii) Define the `gradients function` wrt the loss function, hint: use jax.value_and_grad\n",
    "    # (iii) Get loss and gradients wrt the input batch, hint: use the gradients function\n",
    "    # (iv) Get weight updates and new optimizer state, based on gradients and previous state, hint: use optimizer.update\n",
    "    # (v) Get new params based on previous params and updates, hint: use optax.apply_updates\n",
    "    # (vi) return params, opt_state, loss, key\n",
    "    ##################\n",
    "    return params, opt_state, loss, key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c5f2c1",
   "metadata": {
    "id": "08c5f2c1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def evaluate_on_batch(params, x: np.ndarray, mask, y_gt: np.ndarray):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a single input batch.\n",
    "\n",
    "    Args:\n",
    "        params (List): the parameters of the model.\n",
    "        x (jnp.ndarray): the input\n",
    "        mask (jnp.ndarray): the mask representing valid elements in x\n",
    "        y_gt (jnp.ndarray): the labels\n",
    "\n",
    "    Returns:\n",
    "        Tuple: A tuple containing the accuracy and confusion matrix.\n",
    "            - accuracy (float): The accuracy of the model's predictions.\n",
    "            - confusion_matrix (jnp.ndarray): The confusion matrix with shape (2, 2).\n",
    "    \"\"\"\n",
    "    y_pred = predict_fw(params, x, mask, is_train=False)\n",
    "    y_pred_binary = (y_pred > 0).astype(int)\n",
    "    accuracy = jnp.mean(y_pred_binary == y_gt)\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    true_positive = jnp.sum(jnp.logical_and(y_pred_binary == 1, y_gt == 1))\n",
    "    false_positive = jnp.sum(jnp.logical_and(y_pred_binary == 1, y_gt == 0))\n",
    "    true_negative = jnp.sum(jnp.logical_and(y_pred_binary == 0, y_gt == 0))\n",
    "    false_negative = jnp.sum(jnp.logical_and(y_pred_binary == 0, y_gt == 1))\n",
    "\n",
    "    confusion_matrix = jnp.array([[true_negative, false_positive],\n",
    "                                  [false_negative, true_positive]])\n",
    "    return accuracy, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7865714f",
   "metadata": {
    "id": "7865714f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(params, x: jnp.ndarray, mask, y_gt: jnp.ndarray, batch_encode, batch_size=32):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the full dataset (x) using batches of batch_size.\n",
    "\n",
    "    Args:\n",
    "        params (List): the parameters of the model.\n",
    "        x (jnp.ndarray): the input\n",
    "        mask (jnp.ndarray): the mask representing valid elements in x\n",
    "        y_gt (jnp.ndarray): the labels\n",
    "        batch_encode (Callable): Function to encode the input data into batches.\n",
    "        batch_size (int, optional): Batch size. Defaults to 32.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: A tuple containing the test accuracy and confusion matrix.\n",
    "            - test_accuracy (float): The accuracy of the model's predictions on the test data.\n",
    "            - cm (jnp.ndarray): The confusion matrix with shape (2, 2).\n",
    "    \"\"\"\n",
    "    cm = jnp.zeros([2, 2])\n",
    "\n",
    "    # Evaluate on the test set with batching\n",
    "    test_accuracy = 0.0\n",
    "    num_batches = int(len(x) / batch_size)\n",
    "    for j in range(num_batches):\n",
    "        start_idx = j * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        x_batch = x[start_idx:end_idx]\n",
    "        y_batch = y_gt[start_idx:end_idx]\n",
    "\n",
    "        if mask is not None:\n",
    "            mask_batch = mask[start_idx:end_idx]\n",
    "        else:\n",
    "            mask_batch = None\n",
    "\n",
    "        if batch_encode is not None:\n",
    "            x_batch = batch_encode(x_batch)\n",
    "\n",
    "        # Move to GPU\n",
    "        if gpu:\n",
    "            x_batch = jnp.array(x_batch)\n",
    "            y_batch = jnp.array(y_batch)\n",
    "            x_batch = jax.device_put(x_batch, jax.devices(\"gpu\")[0])  # Assuming you have only one GPU\n",
    "            y_batch = jax.device_put(y_batch, jax.devices(\"gpu\")[0])  # Assuming you have only one GPU\n",
    "\n",
    "        batch_accuracy, batch_cm = evaluate_on_batch(params, x_batch, mask_batch, y_batch)\n",
    "        test_accuracy += batch_accuracy\n",
    "        cm += batch_cm\n",
    "\n",
    "    test_accuracy /= num_batches\n",
    "    return test_accuracy, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968573cc",
   "metadata": {
    "id": "968573cc",
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(params: list,\n",
    "          key,\n",
    "          x_tr: jnp.ndarray,\n",
    "          mask_tr: jnp.ndarray,\n",
    "          y_tr: jnp.ndarray,\n",
    "          epochs: int,\n",
    "          batch_size: int,\n",
    "          x_te: jnp.ndarray,\n",
    "          mask_te: jnp.ndarray,\n",
    "          y_te: jnp.ndarray,\n",
    "          batch_encode: typing.Union[None, typing.Callable] = None,\n",
    "          eval_every: int = 1,\n",
    "          loss_every_batch: int = 32,\n",
    "          gpu=False):\n",
    "    \"\"\"\n",
    "    Train the model using mini-batch stochastic gradient descent.\n",
    "\n",
    "    Args:\n",
    "        params (List): the parameters of the model.\n",
    "        key (jax.random.PRNGKey): the random key that is used for the forward pass\n",
    "        x_tr (jnp.ndarray): Training data\n",
    "        mask_tr (jnp.ndarray): Mask on training data\n",
    "        y_tr (jnp.ndarray): Training set labels\n",
    "        epochs (int): Number of training epochs.\n",
    "        batch_size (int): Batch size.\n",
    "        x_te (jnp.ndarray): Test data\n",
    "        mask_te (jnp.ndarray): Mask on test data\n",
    "        y_te (jnp.ndarray): Test set labels\n",
    "        batch_encode (Union[None, Callable], optional): Function to encode the input data into batches. Defaults to None.\n",
    "        eval_every (int, optional): Number of epochs between evaluations on train and test sets. Defaults to 1.\n",
    "        loss_every_batch (int, optional): Number of batches between printing the loss value. Set to false for disabling printing. Defaults to 32.\n",
    "        gpu (bool, optional): Whether to use GPU acceleration. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        params: The updated model parameters after training.\n",
    "    \"\"\"\n",
    "    opt_state = optimizer.init(params)\n",
    "    nof_instances = x_tr.shape[0]\n",
    "    for e in range(epochs):\n",
    "        nof_full_batches = nof_instances // batch_size\n",
    "        for i in range(nof_full_batches):\n",
    "\n",
    "            # Get batch\n",
    "            batch_start = i * batch_size\n",
    "            batch_end = (i + 1) * batch_size\n",
    "            x_batch = x_tr[batch_start:batch_end]\n",
    "            y_batch = y_tr[batch_start:batch_end]\n",
    "\n",
    "            if mask_tr is not None:\n",
    "                mask_batch = mask_tr[batch_start:batch_end]\n",
    "            else:\n",
    "                mask_batch = None\n",
    "\n",
    "            # Vectorize if needed\n",
    "            if batch_encode is not None:\n",
    "                x_batch = batch_encode(x_batch)\n",
    "\n",
    "            # Move to GPU\n",
    "            if gpu:\n",
    "                x_batch = jnp.array(x_batch)\n",
    "                y_batch = jnp.array(y_batch)\n",
    "                x_batch = jax.device_put(x_batch, jax.devices(\"gpu\")[0])  # Assuming you have only one GPU\n",
    "                y_batch = jax.device_put(y_batch, jax.devices(\"gpu\")[0])  # Assuming you have only one GPU\n",
    "\n",
    "            params, opt_state, loss, key = train_step(params, key, opt_state, x_batch, mask_batch, y_batch)\n",
    "\n",
    "            if loss_every_batch is not False:\n",
    "                if i % loss_every_batch == 0:\n",
    "                    print(\"Epoch: %d, Step %d/%d, Loss: %.3f\" % (e, i, nof_full_batches, loss))\n",
    "\n",
    "        if eval_every is not False:\n",
    "            if e % eval_every == 0:\n",
    "\n",
    "                # Evaluate on the whole training set\n",
    "                train_accuracy, train_cm = evaluate(params, x_tr, mask_tr, y_tr, batch_encode, batch_size)\n",
    "                print(\"Epoch: %d, Train Accuracy: %.4f\" % (e, train_accuracy))\n",
    "                print(\"Confusion Matrix:\\n\", train_cm)\n",
    "\n",
    "                # Evaluate on the test set\n",
    "                test_accuracy, test_cm = evaluate(params, x_te, mask_te, y_te, batch_encode, batch_size)\n",
    "                print(\"Epoch: %d, Test Accuracy: %.4f\" % (e, test_accuracy))\n",
    "                print(\"Confusion Matrix:\\n\", test_cm)\n",
    "\n",
    "                print(\"\\n\")\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec068fe-caba-44af-9f23-350c1c4e687e",
   "metadata": {
    "id": "4ec068fe-caba-44af-9f23-350c1c4e687e"
   },
   "source": [
    "Now that we have all the components let's train our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81212434",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "81212434",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "4974d4ca-dd81-47c3-cbb8-51531598759f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# init network parameters\n",
    "key, skey = jax.random.split(key)\n",
    "params = init_params(skey, x, None, None)\n",
    "\n",
    "# set-up the parameters\n",
    "epochs = 501\n",
    "lr = 0.01\n",
    "batch_size = 1024\n",
    "eval_every = 500\n",
    "loss_every_batch = False\n",
    "optimizer = optax.sgd(lr)\n",
    "key, skey = jax.random.split(key)\n",
    "\n",
    "# train\n",
    "params = train(params, skey, x_tr, None, y_gt, epochs, batch_size, x_te, None, y_te, None, eval_every, loss_every_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb7ce42-bb78-4593-b1a3-f56df5d86330",
   "metadata": {
    "id": "edb7ce42-bb78-4593-b1a3-f56df5d86330",
    "tags": []
   },
   "source": [
    "# Practical 1: Introduction to NLP\n",
    "\n",
    "In the first part of the tutorial we will introduce the basic NLP concepts.\n",
    "\n",
    "## 1.1 The basic NLP pipeline\n",
    "\n",
    "Nearly all of NLP solultions follow a common high-level pipeline:\n",
    "\n",
    "<img src=\"https://github.com/M2Lschool/tutorials2023/raw/f14d01d28e5d5ed91006e1740b5e5a1e53c30f81/3_nlp/images/NLP-pipeline.png\">\n",
    "\n",
    "Let's briefly discuss them step by step:\n",
    "\n",
    "* **Standardization:** Reduces small variations in text. For example, \"the cat,\" \"The Cat,\" and \"the cat\" become \"the cat\".\n",
    "* **Tokenization**: Splits the text in tokens (small individual entities). For example, `\"Hello my friend!` will be come `[hello, my, friend, !]`\n",
    "* **Indexing**: Converts to tokens into integers (indexes). For example, `[hello, my, friend, !]` will become `[1, 2, 3, 4]`.\n",
    "* **Encoding/Embedding**: Maps indexes into vectors, forming the embedding space. For example, the word \"cat\" can be mapped to the vector `[0.1, 0.2, 0.3, 0.4]`\n",
    "* **ML model**: Predicts the output based on the embedding space. For example, the model can predict the sentiment of a movie review.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b406d865-8edd-4374-bf60-f810750545b9",
   "metadata": {
    "id": "b406d865-8edd-4374-bf60-f810750545b9",
    "tags": []
   },
   "source": [
    "### 1.1.1 Load the IMDB dataset\n",
    "\n",
    "For the rest of Practical 1, we will use the [IMDB](https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz) dataset. The IMDB is a dataset for sentiment analysis, containing 50,000 movie reviews from IMDB users that are labeled as either positive (1) or negative (0). The dataset is divided into 25,000 reviews for training and 25,000 reviews for testing, so the training and testing sets are balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf7305c-80d6-444a-b914-62becf3eb817",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8cf7305c-80d6-444a-b914-62becf3eb817",
    "outputId": "d09779a7-4832-4781-84b3-b04aaf3e0d56",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -xf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39131fe-2a3c-4c98-89f6-3e572a6ed051",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a39131fe-2a3c-4c98-89f6-3e572a6ed051",
    "outputId": "b2dc8ff5-0655-4e7e-8fbb-aa304959b692",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_all_txts(dir):\n",
    "    # all files in dir\n",
    "    files = os.listdir(dir)\n",
    "\n",
    "    sentences = []\n",
    "    for file in files:\n",
    "        filepath = dir + '/' + file\n",
    "        with open(filepath, 'r') as ff:\n",
    "            for line in ff:\n",
    "                line = line.strip()\n",
    "                sentences.append(line)\n",
    "    return sentences\n",
    "\n",
    "def load_dataset(dir):\n",
    "    pos_dir = dir + '/pos'\n",
    "    neg_dir = dir + '/neg'\n",
    "\n",
    "    # read all txts in dir\n",
    "    pos_sentences = read_all_txts(pos_dir)\n",
    "    neg_sentences = read_all_txts(neg_dir)\n",
    "\n",
    "    # to a dataframe\n",
    "    df_pos = pd.DataFrame({'text': pos_sentences, 'label': 1})\n",
    "    df_neg = pd.DataFrame({'text': neg_sentences, 'label': 0})\n",
    "\n",
    "    df = pd.concat([df_pos, df_neg])\n",
    "\n",
    "    # shuffle\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "dir = 'aclImdb/train/'\n",
    "df_tr = load_dataset(dir)\n",
    "dir = 'aclImdb/test/'\n",
    "df_te = load_dataset(dir)\n",
    "\n",
    "df_tr = df_tr.dropna()\n",
    "df_te = df_te.dropna()\n",
    "\n",
    "X_tr = df_tr.iloc[:, 0].to_numpy()\n",
    "Y_tr = df_tr.iloc[:, 1].to_numpy()\n",
    "\n",
    "X_te = df_te.iloc[:, 0].to_numpy()\n",
    "Y_te = df_te.iloc[:, 1].to_numpy()\n",
    "\n",
    "print(\"The are %d training examples, where %d are positive and %d are negative reviews\" % (df_tr.shape[0], df_tr.loc[df_tr[\"label\"] == 1, :].shape[0], df_tr.loc[df_tr[\"label\"] == 1, :].shape[0]))\n",
    "print(\"The are %d testing examples, where %d are positive and %d are negative reviews\" % (df_te.shape[0], df_te.loc[df_te[\"label\"] == 1, :].shape[0], df_te.loc[df_te[\"label\"] == 1, :].shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5340865c-78a5-4522-a771-59a15c976a28",
   "metadata": {
    "id": "5340865c-78a5-4522-a771-59a15c976a28"
   },
   "source": [
    "### 1.1.2 Tokenization\n",
    "\n",
    "We will use the [HuggingFace tokenizers](https://huggingface.co/docs/tokenizers/index), a fast and efficient library for tokenizing text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918ce8f3-ff58-408b-be8f-c7b1497034f8",
   "metadata": {
    "id": "918ce8f3-ff58-408b-be8f-c7b1497034f8",
    "outputId": "47c1ba30-21bd-4e2e-8220-35daf4f00173",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define the tokenizer\n",
    "tokenizer = tokenizers.Tokenizer(tokenizers.models.Unigram())\n",
    "tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.Whitespace()\n",
    "tokenizer.normalizer = tokenizers.normalizers.Lowercase()\n",
    "\n",
    "# say how it will be trained, to learn the vocabulary\n",
    "vocab_size = 20000\n",
    "special_tokens = [\"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\",]\n",
    "unk_token = \"[UNK]\"\n",
    "max_piece_length = 16\n",
    "trainer = tokenizers.trainers.UnigramTrainer(\n",
    "    special_tokens=special_tokens, # special tokens\n",
    "    vocab_size=vocab_size, # vocabulary size\n",
    "    unk_token=unk_token, # set the unknown token\n",
    "    show_progress=True, # show progress\n",
    ")\n",
    "\n",
    "# train the tokenizer\n",
    "tokenizer.train_from_iterator(X_tr, trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dc77f7",
   "metadata": {},
   "source": [
    "Let's check the output of the tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0619bbe-ed27-4213-85d5-6e2de1993aa3",
   "metadata": {
    "id": "b0619bbe-ed27-4213-85d5-6e2de1993aa3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# inspect the tokenizer\n",
    "def inspect_tokenizer(inp):\n",
    "    tok_example = tokenizer.encode(inp)\n",
    "    print(\"tokens:\", tok_example.tokens)\n",
    "    print(\"ids:\", tok_example.ids)\n",
    "    print(\"attention mask\", tok_example.attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e81018-bae7-44bb-a5fd-96c20fa8b62f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "05e81018-bae7-44bb-a5fd-96c20fa8b62f",
    "outputId": "08e47860-1c79-4338-cc5a-5c2688602ba0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "inp = \"Hello my friend!\"\n",
    "inp = inspect_tokenizer(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b446bae4-7dcf-476f-b1aa-9667e6812da0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "b446bae4-7dcf-476f-b1aa-9667e6812da0",
    "outputId": "1d2df590-0cdc-49f9-8d8d-544107ff66ef",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# now see the output of the tokenizer\n",
    "tokenizer.decode(tokenizer.encode(\"Hello my friend!\").ids) # reconstruct the sentence from the ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d1ce01-7972-4f93-a55e-50f4068a8e02",
   "metadata": {
    "id": "01d1ce01-7972-4f93-a55e-50f4068a8e02"
   },
   "source": [
    "### 1.1.4 ML Model\n",
    "\n",
    "In the following cells, we will test two approaches; first, we will treat the input text as a **set of words** and later, as a **sequence of words.** In the set of words case, we will use **multi-hot encoding plus an MLP classifier**. In sequence of words **learnable embedding plus a recurrent neural network (LSTM)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222e450b-837a-4194-9406-ddb04d603507",
   "metadata": {
    "id": "222e450b-837a-4194-9406-ddb04d603507"
   },
   "source": [
    "## 1.2 Classification treating text as a Set of words\n",
    "\n",
    "Let's first experiment with the simplest approach: multi-hot encoding + a Fully-Connected NN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450335ff-b1e5-4c8e-8d47-969e65d41293",
   "metadata": {
    "id": "450335ff-b1e5-4c8e-8d47-969e65d41293"
   },
   "source": [
    "### Multi-hot encode the input\n",
    "\n",
    "We need to transform the input text into a multi-hot encoding. First, we will use the tokenizer (defined above) to transform the input text into a list of integers, where each integer represents a word in the vocabulary. Then, we will use the list of integers to create a multi-hot encoding of the input text. The multi-hot encoding is a binary vector with length VOCAB_SIZE, where the $i$-th element indicates whether the $i$-th word of the vocabulary exists in the input. If a word is present, its corresponding element in the tensor is set to 1; otherwise, it is set to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4543ed1-d82b-4b90-bba8-197f41859ba1",
   "metadata": {
    "id": "d4543ed1-d82b-4b90-bba8-197f41859ba1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def batch_encode(X):\n",
    "    \"\"\"\n",
    "    Encode a batch of text data into a multi-hot encoding.\n",
    "\n",
    "    Args:\n",
    "        X (List[str]): A list of input texts.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The multi-hot encoding of the input texts with shape (num_samples, vocab_size).\n",
    "    \"\"\"\n",
    "    enc_list = tokenizer.encode_batch(X)\n",
    "    vocab_size = tokenizer.get_vocab_size()\n",
    "    multi_hot_encoding = np.zeros([len(enc_list), vocab_size])\n",
    "    for i, enc in enumerate(enc_list):\n",
    "        multi_hot_encoding[i, enc.ids] = 1\n",
    "    return multi_hot_encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6694de81",
   "metadata": {},
   "source": [
    "Check the shape of the ouput tensor. It should be (32, 20000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d55103-d430-4a5a-a7d6-da4b793288e0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c9d55103-d430-4a5a-a7d6-da4b793288e0",
    "outputId": "9829e94c-52e5-47de-ff65-43b9a0f1b724",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# inspect the batch\n",
    "batch_size = 32\n",
    "inp_list = X_tr[:batch_size]\n",
    "X_batch = batch_encode(inp_list)\n",
    "print(\"The shape of the batch is:\", X_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uu9MUJ9PL-91",
   "metadata": {
    "id": "uu9MUJ9PL-91"
   },
   "source": [
    "### [ üìù ]  Define the MLP\n",
    "\n",
    "We will use a simple MLP with one hidden layer of 16 units (relu acivation) followed by a dropout layer. The output layer has 1 unit. Implement the model below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8cb52b-120c-44fb-8747-2b2adf8a7bd8",
   "metadata": {
    "id": "8c8cb52b-120c-44fb-8747-2b2adf8a7bd8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def stateful_forward(x, mask, is_train):\n",
    "    \"\"\"\n",
    "    A Neural Network.\n",
    "\n",
    "    Args:\n",
    "        x (jnp.ndarray): Input data with shape (batch_size, num_features).\n",
    "        mask (jnp.ndarray): Mask with shape (batch_size,) representing valid elements in x.\n",
    "        is_train (bool): Whether the model is in training mode or not.\n",
    "\n",
    "    Returns:\n",
    "        jnp.ndarray: The output of the model with shape (batch_size,).\n",
    "    \"\"\"\n",
    "    ##################\n",
    "    # Implement a neural network with\n",
    "    # (i) a linear layer with 16 units and relu activation\n",
    "    # (ii) dropout with 0.5 probability if in training mode\n",
    "    # (iii) final linear layer\n",
    "    ##################\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f032d133-c5e6-4900-85dc-76d884edb70d",
   "metadata": {
    "id": "f032d133-c5e6-4900-85dc-76d884edb70d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model, model_fw, predict, predict_fw, init_params = funcs_from_stateful(stateful_forward, jit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2f1041-a8d5-4e9c-938a-0f322e133d25",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fc2f1041-a8d5-4e9c-938a-0f322e133d25",
    "outputId": "e81353d8-dabf-4607-d07b-9e29cd00c70f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# init parameters and check forward pass\n",
    "xx = X_batch[:2]\n",
    "mask = None\n",
    "key, skey = jax.random.split(key)\n",
    "\n",
    "init_and_pred(key, xx, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddde575b",
   "metadata": {
    "id": "ddde575b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize optimizer\n",
    "lr = 0.001\n",
    "optimizer = optax.adam(learning_rate=lr)\n",
    "opt_state = optimizer.init(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d59881-dc15-4710-a5e5-256cfe87bc2c",
   "metadata": {
    "id": "e8d59881-dc15-4710-a5e5-256cfe87bc2c"
   },
   "source": [
    "### Train the model\n",
    "\n",
    "In this section we will train the model for 5 epochs, using a batch size of 124. There is no need to create a training loop, as we have already implemented it above. Therefore, we will simply call the `train` function with the appropriate arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a5f01d-89d5-45ff-9a8e-679182bcd777",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "54a5f01d-89d5-45ff-9a8e-679182bcd777",
    "outputId": "bc651bed-35fa-4fa2-c89f-58d28341ad40",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train\n",
    "epochs = 3\n",
    "batch_size = 124\n",
    "is_train=True\n",
    "key, skey = jax.random.split(key)\n",
    "\n",
    "# init params\n",
    "params = init_params(skey, xx, mask, is_train)\n",
    "\n",
    "# train loop\n",
    "params = train(params, skey, X_tr, None, Y_tr, epochs, batch_size, X_te, None, Y_te, batch_encode, loss_every_batch=25, gpu=gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38840b00-845e-4db9-b6e5-66786e7d3161",
   "metadata": {
    "id": "38840b00-845e-4db9-b6e5-66786e7d3161"
   },
   "outputs": [],
   "source": [
    "def inspect_an_input(i):\n",
    "    inp = batch_encode(X_te[ii:ii+1])\n",
    "    y_pr_score = model_fw.apply(params, inp, None, False)\n",
    "    y_pr = jnp.greater(y_pr_score, 0).astype(int)\n",
    "\n",
    "    print(\"input:\", X_te[ii])\n",
    "    print(\"\\n\")\n",
    "    print(\"ground truth: %d\" % Y_te[ii])\n",
    "    print(\"\\n\")\n",
    "    print(\"prediction  : %d\" % y_pr)\n",
    "    print(\"\\n\")\n",
    "    print(\"prediction score: %f\" % y_pr_score)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80877422",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "80877422",
    "outputId": "a90ce807-4ef7-4ab7-8d61-f8fa3f19daa2"
   },
   "outputs": [],
   "source": [
    "ii = 10\n",
    "inspect_an_input(ii)\n",
    "\n",
    "ii = 12\n",
    "inspect_an_input(ii)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416da2cc-7660-4624-be66-e2f56e5bfbc3",
   "metadata": {
    "id": "416da2cc-7660-4624-be66-e2f56e5bfbc3"
   },
   "source": [
    "### [ üìù ] Inspect and Discuss\n",
    "\n",
    "Play around with the model. Try to find some weaknesses using some inputs either from the training set or using your own inputs. Keep in mind that the model does not treat the input as sequence but as a set of words. Therefore, it does not take into account the order of the words. Can you find a characteristic input that reveals this weakness?\n",
    "\n",
    "If we were to ask ChatGPT to classify the same inputs, would it have the same weaknesses? Let's try it.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7be8ec",
   "metadata": {
    "id": "ea7be8ec",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_on_custom_input(test_input):\n",
    "    test_input = np.array([test_input])\n",
    "    test_enc = batch_encode(test_input)\n",
    "    print(model_fw.apply(params, test_enc, None, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffabf9aa-c3c2-44e0-8aa9-be592bb337ff",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ffabf9aa-c3c2-44e0-8aa9-be592bb337ff",
    "outputId": "531850c1-611c-4bb0-8998-6e9ee0f9f3b9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "##################\n",
    "# real positive impact\n",
    "test_input = \"I have seen a lot of excellent movies and this one is one of them\"\n",
    "predict_on_custom_input(test_input)\n",
    "\n",
    "# YOUR CODE HERE #\n",
    "# try other inputs\n",
    "test_input = ...\n",
    "predict_on_custom_input(test_input)\n",
    "##################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZaDyc7zXL-92",
   "metadata": {
    "id": "ZaDyc7zXL-92"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86ba0022-9651-444d-84d2-d520d1e29d80",
   "metadata": {
    "id": "86ba0022-9651-444d-84d2-d520d1e29d80"
   },
   "source": [
    "## 1.3 Classification treating the input text as a Sequence\n",
    "\n",
    "In this part, we will treat the input text as a sequence of words, using a learnable embedding plus an LSTM network.\n",
    "The network will have (a) an embedding layer (b) an LSTM and a (c) Dense layer\n",
    "Before we code the models, let's set up appropriatelly the lenght of the sequence returned by the tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5ed18a-f75c-4f48-9000-55471adbf85b",
   "metadata": {
    "id": "7e5ed18a-f75c-4f48-9000-55471adbf85b"
   },
   "source": [
    "### [ üìù ]  Define Tokenizer\n",
    "\n",
    "Since we will experiment with sequential models, we want the tokenizer's output to be a tensor of shape `BS, SEQUENCE_LENGTH)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c4739e-91b9-47ac-b51a-1de36dc21400",
   "metadata": {
    "id": "b4c4739e-91b9-47ac-b51a-1de36dc21400",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# inspect what happens if setting the min and max sequence lengths to 10\n",
    "sequence_length = 10\n",
    "truncation_length = 10\n",
    "tokenizer.enable_padding(length=sequence_length)\n",
    "tokenizer.enable_truncation(max_length=truncation_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940ee3d8-f76e-4475-8d2b-3e963d523cf3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "940ee3d8-f76e-4475-8d2b-3e963d523cf3",
    "outputId": "e573d6bd-5145-4e31-aa2a-bc3680f18406",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# inspect the tokenizer\n",
    "inp = \"Hello my friend!\"\n",
    "inspect_tokenizer(inp)\n",
    "\n",
    "inp = \"Hello my dear dear dear dear dear dear dear dearest friend!\"\n",
    "inspect_tokenizer(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2334a6-2716-4622-a30f-2f7e446d690e",
   "metadata": {
    "id": "2f2334a6-2716-4622-a30f-2f7e446d690e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# now let's set it to a normal value\n",
    "sequence_length = 600\n",
    "truncation_length = 600\n",
    "tokenizer.enable_padding(length=sequence_length)\n",
    "tokenizer.enable_truncation(max_length=truncation_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0853305a-0d5f-47e7-a066-be09624485b7",
   "metadata": {
    "id": "0853305a-0d5f-47e7-a066-be09624485b7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def batch_encode(X):\n",
    "    enc_list = tokenizer.encode_batch(X)\n",
    "    enc_list_2 = []\n",
    "    mask_list = []\n",
    "    for i, enc in enumerate(enc_list):\n",
    "        enc_list_2.append(enc.ids)\n",
    "        mask_list.append(enc.attention_mask)\n",
    "    return np.array(enc_list_2), np.array(mask_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69579c47-46cd-4c8c-97fa-ef557b480735",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "69579c47-46cd-4c8c-97fa-ef557b480735",
    "outputId": "78d818c1-c6a6-4523-fb0d-b284cb5541d9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# but since it is not memory-consuming, we can use the whole dataset\n",
    "X_tr_enc, X_tr_mask = batch_encode(X_tr)\n",
    "X_te_enc, X_te_mask = batch_encode(X_te)\n",
    "\n",
    "print(X_tr_enc.shape)\n",
    "print(X_te_enc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e8433e-cba3-43d0-af70-f2f34054d34a",
   "metadata": {
    "id": "99e8433e-cba3-43d0-af70-f2f34054d34a",
    "tags": []
   },
   "source": [
    " Please define the network in the next cell. It must have:\n",
    "\n",
    " - an embedding of shape `(BS, S, H)`\n",
    " - an LSTM to capture the sequential information\n",
    " - use all hidden states of the LSTM for obtaining the final prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9afcc57-4235-4bd3-bf11-3a88f56f8d12",
   "metadata": {
    "id": "e9afcc57-4235-4bd3-bf11-3a88f56f8d12",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def stateful_forward(x, mask, is_train):\n",
    "    \"\"\"\n",
    "    Implement the forward pass of the stateful model using a DeepRNN with LSTM layers.\n",
    "\n",
    "    Args:\n",
    "        x (jnp.ndarray): Input data with shape (batch_size, sequence_length).\n",
    "        mask (jnp.ndarray): Mask with shape (batch_size, sequence_length) representing valid elements in x.\n",
    "        is_train (bool): Whether the model is in training mode or not.\n",
    "\n",
    "    Returns:\n",
    "        jnp.ndarray: The final predictions with shape (batch_size,).\n",
    "    \"\"\"\n",
    "    ##################\n",
    "    # YOUR CODE HERE #\n",
    "    # (i) Embed the input to a tensor (BS, S, 128)\n",
    "    # (ii) Unroll an LSTM over the embedded sequence, use a linear layer to map each LSTM output to a single value\n",
    "    # (iii) use the single values of all intermediate steps of the LSTM as input to a linear layer to get y\n",
    "    # (iv) return y\n",
    "    ##################\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f0f98a-3103-44df-97d4-b31887305cbb",
   "metadata": {
    "id": "f3f0f98a-3103-44df-97d4-b31887305cbb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model, model_fw, predict, predict_fw, init_params = funcs_from_stateful(stateful_forward, jit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e96c013-2463-4a80-971a-bd4d33cd168d",
   "metadata": {
    "id": "5e96c013-2463-4a80-971a-bd4d33cd168d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# init parameters and check forward pass\n",
    "xx = X_tr_enc[:2]\n",
    "mask = X_tr_mask[:2]\n",
    "key, skey = jax.random.split(key)\n",
    "\n",
    "# init_and_pred(key, xx, mask)\n",
    "params = init_params(key, xx, mask, is_train=False)\n",
    "out = predict(params, key, xx, mask, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b310bc57-8e03-442b-bac0-d95b10dd0bb2",
   "metadata": {
    "id": "b310bc57-8e03-442b-bac0-d95b10dd0bb2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize optimizer\n",
    "lr = 0.01\n",
    "optimizer = optax.adam(learning_rate=lr)\n",
    "opt_state = optimizer.init(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6e434b-af5b-4bdf-8300-01ef7a4757b4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dc6e434b-af5b-4bdf-8300-01ef7a4757b4",
    "outputId": "00f5f606-86fb-4e22-8795-c7f6941805e4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train\n",
    "epochs = 3\n",
    "batch_size = 128\n",
    "key, skey = jax.random.split(key)\n",
    "\n",
    "# init params\n",
    "params = init_params(skey, xx, mask, is_train)\n",
    "\n",
    "# train loop\n",
    "params = train(params,\n",
    "               skey,\n",
    "               X_tr_enc,\n",
    "               X_tr_mask,\n",
    "               Y_tr,\n",
    "               epochs=epochs,\n",
    "               batch_size=batch_size,\n",
    "               x_te=X_te_enc,\n",
    "               mask_te=X_te_mask,\n",
    "               y_te=Y_te,\n",
    "               batch_encode=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e814e5f",
   "metadata": {},
   "source": [
    "### [ üìù ] Inspect and Discuss\n",
    "\n",
    "(You may use the same inputs as in the previous Ispection and Discussion section)\n",
    "\n",
    "Play around with the model. Try to find some weaknesses using some inputs either from the training set or using your own inputs. Keep in mind that the model does not treat the input as sequence but as a set of words. Therefore, it does not take into account the order of the words. Can you find a characteristic input that reveals this weakness?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2d67fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_custom_input(test_input):\n",
    "    test_input = np.array([test_input])\n",
    "    test_enc, mask = batch_encode(test_input)\n",
    "    print(model_fw.apply(params, test_enc, None, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9d2cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "# real positive impact\n",
    "test_input = \"I have seen a lot of excellent movies and this one is one of them\"\n",
    "predict_on_custom_input(test_input)\n",
    "\n",
    "# YOUR CODE HERE #\n",
    "# try other inputs\n",
    "test_input = ...\n",
    "predict_on_custom_input(test_input)\n",
    "##################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9287ce-7787-4d04-9ee3-7597328a1c0a",
   "metadata": {
    "id": "bc9287ce-7787-4d04-9ee3-7597328a1c0a"
   },
   "source": [
    "## 1.4 Discussion\n",
    "\n",
    "- Which model had the best performance? How do you explain that?\n",
    "- Among the sequence-based models which one had the best performance? How would you explain that?\n",
    "- Take the bag of words model and the best sequence-based model, and try one or two custom inputs. Do you see any differences in the results? Could you explain that somehow?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PZ8TXzow3EWB",
   "metadata": {
    "id": "PZ8TXzow3EWB"
   },
   "source": [
    "### [ üìù ] Advanced: Open-end Exercise\n",
    "\n",
    "Try to design a better model. You can go with a bag-of-words based approach or a sequence-based model. Your idea can be an incremental improvement on the previous models, e.g. stack more LSTM layers, increase the dimension of the hidden states, or something completely new, i.e. try GRU layer instead of the LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f339e9b-0171-4ded-85e3-07bcfef0cc09",
   "metadata": {
    "id": "3f339e9b-0171-4ded-85e3-07bcfef0cc09",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def stateful_forward(x, mask, is_train):\n",
    "#     ##################\n",
    "#     # YOUR CODE HERE #\n",
    "#     ##################\n",
    "#     return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf8cde1-347d-4b1c-ac42-ead530ba2ab3",
   "metadata": {
    "id": "caf8cde1-347d-4b1c-ac42-ead530ba2ab3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model, model_fw, predict, predict_fw, init_params = funcs_from_stateful(stateful_forward, jit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b03aaa-e977-40b0-8477-c12c53f3af19",
   "metadata": {
    "id": "d1b03aaa-e977-40b0-8477-c12c53f3af19",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # init parameters and check forward pass\n",
    "# xx = X_tr_enc[:2]\n",
    "# mask = X_tr_mask[:2]\n",
    "# key, skey = jax.random.split(key)\n",
    "\n",
    "# # init_and_pred(key, xx, mask)\n",
    "# params = init_params(key, xx, mask, is_train=False)\n",
    "# out = predict(params, key, xx, mask, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37432602-c903-4ff8-a227-8f32751d84f5",
   "metadata": {
    "id": "37432602-c903-4ff8-a227-8f32751d84f5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # initialize optimizer\n",
    "# optimizer = # complete optimizer\n",
    "# opt_state = optimizer.init(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99050b18-b456-4586-886c-9f4d74bf0166",
   "metadata": {
    "id": "99050b18-b456-4586-886c-9f4d74bf0166",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # train\n",
    "# epochs = #\n",
    "# batch_size = #\n",
    "# key, skey = jax.random.split(key)\n",
    "\n",
    "# # init params\n",
    "# params = init_params(skey, xx, mask, is_train)\n",
    "\n",
    "# # train loop\n",
    "# params = train(params,\n",
    "#                skey,\n",
    "#                X_tr_enc,\n",
    "#                X_tr_mask,\n",
    "#                Y_tr,\n",
    "#                epochs=3,\n",
    "#                batch_size=128,\n",
    "#                x_te=X_te_enc,\n",
    "#                mask_te=X_te_mask,\n",
    "#                y_te=Y_te,\n",
    "#                batch_encode=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rx0IMJ9zL-98",
   "metadata": {
    "id": "rx0IMJ9zL-98"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nIY714z5L-98",
   "metadata": {
    "id": "nIY714z5L-98"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
