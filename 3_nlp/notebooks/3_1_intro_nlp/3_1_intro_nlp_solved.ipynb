{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d996dfcd",
   "metadata": {
    "id": "d996dfcd",
    "tags": []
   },
   "source": [
    "Natural Language Processing Tutorial\n",
    "======\n",
    "\n",
    "This is the tutorial of the 2023 [Mediterranean Machine Learning Summer School](https://www.m2lschool.org/) on Natural Language Processing!\n",
    "\n",
    "This tutorial will explore the fundamental aspects of Natural Language Processing (NLP). Whether you are new to NLP or just beginning your journey, there's no need to worry, as the tutorial assumes minimal prior knowledge. Our focus will be on implementing everything from scratch to ensure clarity and understanding. To facilitate this, we will be using [JAX](https://jax.readthedocs.io/en/latest/), a library that offers an API similar to Numpy (and often identical) with the added benefit of automatic differentiation.\n",
    "\n",
    "## Outline\n",
    "\n",
    "- <span style=\"color:blue\">0 Refresher on JAX, Haiku and Optax </span>\n",
    "- <span style=\"color:blue\">1 Introduction to NLP </span>\n",
    "  - <span style=\"color:blue\">1.1 The NLP pipeline </span>\n",
    "  - <span style=\"color:blue\">1.2 Classification pipeline: Multi-hot encoding + MLP model </span>\n",
    "  - <span style=\"color:blue\">1.3 Classification pipeline: Embeddings + Sequential Model </span>\n",
    "- 2 Introduction to the Transformers architecture\n",
    "  - 2.1 Transformer architecture\n",
    "  - 2.2 Implementing the core components\n",
    "  - 2.3 Transformer for classification pipeline\n",
    "- 3 Advanced: Transformers for language translation\n",
    "  - 3.1 The Transformer Decoder\n",
    "  - 3.2 Transformer Decoder for character-based Language Modelling\n",
    "  - 3.3 The full Transformer\n",
    "  - 3.4 Transformer for Neural Machine Translation\n",
    "\n",
    "## Emojis\n",
    "\n",
    "Sections marked as [ 📝 ] contain cells with missing code that you should complete,[ &#x1F4C4; ] is used for links to interesting external resources. When we use the words of an external resource we will cite it with &#x1F449; resource &#x1F448;.\n",
    "\n",
    "## Libraries\n",
    "\n",
    "We will keep our promise that (almost &#x1F60B; ) everything will be built from scratch. Indeed, all the vital and challenging components will be developed from zero. In fact the whole tutorial could be done only based on JAX. However, we recognize that certain minor technical details can become distracting if much time spent for them. For these tiny bits, we will ask help from: [haiku](https://dm-haiku.readthedocs.io/en/latest/) to code neural network architectures, [optax](https://optax.readthedocs.io/en/latest/) to find the optimal parameters and [tokenizers](https://huggingface.co/docs/tokenizers/index) to learn the vocabulary in each dataset. And the ubiquitous [numpy](https://numpy.org/) and [pandas](https://pandas.pydata.org/) for tensor handling. That's all!\n",
    "\n",
    "It would be also nice, if you have access to GPU! And hopefully due to google colab you can immediately access a cuda(s)-enabled environment pressing the button below.\n",
    "\n",
    "## Credits\n",
    "\n",
    "The tutorial is created by [Vasilis Gkolemis](https://givasile.github.io/) and [Matko Bošnjak](https://matko.info). It is mostly inspired by [Deep Learning with Python (DLP)](https://www.manning.com/books/deep-learning-with-python) the famous book of Francois Chollet, last year's [M2Lschool](https://github.com/M2Lschool/tutorials2022) NLP tutorial (especially for the transformers part) and the [annotated transformer](http://nlp.seas.harvard.edu/annotated-transformer/) presentation.\n",
    "\n",
    "## Note for Colab users\n",
    "\n",
    "To grab a GPU (if available), make sure you go to `Edit -> Notebook settings` and choose a GPU under `Hardware accelerator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ORFKb828ybV6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T06:59:26.794887016Z",
     "start_time": "2023-08-30T06:59:23.466571432Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ORFKb828ybV6",
    "outputId": "3f55017a-0791-49e8-858f-3738b11bf671"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dm-haiku in /home/givasile/miniconda3/envs/jax-cuda-11-8/lib/python3.11/site-packages (0.0.10)\r\n",
      "Requirement already satisfied: optax in /home/givasile/miniconda3/envs/jax-cuda-11-8/lib/python3.11/site-packages (0.1.5)\r\n",
      "Requirement already satisfied: tokenizers in /home/givasile/miniconda3/envs/jax-cuda-11-8/lib/python3.11/site-packages (0.13.3)\r\n",
      "Requirement already satisfied: absl-py>=0.7.1 in /home/givasile/miniconda3/envs/jax-cuda-11-8/lib/python3.11/site-packages (from dm-haiku) (1.4.0)\r\n",
      "Requirement already satisfied: jmp>=0.0.2 in /home/givasile/miniconda3/envs/jax-cuda-11-8/lib/python3.11/site-packages (from dm-haiku) (0.0.4)\r\n",
      "Requirement already satisfied: numpy>=1.18.0 in /home/givasile/miniconda3/envs/jax-cuda-11-8/lib/python3.11/site-packages (from dm-haiku) (1.25.0)\r\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /home/givasile/miniconda3/envs/jax-cuda-11-8/lib/python3.11/site-packages (from dm-haiku) (0.9.0)\r\n",
      "Requirement already satisfied: chex>=0.1.5 in /home/givasile/miniconda3/envs/jax-cuda-11-8/lib/python3.11/site-packages (from optax) (0.1.81)\r\n",
      "Requirement already satisfied: jax>=0.1.55 in /home/givasile/miniconda3/envs/jax-cuda-11-8/lib/python3.11/site-packages (from optax) (0.4.13)\r\n",
      "Requirement already satisfied: jaxlib>=0.1.37 in /home/givasile/miniconda3/envs/jax-cuda-11-8/lib/python3.11/site-packages (from optax) (0.4.13+cuda11.cudnn86)\r\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/givasile/miniconda3/envs/jax-cuda-11-8/lib/python3.11/site-packages (from chex>=0.1.5->optax) (4.6.3)\r\n",
      "Requirement already satisfied: dm-tree>=0.1.5 in /home/givasile/miniconda3/envs/jax-cuda-11-8/lib/python3.11/site-packages (from chex>=0.1.5->optax) (0.1.8)\r\n",
      "Requirement already satisfied: toolz>=0.9.0 in /home/givasile/miniconda3/envs/jax-cuda-11-8/lib/python3.11/site-packages (from chex>=0.1.5->optax) (0.12.0)\r\n",
      "Requirement already satisfied: ml-dtypes>=0.1.0 in /home/givasile/miniconda3/envs/jax-cuda-11-8/lib/python3.11/site-packages (from jax>=0.1.55->optax) (0.2.0)\r\n",
      "Requirement already satisfied: opt-einsum in /home/givasile/miniconda3/envs/jax-cuda-11-8/lib/python3.11/site-packages (from jax>=0.1.55->optax) (3.3.0)\r\n",
      "Requirement already satisfied: scipy>=1.7 in /home/givasile/miniconda3/envs/jax-cuda-11-8/lib/python3.11/site-packages (from jax>=0.1.55->optax) (1.10.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install dm-haiku optax tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d2ef14f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T06:59:28.088984770Z",
     "start_time": "2023-08-30T06:59:26.800331938Z"
    },
    "id": "4d2ef14f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import haiku as hk\n",
    "import optax\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tokenizers\n",
    "import os\n",
    "import typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46e0917a-60de-4f35-a3b8-154418bb8c45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T06:59:28.092677876Z",
     "start_time": "2023-08-30T06:59:28.091022390Z"
    },
    "id": "46e0917a-60de-4f35-a3b8-154418bb8c45",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# forces JAX to allocate memory as needed\n",
    "# see https://jax.readthedocs.io/en/latest/gpu_memory_allocation.html\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "vwShBP5hL-9l",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T06:59:28.846916957Z",
     "start_time": "2023-08-30T06:59:28.093836423Z"
    },
    "id": "vwShBP5hL-9l"
   },
   "outputs": [],
   "source": [
    "# haiku provides a nice helper for returning a seed generator\n",
    "init_seed = 21\n",
    "key_iter = hk.PRNGSequence(jax.random.PRNGKey(init_seed))\n",
    "key = jax.random.PRNGKey(init_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a173516-4bd2-41d5-9802-bcaa252b580c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T06:59:28.868935083Z",
     "start_time": "2023-08-30T06:59:28.851813574Z"
    },
    "id": "1a173516-4bd2-41d5-9802-bcaa252b580c",
    "outputId": "4fe8da49-ef81-44ec-ca6a-5ed372058756",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is my instance gpu-ready? True\n"
     ]
    }
   ],
   "source": [
    "def jax_has_gpu():\n",
    "    try:\n",
    "        _ = jax.device_put(jax.numpy.ones(1), device=jax.devices('gpu')[0])\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "gpu = jax_has_gpu()   # automatically checks for gpus, override if needed.\n",
    "print(\"Is my instance gpu-ready?\", gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NTY20AO8L-9l",
   "metadata": {
    "id": "NTY20AO8L-9l"
   },
   "source": [
    "# Practical 0: Refresher on JAX, Haiku and Optax\n",
    "\n",
    "In Part 0, we will (a) briefly introduce the libraries of the tutorial (JAX, Haiku, Optax) and (b) implement a linear classifier. Apart from getting familiarized with the libraries, we will also implement some crucial accessories (training, evaluation loops) that we will reused throughout the tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41aa70e",
   "metadata": {
    "id": "c41aa70e"
   },
   "source": [
    "## 0.1 Create a linear dataset\n",
    "\n",
    "To remember the key concepts of JAX, Haiku and Optax, we will solve a very simple linear classification problem:\n",
    "\n",
    "$$\n",
    "y = \\begin{cases}\n",
    "1, & \\text{if } wx^T + b > 0 \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where $ w \\in \\mathbb{R}^D $ and $ b \\in \\mathbb{R} $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e484cf36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T06:59:28.899850735Z",
     "start_time": "2023-08-30T06:59:28.868692931Z"
    },
    "id": "e484cf36",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_dataset(key, n_samples: int, dim: int):\n",
    "    key, skey = jax.random.split(key)\n",
    "    x = jax.random.normal(skey, (n_samples, dim))\n",
    "\n",
    "    # generate ground truth\n",
    "    key, skey1, skey2 = jax.random.split(key, num=3)\n",
    "    w_gt = jax.random.normal(skey1, (dim, 1))\n",
    "    b_gt = jax.random.normal(skey2, (1, 1))\n",
    "    y_gt = (jnp.dot(x, w_gt) + b_gt).squeeze()\n",
    "\n",
    "    # convert to 0, 1\n",
    "    y_gt = (y_gt > 0).astype(jnp.int32)\n",
    "\n",
    "    return x, y_gt, [w_gt, b_gt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46f5c47b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T06:59:30.070880331Z",
     "start_time": "2023-08-30T06:59:28.876996410Z"
    },
    "id": "46f5c47b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# generate dataset\n",
    "n_samples = 10_000  # number of training instances\n",
    "dim = 20  # feature vector dimension\n",
    "x, y_gt, params_gt = generate_dataset(key, n_samples, dim)\n",
    "\n",
    "# split into train and test\n",
    "n_tr = int(n_samples * 0.8)\n",
    "x_tr, y_tr = x[:n_tr, :], y_gt[:n_tr]\n",
    "x_te, y_te = x[n_tr:, :], y_gt[n_tr:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304c7cd2-588a-4fdd-9595-d3e767a94e1b",
   "metadata": {
    "id": "304c7cd2-588a-4fdd-9595-d3e767a94e1b",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 0.2 [ 📝 ] Create a linear classifier with Haiku"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d7dbed",
   "metadata": {},
   "source": [
    "[ 📝 ]Fill in the MyLinear module to create a custom linear layer. The module should define two parameters, `w` and `b`, and perform the forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "Hxh0xqu2L-9n",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T06:59:30.075981776Z",
     "start_time": "2023-08-30T06:59:30.074943626Z"
    },
    "id": "Hxh0xqu2L-9n"
   },
   "outputs": [],
   "source": [
    "# create a (custom) linear model\n",
    "class MyLinear(hk.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes a custom linear layer module.\n",
    "        \"\"\"\n",
    "        # there is no need to add something here\n",
    "        # except if you want to add some parameters\n",
    "        # in the initialization of the module\n",
    "        super().__init__()\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Performs the forward pass for the custom linear layer.\n",
    "\n",
    "        Args:\n",
    "            x (jnp.ndarray): Input data with shape (batch_size, num_features).\n",
    "\n",
    "        Returns:\n",
    "            jnp.ndarray: The output of the linear layer with shape (batch_size,).\n",
    "        \"\"\"\n",
    "        ##################\n",
    "        # YOUR CODE HERE #\n",
    "        D = x.shape[-1]\n",
    "        w_init = hk.initializers.RandomNormal(stddev=0.1)\n",
    "        b_init = hk.initializers.RandomNormal(stddev=0.1)\n",
    "        w = hk.get_parameter(\"w\", shape=[D, 1], dtype=x.dtype, init=w_init)\n",
    "        b = hk.get_parameter(\"b\", shape=[1], dtype=x.dtype, init=b_init)\n",
    "        y = (jnp.dot(x, w) + b).squeeze()\n",
    "        # Hint: https://dm-haiku.readthedocs.io/en/latest/notebooks/basics.html#A-first-example-with-hk.transform\n",
    "        # (i) define the initializers of w and b\n",
    "        # (ii) define w and b as parameters of the module\n",
    "        # (iii) perform the linear projection: y = w*x + b\n",
    "        # (iv) make sure the output y has shape (batch_size,)\n",
    "        # return y\n",
    "        ##################\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3183647",
   "metadata": {},
   "source": [
    "[ 📝 ] Define the stateful forward function. The arguments `mask` and `is_train` are not needed for this part, but will be used later in the tutorial. For now, just ignore them. The function should return the output of the custom linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a0ef328-0cc2-4af5-a713-adaff1e29bea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T06:59:30.110008208Z",
     "start_time": "2023-08-30T06:59:30.077484279Z"
    },
    "id": "1a0ef328-0cc2-4af5-a713-adaff1e29bea",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def stateful_forward(x, mask, is_train):\n",
    "    \"\"\"\n",
    "    Perform the forward pass.\n",
    "\n",
    "    Args:\n",
    "        x (jnp.ndarray): Input data with shape (batch_size, num_features).\n",
    "        mask (jnp.ndarray): Mask with shape (batch_size,) representing valid elements in x.\n",
    "        is_train (bool): Flag indicating whether the model is in training mode.\n",
    "\n",
    "    Returns:\n",
    "        jnp.ndarray: The output of the custom linear layer with shape (batch_size,).\n",
    "    \"\"\"\n",
    "    linear = MyLinear()\n",
    "    y = linear(x)\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c86698a",
   "metadata": {},
   "source": [
    "In the following cells, we (a) transform the stateful forward function to a stateless one (as JAX demands), (b) initialize the parameters of the model and (c) perform a forward pass to make sure everything works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b31fd91a-49bd-4cd0-8c82-5c954e968b6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T06:59:30.110202894Z",
     "start_time": "2023-08-30T06:59:30.100588073Z"
    },
    "id": "b31fd91a-49bd-4cd0-8c82-5c954e968b6c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def funcs_from_stateful(stateful_forward, jit=True):\n",
    "    \"\"\"\n",
    "    Helping function for getting stateless (pure JAX) functions from the stateful_forward function.\n",
    "\n",
    "    Args:\n",
    "        stateful_forward (Callable): The stateful forward function to be transformed.\n",
    "        jit (bool, optional): Whether to jit-compile the functions. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: A tuple containing:\n",
    "        - model: the transformed model with rng\n",
    "        - model_fw: the transformed model without rng\n",
    "        - predict: the predict function with rng\n",
    "        - predict_fw: the predict function without rng\n",
    "        - init_params: the init function for initializing the model parameters\n",
    "    \"\"\"\n",
    "    model = hk.transform(stateful_forward)\n",
    "    model_fw = hk.without_apply_rng(model)\n",
    "\n",
    "    if jit:\n",
    "        predict = jax.jit(model.apply)\n",
    "        predict_fw = jax.jit(model_fw.apply)\n",
    "        init_params = jax.jit(model.init)\n",
    "    else:\n",
    "        predict = model.apply\n",
    "        predict_fw = model_fw.apply\n",
    "        init_params = model.init\n",
    "    return model, model_fw, predict, predict_fw, init_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fc12808",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T06:59:30.120400506Z",
     "start_time": "2023-08-30T06:59:30.107126718Z"
    },
    "id": "1fc12808",
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model, model_fw, predict, predict_fw, init_params = funcs_from_stateful(stateful_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86ae2996-9e57-420d-83fa-c83809d32a9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T06:59:30.161490325Z",
     "start_time": "2023-08-30T06:59:30.154495399Z"
    },
    "id": "86ae2996-9e57-420d-83fa-c83809d32a9a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_and_pred(key, x, mask):\n",
    "    \"\"\"\n",
    "    Tests the underlying model. Initializes and performs inference with 3 ways:\n",
    "    - using predict (with rng) and `is_train=False`\n",
    "    - using predict_fw (without rng), `is_train=False`\n",
    "    - using predict (with rng), `is_train=True`\n",
    "\n",
    "    Args:\n",
    "        key (jax.random.PRNGKey): Random number generator key for initialization.\n",
    "        x (jnp.ndarray): Input data with shape (batch_size, num_features).\n",
    "        mask (jnp.ndarray): Mask with shape (batch_size,) representing valid elements in xx.\n",
    "\n",
    "    Returns:\n",
    "        None: This function does not return any value. It prints the results of the inference.\n",
    "    \"\"\"\n",
    "    key, *skey = jax.random.split(key, 3)\n",
    "    params = init_params(skey[0], x, mask, False)\n",
    "\n",
    "    print(\"Inference with predict (with rng), is_train=False\")\n",
    "    print(predict(params, skey[1], x, mask, False))\n",
    "\n",
    "    print(\"Inference with predict_fw (without rng), is_train=False\")\n",
    "    print(predict_fw(params, x, mask, False))\n",
    "\n",
    "    print(\"Inference with predict (with rng), is_train=True\")\n",
    "    print(predict(params, skey[1], x, mask, True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18eb272c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T06:59:30.491479707Z",
     "start_time": "2023-08-30T06:59:30.154771574Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "18eb272c",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "fe9a7890-5f0b-4897-f4a3-9bb9cdec6f00",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference with predict (with rng), is_train=False\n",
      "[-0.03593713 -1.6595669   1.1024079   0.17551488  0.70727295]\n",
      "Inference with predict_fw (without rng), is_train=False\n",
      "[-0.03593713 -1.6595669   1.1024079   0.17551488  0.70727295]\n",
      "Inference with predict (with rng), is_train=True\n",
      "[-0.03593713 -1.6595669   1.1024079   0.17551488  0.70727295]\n"
     ]
    }
   ],
   "source": [
    "# check the model\n",
    "xx = x[0:5, :]\n",
    "mask = None\n",
    "key, skey = jax.random.split(key)\n",
    "init_and_pred(key, xx, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30354b27",
   "metadata": {
    "id": "30354b27"
   },
   "source": [
    "###  [ 📝 ] 0.3 Implement training and evaluation Loops\n",
    "\n",
    "In the following cells we will implement five functions for general use:\n",
    "\n",
    "- `get_loss`: computes the loss on a specific batch\n",
    "- `train_step`: implements one training step [ 📝 ]\n",
    "- `evaluate_on_batch`: evaluates the model on a specific batch\n",
    "- `evaluate`: evaluates the model on the whole dataset (test set)\n",
    "- `train`: implements the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "762d8051-a16a-4750-9432-b6404689653b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T06:59:30.495497857Z",
     "start_time": "2023-08-30T06:59:30.494513173Z"
    },
    "id": "762d8051-a16a-4750-9432-b6404689653b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# defing the function that computes the loss\n",
    "@jax.jit\n",
    "def get_loss(params, skey, x: jnp.ndarray, mask, y_gt: jnp.ndarray):\n",
    "    \"\"\"\n",
    "    Compute the binary cross-entropy loss for the given input and ground truth.\n",
    "\n",
    "    Args:\n",
    "        params (List): the parameters of the model.\n",
    "        skey (jax.random.PRNGKey): the random key that is used for the forward pass\n",
    "        x (jnp.ndarray): the input\n",
    "        mask (jnp.ndarray): the mask representing valid elements in x\n",
    "        y_gt (jnp.ndarray): the labels\n",
    "\n",
    "    Returns:\n",
    "        jnp.ndarray: The computed loss value.\n",
    "    \"\"\"\n",
    "    # Predict using skey state and is_train\n",
    "    y = predict(params, skey, x, mask, is_train=True)\n",
    "\n",
    "    # Compute the loss value\n",
    "    loss_value = optax.sigmoid_binary_cross_entropy(y, y_gt).mean(axis=-1)\n",
    "    return loss_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2894557",
   "metadata": {},
   "source": [
    "[ 📝 ] Implement the `train_step` function. This function should perform a single training step for the given input batch. It should return the updated parameters, the updated optimizer state, the computed loss value and the updated random number generator key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dff7bf77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T06:59:30.549330929Z",
     "start_time": "2023-08-30T06:59:30.498110007Z"
    },
    "id": "dff7bf77",
    "tags": []
   },
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(params, key, opt_state, x, mask, y_gt):\n",
    "    \"\"\"\n",
    "    Perform a single training step for the given input batch.\n",
    "\n",
    "    Args:\n",
    "        params (List): the parameters of the model.\n",
    "        skey (jax.random.PRNGKey): the random key that is used for the forward pass\n",
    "        opt_state (OptState): The state of the optimizer.\n",
    "        x (jnp.ndarray): the input\n",
    "        mask (jnp.ndarray): the mask representing valid elements in x\n",
    "        y_gt (jnp.ndarray): the labels\n",
    "\n",
    "    Returns:\n",
    "        Tuple: A tuple containing:\n",
    "         - params: the updated parameters\n",
    "         - opt_state: the updated optimizer state\n",
    "         - loss: the computed loss value\n",
    "         - key: the updated random number generator key\n",
    "    \"\"\"\n",
    "    ##################\n",
    "    # YOUR CODE HERE #\n",
    "    # Move the random generator\n",
    "    key, skey = jax.random.split(key)\n",
    "\n",
    "    # Define gradients with respect to the loss function\n",
    "    val_grad = jax.value_and_grad(get_loss)\n",
    "\n",
    "    # Get loss and gradients with respect to the input batch\n",
    "    loss, grads = val_grad(params, skey, x, mask, y_gt)\n",
    "\n",
    "    # Get updates and new optimizer state, based on gradients and previous state\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "\n",
    "    # Get new params based on previous params and updates\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    #\n",
    "    # (i) Move the random generator, hint: use jax.random.split\n",
    "    # (ii) Define the `gradients function` wrt the loss function, hint: use jax.value_and_grad\n",
    "    # (iii) Get loss and gradients wrt the input batch, hint: use the gradients function\n",
    "    # (iv) Get weight updates and new optimizer state, based on gradients and previous state, hint: use optimizer.update\n",
    "    # (v) Get new params based on previous params and updates, hint: use optax.apply_updates\n",
    "    # (vi) return params, opt_state, loss, key\n",
    "    ##################\n",
    "    return params, opt_state, loss, key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08c5f2c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T06:59:30.559476866Z",
     "start_time": "2023-08-30T06:59:30.538542907Z"
    },
    "id": "08c5f2c1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def evaluate_on_batch(params, x: np.ndarray, mask, y_gt: np.ndarray):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a single input batch.\n",
    "\n",
    "    Args:\n",
    "        params (List): the parameters of the model.\n",
    "        x (jnp.ndarray): the input\n",
    "        mask (jnp.ndarray): the mask representing valid elements in x\n",
    "        y_gt (jnp.ndarray): the labels\n",
    "\n",
    "    Returns:\n",
    "        Tuple: A tuple containing the accuracy and confusion matrix.\n",
    "            - accuracy (float): The accuracy of the model's predictions.\n",
    "            - confusion_matrix (jnp.ndarray): The confusion matrix with shape (2, 2).\n",
    "    \"\"\"\n",
    "    y_pred = predict_fw(params, x, mask, is_train=False)\n",
    "    y_pred_binary = (y_pred > 0).astype(int)\n",
    "    accuracy = jnp.mean(y_pred_binary == y_gt)\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    true_positive = jnp.sum(jnp.logical_and(y_pred_binary == 1, y_gt == 1))\n",
    "    false_positive = jnp.sum(jnp.logical_and(y_pred_binary == 1, y_gt == 0))\n",
    "    true_negative = jnp.sum(jnp.logical_and(y_pred_binary == 0, y_gt == 0))\n",
    "    false_negative = jnp.sum(jnp.logical_and(y_pred_binary == 0, y_gt == 1))\n",
    "\n",
    "    confusion_matrix = jnp.array([[true_negative, false_positive],\n",
    "                                  [false_negative, true_positive]])\n",
    "    return accuracy, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7865714f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T06:59:30.559699521Z",
     "start_time": "2023-08-30T06:59:30.538712775Z"
    },
    "id": "7865714f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(params, x: jnp.ndarray, mask, y_gt: jnp.ndarray, batch_encode, batch_size=32):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the full dataset (x) using batches of batch_size.\n",
    "\n",
    "    Args:\n",
    "        params (List): the parameters of the model.\n",
    "        x (jnp.ndarray): the input\n",
    "        mask (jnp.ndarray): the mask representing valid elements in x\n",
    "        y_gt (jnp.ndarray): the labels\n",
    "        batch_encode (Callable): Function to encode the input data into batches.\n",
    "        batch_size (int, optional): Batch size. Defaults to 32.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: A tuple containing the test accuracy and confusion matrix.\n",
    "            - test_accuracy (float): The accuracy of the model's predictions on the test data.\n",
    "            - cm (jnp.ndarray): The confusion matrix with shape (2, 2).\n",
    "    \"\"\"\n",
    "    cm = jnp.zeros([2, 2])\n",
    "\n",
    "    # Evaluate on the test set with batching\n",
    "    test_accuracy = 0.0\n",
    "    num_batches = int(len(x) / batch_size)\n",
    "    for j in range(num_batches):\n",
    "        start_idx = j * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        x_batch = x[start_idx:end_idx]\n",
    "        y_batch = y_gt[start_idx:end_idx]\n",
    "\n",
    "        if mask is not None:\n",
    "            mask_batch = mask[start_idx:end_idx]\n",
    "        else:\n",
    "            mask_batch = None\n",
    "\n",
    "        if batch_encode is not None:\n",
    "            x_batch = batch_encode(x_batch)\n",
    "\n",
    "        # Move to GPU\n",
    "        if gpu:\n",
    "            x_batch = jnp.array(x_batch)\n",
    "            y_batch = jnp.array(y_batch)\n",
    "            x_batch = jax.device_put(x_batch, jax.devices(\"gpu\")[0])  # Assuming you have only one GPU\n",
    "            y_batch = jax.device_put(y_batch, jax.devices(\"gpu\")[0])  # Assuming you have only one GPU\n",
    "\n",
    "        batch_accuracy, batch_cm = evaluate_on_batch(params, x_batch, mask_batch, y_batch)\n",
    "        test_accuracy += batch_accuracy\n",
    "        cm += batch_cm\n",
    "\n",
    "    test_accuracy /= num_batches\n",
    "    return test_accuracy, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "968573cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T06:59:30.592908339Z",
     "start_time": "2023-08-30T06:59:30.582454040Z"
    },
    "id": "968573cc",
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(params: list,\n",
    "          key,\n",
    "          x_tr: jnp.ndarray,\n",
    "          mask_tr: jnp.ndarray,\n",
    "          y_tr: jnp.ndarray,\n",
    "          epochs: int,\n",
    "          batch_size: int,\n",
    "          x_te: jnp.ndarray,\n",
    "          mask_te: jnp.ndarray,\n",
    "          y_te: jnp.ndarray,\n",
    "          batch_encode: typing.Union[None, typing.Callable] = None,\n",
    "          eval_every: int = 1,\n",
    "          loss_every_batch: int = 32,\n",
    "          gpu=False):\n",
    "    \"\"\"\n",
    "    Train the model using mini-batch stochastic gradient descent.\n",
    "\n",
    "    Args:\n",
    "        params (List): the parameters of the model.\n",
    "        key (jax.random.PRNGKey): the random key that is used for the forward pass\n",
    "        x_tr (jnp.ndarray): Training data\n",
    "        mask_tr (jnp.ndarray): Mask on training data\n",
    "        y_tr (jnp.ndarray): Training set labels\n",
    "        epochs (int): Number of training epochs.\n",
    "        batch_size (int): Batch size.\n",
    "        x_te (jnp.ndarray): Test data\n",
    "        mask_te (jnp.ndarray): Mask on test data\n",
    "        y_te (jnp.ndarray): Test set labels\n",
    "        batch_encode (Union[None, Callable], optional): Function to encode the input data into batches. Defaults to None.\n",
    "        eval_every (int, optional): Number of epochs between evaluations on train and test sets. Defaults to 1.\n",
    "        loss_every_batch (int, optional): Number of batches between printing the loss value. Set to false for disabling printing. Defaults to 32.\n",
    "        gpu (bool, optional): Whether to use GPU acceleration. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        params: The updated model parameters after training.\n",
    "    \"\"\"\n",
    "    opt_state = optimizer.init(params)\n",
    "    nof_instances = x_tr.shape[0]\n",
    "    for e in range(epochs):\n",
    "        nof_full_batches = nof_instances // batch_size\n",
    "        for i in range(nof_full_batches):\n",
    "\n",
    "            # Get batch\n",
    "            batch_start = i * batch_size\n",
    "            batch_end = (i + 1) * batch_size\n",
    "            x_batch = x_tr[batch_start:batch_end]\n",
    "            y_batch = y_tr[batch_start:batch_end]\n",
    "\n",
    "            if mask_tr is not None:\n",
    "                mask_batch = mask_tr[batch_start:batch_end]\n",
    "            else:\n",
    "                mask_batch = None\n",
    "\n",
    "            # Vectorize if needed\n",
    "            if batch_encode is not None:\n",
    "                x_batch = batch_encode(x_batch)\n",
    "\n",
    "            # Move to GPU\n",
    "            if gpu:\n",
    "                x_batch = jnp.array(x_batch)\n",
    "                y_batch = jnp.array(y_batch)\n",
    "                x_batch = jax.device_put(x_batch, jax.devices(\"gpu\")[0])  # Assuming you have only one GPU\n",
    "                y_batch = jax.device_put(y_batch, jax.devices(\"gpu\")[0])  # Assuming you have only one GPU\n",
    "\n",
    "            params, opt_state, loss, key = train_step(params, key, opt_state, x_batch, mask_batch, y_batch)\n",
    "\n",
    "            if loss_every_batch is not False:\n",
    "                if i % loss_every_batch == 0:\n",
    "                    print(\"Epoch: %d, Step %d/%d, Loss: %.3f\" % (e, i, nof_full_batches, loss))\n",
    "\n",
    "        if eval_every is not False:\n",
    "            if e % eval_every == 0:\n",
    "\n",
    "                # Evaluate on the whole training set\n",
    "                train_accuracy, train_cm = evaluate(params, x_tr, mask_tr, y_tr, batch_encode, batch_size)\n",
    "                print(\"Epoch: %d, Train Accuracy: %.4f\" % (e, train_accuracy))\n",
    "                print(\"Confusion Matrix:\\n\", train_cm)\n",
    "\n",
    "                # Evaluate on the test set\n",
    "                test_accuracy, test_cm = evaluate(params, x_te, mask_te, y_te, batch_encode, batch_size)\n",
    "                print(\"Epoch: %d, Test Accuracy: %.4f\" % (e, test_accuracy))\n",
    "                print(\"Confusion Matrix:\\n\", test_cm)\n",
    "\n",
    "                print(\"\\n\")\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec068fe-caba-44af-9f23-350c1c4e687e",
   "metadata": {
    "id": "4ec068fe-caba-44af-9f23-350c1c4e687e"
   },
   "source": [
    "Now that we have all the components let's train our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81212434",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T06:59:37.350254177Z",
     "start_time": "2023-08-30T06:59:30.582643062Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "81212434",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "4974d4ca-dd81-47c3-cbb8-51531598759f",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train Accuracy: 0.4699\n",
      "Confusion Matrix:\n",
      " [[1027. 2167.]\n",
      " [1633. 2341.]]\n",
      "Epoch: 0, Test Accuracy: 0.4727\n",
      "Confusion Matrix:\n",
      " [[134. 318.]\n",
      " [222. 350.]]\n",
      "\n",
      "\n",
      "Epoch: 500, Train Accuracy: 0.9909\n",
      "Confusion Matrix:\n",
      " [[3169.   25.]\n",
      " [  40. 3934.]]\n",
      "Epoch: 500, Test Accuracy: 0.9932\n",
      "Confusion Matrix:\n",
      " [[448.   4.]\n",
      " [  3. 569.]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# init network parameters\n",
    "key, skey = jax.random.split(key)\n",
    "params = init_params(skey, x, None, None)\n",
    "\n",
    "# set-up the parameters\n",
    "epochs = 501\n",
    "lr = 0.01\n",
    "batch_size = 1024\n",
    "eval_every = 500\n",
    "loss_every_batch = False\n",
    "optimizer = optax.sgd(lr)\n",
    "key, skey = jax.random.split(key)\n",
    "\n",
    "# train\n",
    "params = train(params, skey, x_tr, None, y_gt, epochs, batch_size, x_te, None, y_te, None, eval_every, loss_every_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb7ce42-bb78-4593-b1a3-f56df5d86330",
   "metadata": {
    "id": "edb7ce42-bb78-4593-b1a3-f56df5d86330",
    "tags": []
   },
   "source": [
    "# Practical 1: Introduction to NLP\n",
    "\n",
    "In the first part of the tutorial we will introduce the basic NLP concepts.\n",
    "\n",
    "## 1.1 The basic NLP pipeline\n",
    "\n",
    "Nearly all of NLP solultions follow a common high-level pipeline:\n",
    "\n",
    "<img src=\"./../../images/NLP-pipeline.png\">\n",
    "\n",
    "Let's briefly discuss them step by step:\n",
    "\n",
    "* **Standardization:** Reduces small variations in text. For example, \"the cat,\" \"The Cat,\" and \"the cat\" become \"the cat\".\n",
    "* **Tokenization**: Splits the text in tokens (small individual entities). For example, `\"Hello my friend!` will be come `[hello, my, friend, !]`\n",
    "* **Indexing**: Converts to tokens into integers (indexes). For example, `[hello, my, friend, !]` will become `[1, 2, 3, 4]`.\n",
    "* **Encoding/Embedding**: Maps indexes into vectors, forming the embedding space. For example, the word \"cat\" can be mapped to the vector `[0.1, 0.2, 0.3, 0.4]`\n",
    "* **ML model**: Predicts the output based on the embedding space. For example, the model can predict the sentiment of a movie review.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b406d865-8edd-4374-bf60-f810750545b9",
   "metadata": {
    "id": "b406d865-8edd-4374-bf60-f810750545b9",
    "tags": []
   },
   "source": [
    "### 1.1.1 Load the IMDB dataset\n",
    "\n",
    "For the rest of Practical 1, we will use the [IMDB](https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz) dataset. The IMDB is a dataset for sentiment analysis, containing 50,000 movie reviews from IMDB users that are labeled as either positive (1) or negative (0). The dataset is divided into 25,000 reviews for training and 25,000 reviews for testing, so the training and testing sets are balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8cf7305c-80d6-444a-b914-62becf3eb817",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T07:00:18.811869714Z",
     "start_time": "2023-08-30T06:59:37.361782085Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8cf7305c-80d6-444a-b914-62becf3eb817",
    "outputId": "d09779a7-4832-4781-84b3-b04aaf3e0d56",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\r\n",
      "100 80.2M  100 80.2M    0     0  2407k      0  0:00:34  0:00:34 --:--:-- 3204k\r\n"
     ]
    }
   ],
   "source": [
    "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -xf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a39131fe-2a3c-4c98-89f6-3e572a6ed051",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T07:00:19.959529857Z",
     "start_time": "2023-08-30T07:00:18.817822983Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a39131fe-2a3c-4c98-89f6-3e572a6ed051",
    "outputId": "b2dc8ff5-0655-4e7e-8fbb-aa304959b692",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The are 25000 training examples, where 12500 are positive and 12500 are negative reviews\n",
      "The are 25000 testing examples, where 12500 are positive and 12500 are negative reviews\n"
     ]
    }
   ],
   "source": [
    "def read_all_txts(dir):\n",
    "    # all files in dir\n",
    "    files = os.listdir(dir)\n",
    "\n",
    "    sentences = []\n",
    "    for file in files:\n",
    "        filepath = dir + '/' + file\n",
    "        with open(filepath, 'r') as ff:\n",
    "            for line in ff:\n",
    "                line = line.strip()\n",
    "                sentences.append(line)\n",
    "    return sentences\n",
    "\n",
    "def load_dataset(dir):\n",
    "    pos_dir = dir + '/pos'\n",
    "    neg_dir = dir + '/neg'\n",
    "\n",
    "    # read all txts in dir\n",
    "    pos_sentences = read_all_txts(pos_dir)\n",
    "    neg_sentences = read_all_txts(neg_dir)\n",
    "\n",
    "    # to a dataframe\n",
    "    df_pos = pd.DataFrame({'text': pos_sentences, 'label': 1})\n",
    "    df_neg = pd.DataFrame({'text': neg_sentences, 'label': 0})\n",
    "\n",
    "    df = pd.concat([df_pos, df_neg])\n",
    "\n",
    "    # shuffle\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "dir = 'aclImdb/train/'\n",
    "df_tr = load_dataset(dir)\n",
    "dir = 'aclImdb/test/'\n",
    "df_te = load_dataset(dir)\n",
    "\n",
    "df_tr = df_tr.dropna()\n",
    "df_te = df_te.dropna()\n",
    "\n",
    "X_tr = df_tr.iloc[:, 0].to_numpy()\n",
    "Y_tr = df_tr.iloc[:, 1].to_numpy()\n",
    "\n",
    "X_te = df_te.iloc[:, 0].to_numpy()\n",
    "Y_te = df_te.iloc[:, 1].to_numpy()\n",
    "\n",
    "print(\"The are %d training examples, where %d are positive and %d are negative reviews\" % (df_tr.shape[0], df_tr.loc[df_tr[\"label\"] == 1, :].shape[0], df_tr.loc[df_tr[\"label\"] == 1, :].shape[0]))\n",
    "print(\"The are %d testing examples, where %d are positive and %d are negative reviews\" % (df_te.shape[0], df_te.loc[df_te[\"label\"] == 1, :].shape[0], df_te.loc[df_te[\"label\"] == 1, :].shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5340865c-78a5-4522-a771-59a15c976a28",
   "metadata": {
    "id": "5340865c-78a5-4522-a771-59a15c976a28"
   },
   "source": [
    "### 1.1.2 Tokenization\n",
    "\n",
    "We will use the [HuggingFace tokenizers](https://huggingface.co/docs/tokenizers/index), a fast and efficient library for tokenizing text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "918ce8f3-ff58-408b-be8f-c7b1497034f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T07:00:27.078874850Z",
     "start_time": "2023-08-30T07:00:19.961480753Z"
    },
    "id": "918ce8f3-ff58-408b-be8f-c7b1497034f8",
    "outputId": "47c1ba30-21bd-4e2e-8220-35daf4f00173",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define the tokenizer\n",
    "tokenizer = tokenizers.Tokenizer(tokenizers.models.Unigram())\n",
    "tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.Whitespace()\n",
    "tokenizer.normalizer = tokenizers.normalizers.Lowercase()\n",
    "\n",
    "# say how it will be trained, to learn the vocabulary\n",
    "vocab_size = 20000\n",
    "special_tokens = [\"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\",]\n",
    "unk_token = \"[UNK]\"\n",
    "max_piece_length = 16\n",
    "trainer = tokenizers.trainers.UnigramTrainer(\n",
    "    special_tokens=special_tokens, # special tokens\n",
    "    vocab_size=vocab_size, # vocabulary size\n",
    "    unk_token=unk_token, # set the unknown token\n",
    "    show_progress=True, # show progress\n",
    ")\n",
    "\n",
    "# train the tokenizer\n",
    "tokenizer.train_from_iterator(X_tr, trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12a1a45",
   "metadata": {},
   "source": [
    "Let's check the output of the tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b0619bbe-ed27-4213-85d5-6e2de1993aa3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T07:00:27.079169847Z",
     "start_time": "2023-08-30T07:00:27.066557526Z"
    },
    "id": "b0619bbe-ed27-4213-85d5-6e2de1993aa3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# inspect the tokenizer\n",
    "def inspect_tokenizer(inp):\n",
    "    tok_example = tokenizer.encode(inp)\n",
    "    print(\"tokens:\", tok_example.tokens)\n",
    "    print(\"ids:\", tok_example.ids)\n",
    "    print(\"attention mask\", tok_example.attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05e81018-bae7-44bb-a5fd-96c20fa8b62f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T07:00:27.079478197Z",
     "start_time": "2023-08-30T07:00:27.066825852Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "05e81018-bae7-44bb-a5fd-96c20fa8b62f",
    "outputId": "08e47860-1c79-4338-cc5a-5c2688602ba0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: ['hello', 'my', 'friend', '!']\n",
      "ids: [3870, 81, 447, 84]\n",
      "attention mask [1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "inp = \"Hello my friend!\"\n",
    "inp = inspect_tokenizer(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b446bae4-7dcf-476f-b1aa-9667e6812da0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T07:00:27.079767377Z",
     "start_time": "2023-08-30T07:00:27.067046811Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "b446bae4-7dcf-476f-b1aa-9667e6812da0",
    "outputId": "1d2df590-0cdc-49f9-8d8d-544107ff66ef",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello my friend !'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now see the output of the tokenizer\n",
    "tokenizer.decode(tokenizer.encode(\"Hello my friend!\").ids) # reconstruct the sentence from the ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d1ce01-7972-4f93-a55e-50f4068a8e02",
   "metadata": {
    "id": "01d1ce01-7972-4f93-a55e-50f4068a8e02"
   },
   "source": [
    "### 1.1.4 ML Model\n",
    "\n",
    "In the following cells, we will test two approaches; first, we will treat the input text as a **set of words** and later, as a **sequence of words.** In the set of words case, we will use **multi-hot encoding plus an MLP classifier**. In sequence of words **learnable embedding plus a recurrent neural network (LSTM)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222e450b-837a-4194-9406-ddb04d603507",
   "metadata": {
    "id": "222e450b-837a-4194-9406-ddb04d603507"
   },
   "source": [
    "## 1.2 Classification treating text as a Set of words\n",
    "\n",
    "Let's first experiment with the simplest approach: multi-hot encoding + a Fully-Connected NN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450335ff-b1e5-4c8e-8d47-969e65d41293",
   "metadata": {
    "id": "450335ff-b1e5-4c8e-8d47-969e65d41293"
   },
   "source": [
    "### [ 📝 ]  Multi-hot encode the input\n",
    "\n",
    "We need to transform the input text into a multi-hot encoding. First, we will use the tokenizer (defined above) to transform the input text into a list of integers, where each integer represents a word in the vocabulary. Then, we will use the list of integers to create a multi-hot encoding of the input text. The multi-hot encoding is a binary vector with length VOCAB_SIZE, where the $i$-th element indicates whether the $i$-th word of the vocabulary exists in the input. If a word is present, its corresponding element in the tensor is set to 1; otherwise, it is set to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d4543ed1-d82b-4b90-bba8-197f41859ba1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T07:00:27.079925276Z",
     "start_time": "2023-08-30T07:00:27.067274592Z"
    },
    "id": "d4543ed1-d82b-4b90-bba8-197f41859ba1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def batch_encode(X):\n",
    "    \"\"\"\n",
    "    Encode a batch of text data into a multi-hot encoding.\n",
    "\n",
    "    Args:\n",
    "        X (List[str]): A list of input texts.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The multi-hot encoding of the input texts with shape (num_samples, vocab_size).\n",
    "    \"\"\"\n",
    "    ##################\n",
    "    # YOUR CODE HERE #\n",
    "    enc_list = tokenizer.encode_batch(X)\n",
    "    vocab_size = tokenizer.get_vocab_size()\n",
    "    multi_hot_encoding = np.zeros([len(enc_list), vocab_size])\n",
    "    for i, enc in enumerate(enc_list):\n",
    "        multi_hot_encoding[i, enc.ids] = 1\n",
    "\n",
    "    # (i) get a list with encoded vector, hint: use tokenizer.encode_batch\n",
    "    # (ii) get the vocabulary size, hint: use tokenizer.get_vocab_size\n",
    "    # (iii) create and return the multi_hot encoded tensor as a numpy array\n",
    "    ##################\n",
    "    return multi_hot_encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ae1a6a",
   "metadata": {},
   "source": [
    "Check the shape of the ouput tensor. It should be (32, 20000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c9d55103-d430-4a5a-a7d6-da4b793288e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T07:00:27.094448874Z",
     "start_time": "2023-08-30T07:00:27.072789623Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c9d55103-d430-4a5a-a7d6-da4b793288e0",
    "outputId": "9829e94c-52e5-47de-ff65-43b9a0f1b724",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the batch is: (32, 20000)\n"
     ]
    }
   ],
   "source": [
    "# inspect the batch\n",
    "batch_size = 32\n",
    "inp_list = X_tr[:batch_size]\n",
    "X_batch = batch_encode(inp_list)\n",
    "print(\"The shape of the batch is:\", X_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uu9MUJ9PL-91",
   "metadata": {
    "id": "uu9MUJ9PL-91"
   },
   "source": [
    "### [ 📝 ]  Define the MLP\n",
    "\n",
    "We will use a simple MLP with one hidden layer of 16 units (relu acivation) followed by a dropout layer. The output layer has 1 unit. Implement the model below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c8cb52b-120c-44fb-8747-2b2adf8a7bd8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T07:00:27.094692377Z",
     "start_time": "2023-08-30T07:00:27.088538394Z"
    },
    "id": "8c8cb52b-120c-44fb-8747-2b2adf8a7bd8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def stateful_forward(x, mask, is_train):\n",
    "    \"\"\"\n",
    "    A Neural Network.\n",
    "\n",
    "    Args:\n",
    "        x (jnp.ndarray): Input data with shape (batch_size, num_features).\n",
    "        mask (jnp.ndarray): Mask with shape (batch_size,) representing valid elements in x.\n",
    "        is_train (bool): Whether the model is in training mode or not.\n",
    "\n",
    "    Returns:\n",
    "        jnp.ndarray: The output of the model with shape (batch_size,).\n",
    "    \"\"\"\n",
    "    ##################\n",
    "    # YOUR CODE HERE #\n",
    "    x = hk.Linear(16)(x)\n",
    "    x = jax.nn.relu(x)\n",
    "    if is_train:\n",
    "        x = hk.dropout(hk.next_rng_key(), 0.5, x)\n",
    "    y = hk.Linear(1)(x).squeeze()\n",
    "    # Implement a neural network with\n",
    "    # (i) a linear layer with 16 units and relu activation\n",
    "    # (ii) dropout with 0.5 probability if in training mode\n",
    "    # (iii) final linear layer\n",
    "    ##################\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f032d133-c5e6-4900-85dc-76d884edb70d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T07:00:27.211843247Z",
     "start_time": "2023-08-30T07:00:27.092141091Z"
    },
    "id": "f032d133-c5e6-4900-85dc-76d884edb70d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model, model_fw, predict, predict_fw, init_params = funcs_from_stateful(stateful_forward, jit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fc2f1041-a8d5-4e9c-938a-0f322e133d25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T07:00:28.901865231Z",
     "start_time": "2023-08-30T07:00:27.108073416Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fc2f1041-a8d5-4e9c-938a-0f322e133d25",
    "outputId": "e81353d8-dabf-4607-d07b-9e29cd00c70f",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/givasile/miniconda3/envs/jax-cuda-11-8/lib/python3.11/site-packages/haiku/_src/base.py:682: UserWarning: Explicitly requested dtype float64 requested in zeros is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  param = init(shape, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference with predict (with rng), is_train=False\n",
      "[0.05670299 0.02233897]\n",
      "Inference with predict_fw (without rng), is_train=False\n",
      "[0.05670299 0.02233897]\n",
      "Inference with predict (with rng), is_train=True\n",
      "[-0.00528321 -0.04502412]\n"
     ]
    }
   ],
   "source": [
    "# init parameters and check forward pass\n",
    "xx = X_batch[:2]\n",
    "mask = None\n",
    "key, skey = jax.random.split(key)\n",
    "\n",
    "init_and_pred(key, xx, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ddde575b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T07:00:28.949993287Z",
     "start_time": "2023-08-30T07:00:28.890135875Z"
    },
    "id": "ddde575b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize optimizer\n",
    "lr = 0.001\n",
    "optimizer = optax.adam(learning_rate=lr)\n",
    "opt_state = optimizer.init(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d59881-dc15-4710-a5e5-256cfe87bc2c",
   "metadata": {
    "id": "e8d59881-dc15-4710-a5e5-256cfe87bc2c"
   },
   "source": [
    "### Train the model\n",
    "\n",
    "In this section we will train the model for 5 epochs, using a batch size of 124. There is no need to create a training loop, as we have already implemented it above. Therefore, we will simply call the `train` function with the appropriate arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "54a5f01d-89d5-45ff-9a8e-679182bcd777",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T07:02:30.425383550Z",
     "start_time": "2023-08-30T07:00:28.952970994Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "54a5f01d-89d5-45ff-9a8e-679182bcd777",
    "outputId": "bc651bed-35fa-4fa2-c89f-58d28341ad40",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Step 0/201, Loss: 0.696\n",
      "Epoch: 0, Step 25/201, Loss: 0.626\n",
      "Epoch: 0, Step 50/201, Loss: 0.517\n",
      "Epoch: 0, Step 75/201, Loss: 0.483\n",
      "Epoch: 0, Step 100/201, Loss: 0.376\n",
      "Epoch: 0, Step 125/201, Loss: 0.383\n",
      "Epoch: 0, Step 150/201, Loss: 0.405\n",
      "Epoch: 0, Step 175/201, Loss: 0.411\n",
      "Epoch: 0, Step 200/201, Loss: 0.374\n",
      "Epoch: 0, Train Accuracy: 0.9103\n",
      "Confusion Matrix:\n",
      " [[10999.  1470.]\n",
      " [  766. 11689.]]\n",
      "Epoch: 0, Test Accuracy: 0.8791\n",
      "Confusion Matrix:\n",
      " [[10625.  1830.]\n",
      " [ 1183. 11286.]]\n",
      "\n",
      "\n",
      "Epoch: 1, Step 0/201, Loss: 0.337\n",
      "Epoch: 1, Step 25/201, Loss: 0.331\n",
      "Epoch: 1, Step 50/201, Loss: 0.224\n",
      "Epoch: 1, Step 75/201, Loss: 0.290\n",
      "Epoch: 1, Step 100/201, Loss: 0.248\n",
      "Epoch: 1, Step 125/201, Loss: 0.286\n",
      "Epoch: 1, Step 150/201, Loss: 0.321\n",
      "Epoch: 1, Step 175/201, Loss: 0.257\n",
      "Epoch: 1, Step 200/201, Loss: 0.329\n",
      "Epoch: 1, Train Accuracy: 0.9335\n",
      "Confusion Matrix:\n",
      " [[11310.  1159.]\n",
      " [  498. 11957.]]\n",
      "Epoch: 1, Test Accuracy: 0.8843\n",
      "Confusion Matrix:\n",
      " [[10634.  1821.]\n",
      " [ 1062. 11407.]]\n",
      "\n",
      "\n",
      "Epoch: 2, Step 0/201, Loss: 0.261\n",
      "Epoch: 2, Step 25/201, Loss: 0.222\n",
      "Epoch: 2, Step 50/201, Loss: 0.218\n",
      "Epoch: 2, Step 75/201, Loss: 0.230\n",
      "Epoch: 2, Step 100/201, Loss: 0.175\n",
      "Epoch: 2, Step 125/201, Loss: 0.206\n",
      "Epoch: 2, Step 150/201, Loss: 0.185\n",
      "Epoch: 2, Step 175/201, Loss: 0.255\n",
      "Epoch: 2, Step 200/201, Loss: 0.234\n",
      "Epoch: 2, Train Accuracy: 0.9508\n",
      "Confusion Matrix:\n",
      " [[11692.   777.]\n",
      " [  450. 12005.]]\n",
      "Epoch: 2, Test Accuracy: 0.8864\n",
      "Confusion Matrix:\n",
      " [[10831.  1624.]\n",
      " [ 1208. 11261.]]\n",
      "\n",
      "\n",
      "Epoch: 3, Step 0/201, Loss: 0.222\n",
      "Epoch: 3, Step 25/201, Loss: 0.216\n",
      "Epoch: 3, Step 50/201, Loss: 0.186\n",
      "Epoch: 3, Step 75/201, Loss: 0.191\n",
      "Epoch: 3, Step 100/201, Loss: 0.126\n",
      "Epoch: 3, Step 125/201, Loss: 0.194\n",
      "Epoch: 3, Step 150/201, Loss: 0.202\n",
      "Epoch: 3, Step 175/201, Loss: 0.197\n",
      "Epoch: 3, Step 200/201, Loss: 0.194\n",
      "Epoch: 3, Train Accuracy: 0.9589\n",
      "Confusion Matrix:\n",
      " [[11767.   702.]\n",
      " [  322. 12133.]]\n",
      "Epoch: 3, Test Accuracy: 0.8839\n",
      "Confusion Matrix:\n",
      " [[10708.  1747.]\n",
      " [ 1147. 11322.]]\n",
      "\n",
      "\n",
      "Epoch: 4, Step 0/201, Loss: 0.213\n",
      "Epoch: 4, Step 25/201, Loss: 0.186\n",
      "Epoch: 4, Step 50/201, Loss: 0.151\n",
      "Epoch: 4, Step 75/201, Loss: 0.149\n",
      "Epoch: 4, Step 100/201, Loss: 0.108\n",
      "Epoch: 4, Step 125/201, Loss: 0.183\n",
      "Epoch: 4, Step 150/201, Loss: 0.168\n",
      "Epoch: 4, Step 175/201, Loss: 0.175\n",
      "Epoch: 4, Step 200/201, Loss: 0.185\n",
      "Epoch: 4, Train Accuracy: 0.9679\n",
      "Confusion Matrix:\n",
      " [[11959.   510.]\n",
      " [  291. 12164.]]\n",
      "Epoch: 4, Test Accuracy: 0.8834\n",
      "Confusion Matrix:\n",
      " [[10843.  1612.]\n",
      " [ 1293. 11176.]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "epochs = 3\n",
    "batch_size = 124\n",
    "is_train=True\n",
    "key, skey = jax.random.split(key)\n",
    "\n",
    "# init params\n",
    "params = init_params(skey, xx, mask, is_train)\n",
    "\n",
    "# train loop\n",
    "params = train(params, skey, X_tr, None, Y_tr, epochs, batch_size, X_te, None, Y_te, batch_encode, loss_every_batch=25, gpu=gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "38840b00-845e-4db9-b6e5-66786e7d3161",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T07:02:30.430844745Z",
     "start_time": "2023-08-30T07:02:30.428140777Z"
    },
    "id": "38840b00-845e-4db9-b6e5-66786e7d3161"
   },
   "outputs": [],
   "source": [
    "def inspect_an_input(i):\n",
    "    inp = batch_encode(X_te[ii:ii+1])\n",
    "    y_pr_score = model_fw.apply(params, inp, None, False)\n",
    "    y_pr = jnp.greater(y_pr_score, 0).astype(int)\n",
    "\n",
    "    print(\"input:\", X_te[ii])\n",
    "    print(\"\\n\")\n",
    "    print(\"ground truth: %d\" % Y_te[ii])\n",
    "    print(\"\\n\")\n",
    "    print(\"prediction  : %d\" % y_pr)\n",
    "    print(\"\\n\")\n",
    "    print(\"prediction score: %f\" % y_pr_score)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "80877422",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T07:02:30.982906429Z",
     "start_time": "2023-08-30T07:02:30.433143103Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "80877422",
    "outputId": "a90ce807-4ef7-4ab7-8d61-f8fa3f19daa2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: Rigoletto is Verdi's masterpiece, full of drama, emotion and powerful, memorable music. The maestro must have rolled in his grave when this bawdy travesty of his work was released with its needless frontal nudity and cheap copulating and its portrayal of the naive but principalled Gilda as a horny ditz. Opera certainly can be adapted to cinema --- look at Zeferelli's magnificent La Traviata --- but when a work is as superb as Rigoletto, it doesn't need cheap gimmicks. It might even have been acceptable if the dubbed in music had been good but it is a mediocre rendering of the libretto with second rate sound quality at that.\n",
      "\n",
      "\n",
      "ground truth: 0\n",
      "\n",
      "\n",
      "prediction  : 1\n",
      "\n",
      "\n",
      "prediction score: 0.158785\n",
      "\n",
      "\n",
      "input: This movie was horrible and the only reason it was even made was because the story appealed to the far-left. I consider my self a moderate, so I was able to see this film as the pile of garbage it was. While I'm not a Bush fan, your dislike for GW is not enough of a reason to see this movie.<br /><br />To start, the movie was shot on such low-grade film that it comes off as cheap, rather then artsy. Additionally, the characters are seriously lacking in depth. Chris Cooper's character was a poor parody of George Bush; better suited for Saturday Night Live then a Dramatic film. The rest of the characters are walking clichés and are poor facsimiles of other characters from much better movies.<br /><br />Avoid this movie at all costs!\n",
      "\n",
      "\n",
      "ground truth: 0\n",
      "\n",
      "\n",
      "prediction  : 0\n",
      "\n",
      "\n",
      "prediction score: -9.034957\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ii = 10\n",
    "inspect_an_input(ii)\n",
    "\n",
    "ii = 12\n",
    "inspect_an_input(ii)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416da2cc-7660-4624-be66-e2f56e5bfbc3",
   "metadata": {
    "id": "416da2cc-7660-4624-be66-e2f56e5bfbc3"
   },
   "source": [
    "### [ 📝 ] Inspect and Discuss\n",
    "\n",
    "Play around with the model. Try to find some weaknesses using some inputs either from the training set or using your own inputs. Keep in mind that the model does not treat the input as sequence but as a set of words. Therefore, it does not take into account the order of the words. Can you find a characteristic input that reveals this weakness?\n",
    "\n",
    "If we were to ask ChatGPT to classify the same inputs, would it have the same weaknesses? Let's try it.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ea7be8ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T07:02:30.985744901Z",
     "start_time": "2023-08-30T07:02:30.983577525Z"
    },
    "id": "ea7be8ec",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_on_custom_input(test_input):\n",
    "    test_input = np.array([test_input])\n",
    "    test_enc = batch_encode(test_input)\n",
    "    print(model_fw.apply(params, test_enc, None, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ffabf9aa-c3c2-44e0-8aa9-be592bb337ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T07:02:31.064314530Z",
     "start_time": "2023-08-30T07:02:30.986373050Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ffabf9aa-c3c2-44e0-8aa9-be592bb337ff",
    "outputId": "531850c1-611c-4bb0-8998-6e9ee0f9f3b9",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0342286\n",
      "1.8297132\n",
      "-0.44754705\n",
      "0.88967776\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "# YOUR CODE HERE #\n",
    "# real positive impact\n",
    "test_input = \"I have seen a lot of excellent movies and this one is one of them\"\n",
    "predict_on_custom_input(test_input)\n",
    "\n",
    "# the word not inverts the comment, but the model cannot understand, it simply understands not as something that makes the comment more negative\n",
    "test_input = \"I have seen a lot of excellent movies and this one is not one of them\"\n",
    "predict_on_custom_input(test_input)\n",
    "\n",
    "# same effect\n",
    "test_input = \"This is, for sure, one of the bad movies.\"\n",
    "predict_on_custom_input(test_input)\n",
    "\n",
    "# same effect\n",
    "test_input = \"This was a worth-seeing movie but, for sure, not one of the best.\"\n",
    "predict_on_custom_input(test_input)\n",
    "##################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZaDyc7zXL-92",
   "metadata": {
    "id": "ZaDyc7zXL-92"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86ba0022-9651-444d-84d2-d520d1e29d80",
   "metadata": {
    "id": "86ba0022-9651-444d-84d2-d520d1e29d80"
   },
   "source": [
    "## 1.3 Classification treating the input text as a Sequence\n",
    "\n",
    "In this part, we will treat the input text as a sequence of words, using a learnable embedding plus an LSTM network.\n",
    "The network will have (a) an embedding layer (b) an LSTM and a (c) Dense layer\n",
    "Before we code the models, let's set up appropriatelly the lenght of the sequence returned by the tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5ed18a-f75c-4f48-9000-55471adbf85b",
   "metadata": {
    "id": "7e5ed18a-f75c-4f48-9000-55471adbf85b"
   },
   "source": [
    "### [ 📝 ]  Define Tokenizer\n",
    "\n",
    "Since we will experiment with sequential models, we want the tokenizer's output to be a tensor of shape `BS, SEQUENCE_LENGTH)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b4c4739e-91b9-47ac-b51a-1de36dc21400",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T07:02:31.064691221Z",
     "start_time": "2023-08-30T07:02:31.020439473Z"
    },
    "id": "b4c4739e-91b9-47ac-b51a-1de36dc21400",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# inspect what happens if setting the min and max sequence lengths to 10\n",
    "sequence_length = 10\n",
    "truncation_length = 10\n",
    "tokenizer.enable_padding(length=sequence_length)\n",
    "tokenizer.enable_truncation(max_length=truncation_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "940ee3d8-f76e-4475-8d2b-3e963d523cf3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T07:02:31.066056170Z",
     "start_time": "2023-08-30T07:02:31.022243385Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "940ee3d8-f76e-4475-8d2b-3e963d523cf3",
    "outputId": "e573d6bd-5145-4e31-aa2a-bc3680f18406",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: ['hello', 'my', 'friend', '!', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "ids: [3870, 81, 447, 84, 0, 0, 0, 0, 0, 0]\n",
      "attention mask [1, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n",
      "tokens: ['hello', 'my', 'dear', 'dear', 'dear', 'dear', 'dear', 'dear', 'dear', 'd']\n",
      "ids: [3870, 81, 2841, 2841, 2841, 2841, 2841, 2841, 2841, 23]\n",
      "attention mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# inspect the tokenizer\n",
    "inp = \"Hello my friend!\"\n",
    "inspect_tokenizer(inp)\n",
    "\n",
    "inp = \"Hello my dear dear dear dear dear dear dear dearest friend!\"\n",
    "inspect_tokenizer(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2f2334a6-2716-4622-a30f-2f7e446d690e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T07:02:31.066374712Z",
     "start_time": "2023-08-30T07:02:31.062739472Z"
    },
    "id": "2f2334a6-2716-4622-a30f-2f7e446d690e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# now let's set it to a normal value\n",
    "sequence_length = 600\n",
    "truncation_length = 600\n",
    "tokenizer.enable_padding(length=sequence_length)\n",
    "tokenizer.enable_truncation(max_length=truncation_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0853305a-0d5f-47e7-a066-be09624485b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T07:02:31.066660170Z",
     "start_time": "2023-08-30T07:02:31.063063849Z"
    },
    "id": "0853305a-0d5f-47e7-a066-be09624485b7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def batch_encode(X):\n",
    "    enc_list = tokenizer.encode_batch(X)\n",
    "    enc_list_2 = []\n",
    "    mask_list = []\n",
    "    for i, enc in enumerate(enc_list):\n",
    "        enc_list_2.append(enc.ids)\n",
    "        mask_list.append(enc.attention_mask)\n",
    "    return np.array(enc_list_2), np.array(mask_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "69579c47-46cd-4c8c-97fa-ef557b480735",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T07:02:48.428543224Z",
     "start_time": "2023-08-30T07:02:31.063258743Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "69579c47-46cd-4c8c-97fa-ef557b480735",
    "outputId": "78d818c1-c6a6-4523-fb0d-b284cb5541d9",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 600)\n",
      "(25000, 600)\n"
     ]
    }
   ],
   "source": [
    "# but since it is not memory-consuming, we can use the whole dataset\n",
    "X_tr_enc, X_tr_mask = batch_encode(X_tr)\n",
    "X_te_enc, X_te_mask = batch_encode(X_te)\n",
    "\n",
    "print(X_tr_enc.shape)\n",
    "print(X_te_enc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e8433e-cba3-43d0-af70-f2f34054d34a",
   "metadata": {
    "id": "99e8433e-cba3-43d0-af70-f2f34054d34a",
    "tags": []
   },
   "source": [
    " Please define the network in the next cell. It must have:\n",
    "\n",
    " - an embedding of shape `(BS, S, H)`\n",
    " - an LSTM to capture the sequential information\n",
    " - use all hidden states of the LSTM for obtaining the final prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e9afcc57-4235-4bd3-bf11-3a88f56f8d12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T07:02:48.434203484Z",
     "start_time": "2023-08-30T07:02:48.431761705Z"
    },
    "id": "e9afcc57-4235-4bd3-bf11-3a88f56f8d12",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def stateful_forward(x, mask, is_train):\n",
    "    \"\"\"\n",
    "    Implement the forward pass of the stateful model using a DeepRNN with LSTM layers.\n",
    "\n",
    "    Args:\n",
    "        x (jnp.ndarray): Input data with shape (batch_size, sequence_length).\n",
    "        mask (jnp.ndarray): Mask with shape (batch_size, sequence_length) representing valid elements in x.\n",
    "        is_train (bool): Whether the model is in training mode or not.\n",
    "\n",
    "    Returns:\n",
    "        jnp.ndarray: The final predictions with shape (batch_size,).\n",
    "    \"\"\"\n",
    "    ##################\n",
    "    # YOUR CODE HERE #\n",
    "    # Initialize classes and dimensions\n",
    "    batch_size = x.shape[0]\n",
    "    embed_dim = 128\n",
    "    embed = hk.Embed(vocab_size, embed_dim)\n",
    "    core = hk.DeepRNN([hk.LSTM(64), hk.Linear(1)])\n",
    "    linear = hk.Linear(1)\n",
    "\n",
    "    # Embed the input\n",
    "    initial_state = core.initial_state(batch_size=batch_size)\n",
    "    x = embed(x)\n",
    "\n",
    "    # Unroll the LSTM across time steps using dynamic_unroll\n",
    "    outs, states = hk.dynamic_unroll(core, x, initial_state, time_major=False)\n",
    "\n",
    "    # Apply dropout if in training mode\n",
    "    if is_train:\n",
    "        outs = hk.dropout(hk.next_rng_key(), 0.2, outs)\n",
    "\n",
    "    # Compute final predictions\n",
    "    outs = jnp.squeeze(outs, axis=-1)\n",
    "    outs = linear(outs)\n",
    "    y = jnp.squeeze(outs, axis=-1)\n",
    "    #\n",
    "    # (i) Embed the input to a tensor (BS, S, 128)\n",
    "    # (ii) Unroll an LSTM over the embedded sequence, use a linear layer to map each LSTM output to a single value\n",
    "    # (iii) use the single values of all intermediate steps of the LSTM as input to a linear layer to get y\n",
    "    # (iv) return y\n",
    "    ##################\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f3f0f98a-3103-44df-97d4-b31887305cbb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T07:02:48.492783133Z",
     "start_time": "2023-08-30T07:02:48.436484113Z"
    },
    "id": "f3f0f98a-3103-44df-97d4-b31887305cbb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model, model_fw, predict, predict_fw, init_params = funcs_from_stateful(stateful_forward, jit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5e96c013-2463-4a80-971a-bd4d33cd168d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T07:02:52.966664326Z",
     "start_time": "2023-08-30T07:02:48.478548949Z"
    },
    "id": "5e96c013-2463-4a80-971a-bd4d33cd168d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# init parameters and check forward pass\n",
    "xx = X_tr_enc[:2]\n",
    "mask = X_tr_mask[:2]\n",
    "key, skey = jax.random.split(key)\n",
    "\n",
    "# init_and_pred(key, xx, mask)\n",
    "params = init_params(key, xx, mask, is_train=False)\n",
    "out = predict(params, key, xx, mask, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b310bc57-8e03-442b-bac0-d95b10dd0bb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T07:02:53.289878692Z",
     "start_time": "2023-08-30T07:02:53.010473065Z"
    },
    "id": "b310bc57-8e03-442b-bac0-d95b10dd0bb2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize optimizer\n",
    "lr = 0.01\n",
    "optimizer = optax.adam(learning_rate=lr)\n",
    "opt_state = optimizer.init(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dc6e434b-af5b-4bdf-8300-01ef7a4757b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T07:04:12.853062203Z",
     "start_time": "2023-08-30T07:02:53.283181780Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dc6e434b-af5b-4bdf-8300-01ef7a4757b4",
    "outputId": "00f5f606-86fb-4e22-8795-c7f6941805e4",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Step 0/195, Loss: 0.700\n",
      "Epoch: 0, Step 32/195, Loss: 0.708\n",
      "Epoch: 0, Step 64/195, Loss: 0.709\n",
      "Epoch: 0, Step 96/195, Loss: 0.640\n",
      "Epoch: 0, Step 128/195, Loss: 0.435\n",
      "Epoch: 0, Step 160/195, Loss: 0.320\n",
      "Epoch: 0, Step 192/195, Loss: 0.327\n",
      "Epoch: 0, Train Accuracy: 0.9154\n",
      "Confusion Matrix:\n",
      " [[11175.  1308.]\n",
      " [  803. 11674.]]\n",
      "Epoch: 0, Test Accuracy: 0.8696\n",
      "Confusion Matrix:\n",
      " [[10639.  1840.]\n",
      " [ 1415. 11066.]]\n",
      "\n",
      "\n",
      "Epoch: 1, Step 0/195, Loss: 0.391\n",
      "Epoch: 1, Step 32/195, Loss: 0.287\n",
      "Epoch: 1, Step 64/195, Loss: 0.253\n",
      "Epoch: 1, Step 96/195, Loss: 0.336\n",
      "Epoch: 1, Step 128/195, Loss: 0.265\n",
      "Epoch: 1, Step 160/195, Loss: 0.188\n",
      "Epoch: 1, Step 192/195, Loss: 0.162\n",
      "Epoch: 1, Train Accuracy: 0.9460\n",
      "Confusion Matrix:\n",
      " [[11260.  1223.]\n",
      " [  124. 12353.]]\n",
      "Epoch: 1, Test Accuracy: 0.8628\n",
      "Confusion Matrix:\n",
      " [[ 9758.  2721.]\n",
      " [  704. 11777.]]\n",
      "\n",
      "\n",
      "Epoch: 2, Step 0/195, Loss: 0.325\n",
      "Epoch: 2, Step 32/195, Loss: 0.152\n",
      "Epoch: 2, Step 64/195, Loss: 0.106\n",
      "Epoch: 2, Step 96/195, Loss: 0.186\n",
      "Epoch: 2, Step 128/195, Loss: 0.116\n",
      "Epoch: 2, Step 160/195, Loss: 0.069\n",
      "Epoch: 2, Step 192/195, Loss: 0.030\n",
      "Epoch: 2, Train Accuracy: 0.9847\n",
      "Confusion Matrix:\n",
      " [[12317.   166.]\n",
      " [  216. 12261.]]\n",
      "Epoch: 2, Test Accuracy: 0.8642\n",
      "Confusion Matrix:\n",
      " [[11096.  1383.]\n",
      " [ 2007. 10474.]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "epochs = 3\n",
    "batch_size = 128\n",
    "key, skey = jax.random.split(key)\n",
    "\n",
    "# init params\n",
    "params = init_params(skey, xx, mask, is_train)\n",
    "\n",
    "# train loop\n",
    "params = train(params,\n",
    "               skey,\n",
    "               X_tr_enc,\n",
    "               X_tr_mask,\n",
    "               Y_tr,\n",
    "               epochs=epochs,\n",
    "               batch_size=batch_size,\n",
    "               x_te=X_te_enc,\n",
    "               mask_te=X_te_mask,\n",
    "               y_te=Y_te,\n",
    "               batch_encode=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9329f5",
   "metadata": {},
   "source": [
    "### [ 📝 ] Inspect and Discuss\n",
    "\n",
    "(You may use the same inputs as in the previous Ispection and Discussion section)\n",
    "\n",
    "Play around with the model. Try to find some weaknesses using some inputs either from the training set or using your own inputs. Keep in mind that the model does not treat the input as sequence but as a set of words. Therefore, it does not take into account the order of the words. Can you find a characteristic input that reveals this weakness?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b375e36a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T07:09:33.479071695Z",
     "start_time": "2023-08-30T07:09:33.435108828Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_on_custom_input(test_input):\n",
    "    test_input = np.array([test_input])\n",
    "    test_enc, mask = batch_encode(test_input)\n",
    "    print(model_fw.apply(params, test_enc, None, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "72600370",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T07:09:34.910282400Z",
     "start_time": "2023-08-30T07:09:34.336552272Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.3750224]\n",
      "[2.9429648]\n",
      "[-0.5702515]\n",
      "[2.429214]\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "# YOUR CODE HERE #\n",
    "# real positive impact\n",
    "test_input = \"I have seen a lot of excellent movies and this one is one of them\"\n",
    "predict_on_custom_input(test_input)\n",
    "\n",
    "# the word not inverts the comment, but the model cannot understand, it simply understands not as something that makes the comment more negative\n",
    "test_input = \"I have seen a lot of excellent movies and this one is not one of them\"\n",
    "predict_on_custom_input(test_input)\n",
    "\n",
    "# same effect\n",
    "test_input = \"This is, for sure, one of the bad movies.\"\n",
    "predict_on_custom_input(test_input)\n",
    "\n",
    "# same effect\n",
    "test_input = \"This was a worth-seeing movie but, for sure, not one of the best.\"\n",
    "predict_on_custom_input(test_input)\n",
    "##################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9287ce-7787-4d04-9ee3-7597328a1c0a",
   "metadata": {
    "id": "bc9287ce-7787-4d04-9ee3-7597328a1c0a"
   },
   "source": [
    "## 1.4 Discussion\n",
    "\n",
    "- Which model had the best performance? How do you explain that?\n",
    "- Among the sequence-based models which one had the best performance? How would you explain that?\n",
    "- Take the bag of words model and the best sequence-based model, and try one or two custom inputs. Do you see any differences in the results? Could you explain that somehow?\n",
    "\n",
    "Hidden answer:\n",
    "\n",
    "Although we tried so much to code quite involved sequential models, first with LSTM and then with transformers, you may have noticed that the best accuracy is achieved with a simple Fully Connected Model after multi-hot encoding. In reality, the best sequential models have almost similar accuracy. Why is this the case? Our dear &#x1F449; [Deep Learning with Python (DLP)](https://www.manning.com/books/deep-learning-with-python) &#x1F448; has a rule of thumb:\n",
    "\n",
    "\n",
    "> You may sometimes hear that bag-of-words methods are outdated, and that Transformer-\n",
    "based sequence models are the way to go, no matter what task or dataset you’re look-\n",
    "ing at. This is definitely not the case: a small stack of Dense layers on top of a bag-of-\n",
    "bigrams remains a perfectly valid and relevant approach in many cases. In fact, among\n",
    "the various techniques that we’ve tried on the IMDB dataset throughout this chapter,\n",
    "the best performing so far was the bag-of-bigrams!\n",
    "So, when should you prefer one approach over the other?\n",
    "In 2017, my team and I ran a systematic analysis of the performance of various text-\n",
    "classification techniques across many different types of text datasets, and we discov-\n",
    "ered a remarkable and surprising rule of thumb for deciding whether to go with a\n",
    "bag-of-words model or a sequence model (http://mng.bz/AOzK)—a golden constant\n",
    "of sorts.\n",
    "It turns out that when approaching a new text-classification task, you should pay\n",
    "close attention to the ratio between the number of samples in your training data and\n",
    "the mean number of words per sample (see figure 11.11). If that ratio is small—less\n",
    "than 1,500—then the bag-of-bigrams model will perform better (and as a bonus, it will\n",
    "be much faster to train and to iterate on too). If that ratio is higher than 1,500, then\n",
    "you should go with a sequence model. In other words, sequence models work best\n",
    "when lots of training data is available and when each sample is relatively short.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PZ8TXzow3EWB",
   "metadata": {
    "id": "PZ8TXzow3EWB"
   },
   "source": [
    "### [ 📝 ] Advanced: Open-end Exercise\n",
    "\n",
    "Try to design a better model. You can go with a bag-of-words based approach or a sequence-based model. Your idea can be an incremental improvement on the previous models, e.g. stack more LSTM layers, increase the dimension of the hidden states, or something completely new, i.e. try GRU layer instead of the LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3f339e9b-0171-4ded-85e3-07bcfef0cc09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T07:09:39.087941016Z",
     "start_time": "2023-08-30T07:09:39.008257960Z"
    },
    "id": "3f339e9b-0171-4ded-85e3-07bcfef0cc09",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def stateful_forward(x, mask, is_train):\n",
    "#     ##################\n",
    "#     # YOUR CODE HERE #\n",
    "#     ##################\n",
    "#     return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "caf8cde1-347d-4b1c-ac42-ead530ba2ab3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T07:09:39.470759153Z",
     "start_time": "2023-08-30T07:09:39.454740196Z"
    },
    "id": "caf8cde1-347d-4b1c-ac42-ead530ba2ab3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model, model_fw, predict, predict_fw, init_params = funcs_from_stateful(stateful_forward, jit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d1b03aaa-e977-40b0-8477-c12c53f3af19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T07:09:39.708968047Z",
     "start_time": "2023-08-30T07:09:39.678590660Z"
    },
    "id": "d1b03aaa-e977-40b0-8477-c12c53f3af19",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # init parameters and check forward pass\n",
    "# xx = X_tr_enc[:2]\n",
    "# mask = X_tr_mask[:2]\n",
    "# key, skey = jax.random.split(key)\n",
    "\n",
    "# # init_and_pred(key, xx, mask)\n",
    "# params = init_params(key, xx, mask, is_train=False)\n",
    "# out = predict(params, key, xx, mask, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "37432602-c903-4ff8-a227-8f32751d84f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T07:09:40.307750814Z",
     "start_time": "2023-08-30T07:09:40.295859950Z"
    },
    "id": "37432602-c903-4ff8-a227-8f32751d84f5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # initialize optimizer\n",
    "# optimizer = # complete optimizer\n",
    "# opt_state = optimizer.init(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "99050b18-b456-4586-886c-9f4d74bf0166",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T07:09:40.511978445Z",
     "start_time": "2023-08-30T07:09:40.497737943Z"
    },
    "id": "99050b18-b456-4586-886c-9f4d74bf0166",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # train\n",
    "# epochs = #\n",
    "# batch_size = #\n",
    "# key, skey = jax.random.split(key)\n",
    "\n",
    "# # init params\n",
    "# params = init_params(skey, xx, mask, is_train)\n",
    "\n",
    "# # train loop\n",
    "# params = train(params,\n",
    "#                skey,\n",
    "#                X_tr_enc,\n",
    "#                X_tr_mask,\n",
    "#                Y_tr,\n",
    "#                epochs=3,\n",
    "#                batch_size=128,\n",
    "#                x_te=X_te_enc,\n",
    "#                mask_te=X_te_mask,\n",
    "#                y_te=Y_te,\n",
    "#                batch_encode=None)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
