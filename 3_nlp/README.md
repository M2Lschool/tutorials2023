ll# [[M2L2023](https://www.m2lschool.org/home)] Tutorial 3: Natural Language Processing

**Authors:** **[Vasilis Gkolemis](https://givasile.github.io/)** and **[Matko Bosnjak](https://scholar.google.com/citations?user=JDaHecMAAAAJ&hl=en)**

--- 

## Practical 1: The NLP pipeline

### Outline

In this tutorial you will look into basic components of NLP models, including approaches to encoding and sequential modelling.

Contents:
 - 0 Refresher on JAX, Haiku and Optax
 - 1 Introduction to NLP
 - 1.1 The NLP pipeline
 - 1.2 Classification pipeline: Multi-hot encoding + MLP model
 - 1.3 Classification pipeline: Embeddings + Sequential Model

### Notebooks

Tutorial: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/M2Lschool/tutorials2023/blob/main/3_nlp/notebooks/3_1_intro_nlp/3_1_intro_nlp.ipynb)

Solution: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/M2Lschool/tutorials2023/blob/main/3_nlp/notebooks/3_1_intro_nlp/3_1_intro_nlp_solved.ipynb)


---

## Practical 2: Introduction to the Transformers architecture

### Outline

In this tutorial we will implement a Transformer model from scratsh and use it for text classification.

Contents:
 - 2 Introduction to the Transformers architecture
 - 2.1 Transformer architecture
 - 2.2 Implementing the core components
 - 2.3 Transformer for classification pipeline

### Notebook

Tutorial: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/M2Lschool/tutorials2023/blob/main/3_nlp/notebooks/3_2_transformers_classification/3_2_transformers_classification.ipynb)

Solution: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/M2Lschool/tutorials2023/blob/main/3_nlp/notebooks/3_2_transformers_classification/3_2_transformers_classification_solved.ipynb)


---

## Practical 3: Transformers for language translation

This tutorial builds on the Transformer model and uses it for language translation.

Contents:
 - 3 Continuing Transformers: Decoder and the complete architecture
 - 3.1 The Transformer Decoder
 - 3.2 Transformer Decoder for character-based Language Modelling
 - 3.3 The full Transformer
 - 3.4 Transformer for Neural Machine Translation

### Notebook

Tutorial: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/M2Lschool/tutorials2023/blob/main/3_nlp/notebooks/3_3_transformers_translation/3_3_transformers_translation.ipynb)

Solution: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/M2Lschool/tutorials2023/blob/main/3_nlp/notebooks/3_3_transformers_translation/3_3_transformers_translation_solved.ipynb)


---

**Note:** Designed for education purposes. Please do not distribute without permission.
<br>
**Questions**: Please contact us on the Slack workspace of the M2L school.
